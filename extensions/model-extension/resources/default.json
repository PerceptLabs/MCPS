[
  {
    "author": "Menlo",
    "id": "Menlo/Jan-nano-gguf",
    "metadata": {
      "_id": "68492cd9cada68b1d11ca1bd",
      "author": "Menlo",
      "cardData": {
        "license": "apache-2.0",
        "pipeline_tag": "text-generation"
      },
      "createdAt": "2025-06-11T07:14:33.000Z",
      "description": "---\nlicense: apache-2.0\npipeline_tag: text-generation\n---\n# Jan Nano\n\n\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/657a81129ea9d52e5cbd67f7/YQci8jiHjAAFpXWYOadrU.png)\n\n## Overview\n\nJan Nano is a fine-tuned language model built on top of the Qwen3 architecture. Developed as part of the Jan ecosystem, it balances compact size and extended context length, making it ideal for efficient, high-quality text generation in local or embedded environments.\n\n## Features\n\n- **Tool Use**: Excellent function calling and tool integration\n- **Research**: Enhanced research and information processing capabilities\n- **Small Model**: VRAM efficient for local deployment\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)",
      "disabled": false,
      "downloads": 1434,
      "gated": false,
      "gguf": {
        "architecture": "qwen3",
        "bos_token": "<|endoftext|>",
        "chat_template": "{%- if tools %} {{- '<|im_start|>system\\n' }} {%- if messages[0].role == 'system' %} {{- messages[0].content + '\\n\\n' }} {%- endif %} {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }} {%- for tool in tools %} {{- \"\\n\" }} {{- tool | tojson }} {%- endfor %} {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }} {%- else %} {%- if messages[0].role == 'system' %} {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }} {%- endif %} {%- endif %} {%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %} {%- for message in messages[::-1] %} {%- set index = (messages|length - 1) - loop.index0 %} {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %} {%- set ns.multi_step_tool = false %} {%- set ns.last_query_index = index %} {%- endif %} {%- endfor %} {%- for message in messages %} {%- if message.content is string %} {%- set content = message.content %} {%- else %} {%- set content = '' %} {%- endif %} {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %} {{- '<|im_start|>' + message.role + '\\n' + content + '<|im_end|>' + '\\n' }} {%- elif message.role == \"assistant\" %} {%- set reasoning_content = '' %} {%- if message.reasoning_content is string %} {%- set reasoning_content = message.reasoning_content %} {%- else %} {%- if '</think>' in content %} {%- set reasoning_content = content.split('</think>')[0].rstrip('\\n').split('<think>')[-1].lstrip('\\n') %} {%- set content = content.split('</think>')[-1].lstrip('\\n') %} {%- endif %} {%- endif %} {%- if loop.index0 > ns.last_query_index %} {%- if loop.last or (not loop.last and reasoning_content) %} {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip('\\n') + '\\n</think>\\n\\n' + content.lstrip('\\n') }} {%- else %} {{- '<|im_start|>' + message.role + '\\n' + content }} {%- endif %} {%- else %} {{- '<|im_start|>' + message.role + '\\n' + content }} {%- endif %} {%- if message.tool_calls %} {%- for tool_call in message.tool_calls %} {%- if (loop.first and content) or (not loop.first) %} {{- '\\n' }} {%- endif %} {%- if tool_call.function %} {%- set tool_call = tool_call.function %} {%- endif %} {{- '<tool_call>\\n{\"name\": \"' }} {{- tool_call.name }} {{- '\", \"arguments\": ' }} {%- if tool_call.arguments is string %} {{- tool_call.arguments }} {%- else %} {{- tool_call.arguments | tojson }} {%- endif %} {{- '}\\n</tool_call>' }} {%- endfor %} {%- endif %} {{- '<|im_end|>\\n' }} {%- elif message.role == \"tool\" %} {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %} {{- '<|im_start|>user' }} {%- endif %} {{- '\\n<tool_response>\\n' }} {{- content }} {{- '\\n</tool_response>' }} {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %} {{- '<|im_end|>\\n' }} {%- endif %} {%- endif %} {%- endfor %} {%- if add_generation_prompt %} {{- '<|im_start|>assistant\\n' }} {{- '<think>\\n\\n</think>\\n\\n' }} {%- endif %}",
        "context_length": 40960,
        "eos_token": "<|im_end|>",
        "quantize_imatrix_file": "imatrix.dat",
        "total": 4022468096
      },
      "id": "Menlo/Jan-nano-gguf",
      "lastModified": "2025-06-13T16:57:55.000Z",
      "likes": 3,
      "model-index": null,
      "modelId": "Menlo/Jan-nano-gguf",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "a04aab0878648d8f284c63a52664a482ead16f06",
      "siblings": [
        {
          "rfilename": ".gitattributes",
          "size": 3460
        },
        {
          "rfilename": "README.md",
          "size": 776
        },
        {
          "rfilename": "jan-nano-4b-iQ4_XS.gguf",
          "size": 2270750400
        },
        {
          "rfilename": "jan-nano-4b-Q3_K_L.gguf",
          "size": 2239784384
        },
        {
          "rfilename": "jan-nano-4b-Q3_K_M.gguf",
          "size": 2075616704
        },
        {
          "rfilename": "jan-nano-4b-Q3_K_S.gguf",
          "size": 1886995904
        },
        {
          "rfilename": "jan-nano-4b-Q4_0.gguf",
          "size": 2369545664
        },
        {
          "rfilename": "jan-nano-4b-Q4_1.gguf",
          "size": 2596627904
        },
        {
          "rfilename": "jan-nano-4b-Q4_K_M.gguf",
          "size": 2497279424
        },
        {
          "rfilename": "jan-nano-4b-Q4_K_S.gguf",
          "size": 2383308224
        },
        {
          "rfilename": "jan-nano-4b-Q5_0.gguf",
          "size": 2823710144
        },
        {
          "rfilename": "jan-nano-4b-Q5_1.gguf",
          "size": 3050792384
        },
        {
          "rfilename": "jan-nano-4b-Q5_K_M.gguf",
          "size": 2889512384
        },
        {
          "rfilename": "jan-nano-4b-Q5_K_S.gguf",
          "size": 2823710144
        },
        {
          "rfilename": "jan-nano-4b-Q6_K.gguf",
          "size": 3306259904
        },
        {
          "rfilename": "jan-nano-4b-Q8_0.gguf",
          "size": 4280403904
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "text-generation",
        "license:apache-2.0",
        "endpoints_compatible",
        "region:us",
        "imatrix",
        "conversational"
      ],
      "usedStorage": 93538518464,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "Menlo:Jan-nano:jan-nano-4b-iQ4_XS.gguf",
        "size": 2270750400
      },
      {
        "id": "Menlo:Jan-nano:jan-nano-4b-Q3_K_L.gguf",
        "size": 2239784384
      },
      {
        "id": "Menlo:Jan-nano:jan-nano-4b-Q3_K_M.gguf",
        "size": 2075616704
      },
      {
        "id": "Menlo:Jan-nano:jan-nano-4b-Q3_K_S.gguf",
        "size": 1886995904
      },
      {
        "id": "Menlo:Jan-nano:jan-nano-4b-Q4_0.gguf",
        "size": 2369545664
      },
      {
        "id": "Menlo:Jan-nano:jan-nano-4b-Q4_1.gguf",
        "size": 2596627904
      },
      {
        "id": "Menlo:Jan-nano:jan-nano-4b-Q4_K_M.gguf",
        "size": 2497279424
      },
      {
        "id": "Menlo:Jan-nano:jan-nano-4b-Q4_K_S.gguf",
        "size": 2383308224
      },
      {
        "id": "Menlo:Jan-nano:jan-nano-4b-Q5_0.gguf",
        "size": 2823710144
      },
      {
        "id": "Menlo:Jan-nano:jan-nano-4b-Q5_1.gguf",
        "size": 3050792384
      },
      {
        "id": "Menlo:Jan-nano:jan-nano-4b-Q5_K_M.gguf",
        "size": 2889512384
      },
      {
        "id": "Menlo:Jan-nano:jan-nano-4b-Q5_K_S.gguf",
        "size": 2823710144
      },
      {
        "id": "Menlo:Jan-nano:jan-nano-4b-Q6_K.gguf",
        "size": 3306259904
      },
      {
        "id": "Menlo:Jan-nano:jan-nano-4b-Q8_0.gguf",
        "size": 4280403904
      }
    ]
  },
  {
    "author": "PrimeIntellect",
    "id": "cortexso/intellect-2",
    "metadata": {
      "_id": "6821ac2482ae7d76d34abdb8",
      "author": "cortexso",
      "cardData": {
        "license": "apache-2.0",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp", "featured"]
      },
      "createdAt": "2025-05-12T08:07:00.000Z",
      "description": "---\nlicense: apache-2.0\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n- featured\n---\n\n## Overview\n\n**Prime Intellect** released **INTELLECT-2**, a 32 billion parameter large language model (LLM) trained through distributed reinforcement learning on globally donated GPU resources. Built on the **Qwen2** architecture and fine-tuned with the **prime-rl** framework, INTELLECT-2 demonstrates strong performance in math, coding, and logical reasoning.\n\nThis model leverages GRPO (Generalized Reinforcement Policy Optimization) over verifiable rewards, introducing asynchronous distributed RL training with enhanced stability techniques. While its primary focus was on verifiable mathematical and coding tasks, it remains compatible with general-purpose text generation tasks.\n\n## Variants\n\n### INTELLECT-2\n\n| No | Variant                                                                         | Branch | Cortex CLI command                 |\n|----|----------------------------------------------------------------------------------|--------|-----------------------------------|\n| 1  | [INTELLECT-2 (32B)](https://huggingface.co/cortexso/intellect-2/tree/32b) | 32b    | `cortex run intellect-2:32b`      |\n\nEach branch includes multiple GGUF quantized versions, optimized for various hardware configurations:\n- **INTELLECT-2-32B:** q2_k, q3_k_l, q3_k_m, q3_k_s, q4_k_m, q4_k_s, q5_k_m, q5_k_s, q6_k, q8_0\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n   ```bash\n   cortexso/intellect-2\n   ```\n\n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n   ```bash\n   cortex run intellect-2\n   ```\n\n## Credits\n\n- **Author:** Prime Intellect\n- **Converter:** [Menlo Research](https://menlo.ai/)\n- **Original License:** [Apache-2.0](https://choosealicense.com/licenses/apache-2.0/)\n- **Paper:** [Intellect 2 Technical Report](https://storage.googleapis.com/public-technical-paper/INTELLECT_2_Technical_Report.pdf)",
      "disabled": false,
      "downloads": 1436,
      "gated": false,
      "gguf": {
        "architecture": "qwen2",
        "bos_token": "<|endoftext|>",
        "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- '' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n  {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" and not message.tool_calls %}\n        {%- set content = message.content %}\n        {%- if not loop.last %}\n            {%- set content = message.content.split('</think>')[-1].lstrip('\\n') %}\n        {%- endif %}\n        {{- '<|im_start|>' + message.role + '\\n' + content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set content = message.content %}\n        {%- if not loop.last %}\n            {%- set content = message.content.split('</think>')[-1].lstrip('\\n') %}\n        {%- endif %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n<think>\\n' }}\n{%- endif %}\n",
        "context_length": 40960,
        "eos_token": "<|im_end|>",
        "total": 32763876352
      },
      "id": "cortexso/intellect-2",
      "lastModified": "2025-05-12T14:18:35.000Z",
      "likes": 0,
      "model-index": null,
      "modelId": "cortexso/intellect-2",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "9d237b26053af28e0119331e0dfbc75b45a0317b",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "intellect-2-q2_k.gguf"
        },
        {
          "rfilename": "intellect-2-q3_k_l.gguf"
        },
        {
          "rfilename": "intellect-2-q3_k_m.gguf"
        },
        {
          "rfilename": "intellect-2-q3_k_s.gguf"
        },
        {
          "rfilename": "intellect-2-q4_k_m.gguf"
        },
        {
          "rfilename": "intellect-2-q4_k_s.gguf"
        },
        {
          "rfilename": "intellect-2-q5_k_m.gguf"
        },
        {
          "rfilename": "intellect-2-q5_k_s.gguf"
        },
        {
          "rfilename": "intellect-2-q6_k.gguf"
        },
        {
          "rfilename": "intellect-2-q8_0.gguf"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "featured",
        "text-generation",
        "license:apache-2.0",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 206130755200,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "intellect-2:32b",
        "size": 19851336256
      }
    ]
  },
  {
    "author": "Microsoft",
    "id": "cortexso/phi-4-reasoning",
    "metadata": {
      "_id": "681857cda178d73748a1295f",
      "author": "cortexso",
      "cardData": {
        "license": "mit",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp", "featured"]
      },
      "createdAt": "2025-05-05T06:16:45.000Z",
      "description": "---\nlicense: mit\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n- featured\n---\n\n## Overview\n\n**Microsoft Research** developed and released the **Phi-4-reasoning** series, a cutting-edge family of reasoning-focused language models optimized for chain-of-thought (CoT), step-by-step problem solving, and high-efficiency inference. These models excel in advanced mathematical reasoning, scientific Q&A, and instruction-following scenarios.\n\nThe Phi-4 models introduce extended context lengths, ChatML reasoning templates, and strong performance on benchmark datasets, while maintaining compact sizes that are ideal for memory- and latency-constrained environments.\n\n## Variants\n\n### Phi-4-reasoning\n\n| No | Variant                                                                            | Branch     | Cortex CLI command                   |\n|----|-------------------------------------------------------------------------------------|------------|-------------------------------------|\n| 1  | [phi-4-mini-reasoning](https://huggingface.co/microsoft/phi-4-mini-reasoning)      | 4b         | `cortex run phi4:4b`                |\n| 2  | [phi-4-reasoning](https://huggingface.co/microsoft/phi-4-reasoning-plus)      | 14b   | `cortex run phi4:14b`          |\n| 3  | [phi-4-reasoning-plus](https://huggingface.co/microsoft/phi-4-reasoning-plus)      | 14b-plus   | `cortex run phi4:14b-plus`          |\n\nEach branch supports multiple quantized GGUF versions:\n- **phi-4-mini-reasoning:** q2_k, q3_k_l, q3_k_m, q3_k_s, q4_k_m, q4_k_s, q5_k_m, q5_k_s, q6_k, q8_0\n- **phi-4-reasoning:** q2_k, q3_k_l, q3_k_m, q3_k_s, q4_k_m, q4_k_s, q5_k_m, q5_k_s, q6_k, q8_0\n- **phi-4-reasoning-plus:** q2_k, q3_k_l, q3_k_m, q3_k_s, q4_k_m, q4_k_s, q5_k_m, q5_k_s, q6_k, q8_0\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n   ```bash\n   cortexso/phi4\n   ```\n\n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n   ```bash\n   cortex run phi4\n   ```\n\n## Credits\n\n- **Author:** Microsoft Research\n- **Converter:** [Menlo Research](https://menlo.ai/)\n- **Original License:** [MIT License](https://opensource.org/license/mit/)\n- **Blogs:** [Phi-4 Reasoning Blog](https://www.microsoft.com/en-us/research/blog/)\n",
      "disabled": false,
      "downloads": 2894,
      "gated": false,
      "gguf": {
        "architecture": "phi3",
        "bos_token": "<|endoftext|>",
        "chat_template": "{{ '<|system|>Your name is Phi, an AI math expert developed by Microsoft.' }}{% for message in messages %}{% if message['role'] == 'system' %} {{ message['content'] }}{% if 'tools' in message and message['tools'] is not none %}{{ '<|tool|>' + message['tools'] + '<|/tool|>' }}{% endif %}{% endif %}{% endfor %}{{ '<|end|>' }}{% for message in messages %}{% if message['role'] != 'system' %}{{ '<|' + message['role'] + '|>' + message['content'] + '<|end|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>' }}{% else %}{{ eos_token }}{% endif %}",
        "context_length": 131072,
        "eos_token": "<|endoftext|>",
        "total": 3836021856
      },
      "id": "cortexso/phi-4-reasoning",
      "lastModified": "2025-05-05T09:36:18.000Z",
      "likes": 0,
      "model-index": null,
      "modelId": "cortexso/phi-4-reasoning",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "218f08078412d1bcd46e7ce48c4442b14b98164d",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        },
        {
          "rfilename": "phi-4-mini-reasoning-q2_k.gguf"
        },
        {
          "rfilename": "phi-4-mini-reasoning-q3_k_l.gguf"
        },
        {
          "rfilename": "phi-4-mini-reasoning-q3_k_m.gguf"
        },
        {
          "rfilename": "phi-4-mini-reasoning-q3_k_s.gguf"
        },
        {
          "rfilename": "phi-4-mini-reasoning-q4_k_m.gguf"
        },
        {
          "rfilename": "phi-4-mini-reasoning-q4_k_s.gguf"
        },
        {
          "rfilename": "phi-4-mini-reasoning-q5_k_m.gguf"
        },
        {
          "rfilename": "phi-4-mini-reasoning-q5_k_s.gguf"
        },
        {
          "rfilename": "phi-4-mini-reasoning-q6_k.gguf"
        },
        {
          "rfilename": "phi-4-mini-reasoning-q8_0.gguf"
        },
        {
          "rfilename": "phi-4-reasoning-plus-q2_k.gguf"
        },
        {
          "rfilename": "phi-4-reasoning-plus-q3_k_l.gguf"
        },
        {
          "rfilename": "phi-4-reasoning-plus-q3_k_m.gguf"
        },
        {
          "rfilename": "phi-4-reasoning-plus-q3_k_s.gguf"
        },
        {
          "rfilename": "phi-4-reasoning-plus-q4_k_m.gguf"
        },
        {
          "rfilename": "phi-4-reasoning-plus-q4_k_s.gguf"
        },
        {
          "rfilename": "phi-4-reasoning-plus-q5_k_m.gguf"
        },
        {
          "rfilename": "phi-4-reasoning-plus-q5_k_s.gguf"
        },
        {
          "rfilename": "phi-4-reasoning-plus-q6_k.gguf"
        },
        {
          "rfilename": "phi-4-reasoning-plus-q8_0.gguf"
        },
        {
          "rfilename": "phi-4-reasoning-q2_k.gguf"
        },
        {
          "rfilename": "phi-4-reasoning-q3_k_l.gguf"
        },
        {
          "rfilename": "phi-4-reasoning-q3_k_m.gguf"
        },
        {
          "rfilename": "phi-4-reasoning-q3_k_s.gguf"
        },
        {
          "rfilename": "phi-4-reasoning-q4_k_m.gguf"
        },
        {
          "rfilename": "phi-4-reasoning-q4_k_s.gguf"
        },
        {
          "rfilename": "phi-4-reasoning-q5_k_m.gguf"
        },
        {
          "rfilename": "phi-4-reasoning-q5_k_s.gguf"
        },
        {
          "rfilename": "phi-4-reasoning-q6_k.gguf"
        },
        {
          "rfilename": "phi-4-reasoning-q8_0.gguf"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "featured",
        "text-generation",
        "license:mit",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 212004788352,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "phi-4-reasoning:14b",
        "size": 9053115968
      },
      {
        "id": "phi-4-reasoning:4b",
        "size": 2491874464
      },
      {
        "id": "phi-4-reasoning:14b-plus",
        "size": 9053116000
      }
    ]
  },
  {
    "author": "Internlm",
    "id": "cortexso/internlm3-8b-it",
    "metadata": {
      "_id": "678dcf22fbe4dceca4562d1f",
      "author": "cortexso",
      "cardData": {
        "license": "apache-2.0",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2025-01-20T04:20:50.000Z",
      "description": "---\nlicense: apache-2.0\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n## Overview\n\n**InternLM** developed and released the [InternLM3-8B-Instruct](https://huggingface.co/internlm/InternLM3-8B-Instruct), an 8-billion parameter instruction-tuned language model designed for general-purpose usage and advanced reasoning tasks. The model delivers state-of-the-art performance on reasoning and knowledge-intensive tasks, outperforming other models like Llama3.1-8B and Qwen2.5-7B. Trained on 4 trillion high-quality tokens, InternLM3 achieves exceptional efficiency, reducing training costs by over 75% compared to other models of similar scale. \n\nThe model features dual operational modes: a deep thinking mode for solving complex reasoning tasks through long chain-of-thought processes and a normal response mode for fluent and interactive user experiences. These capabilities make InternLM3-8B-Instruct ideal for applications in conversational AI, advanced reasoning, and general-purpose language understanding.\n\n## Variants\n\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Internlm3-8b-it](https://huggingface.co/cortexso/internlm3-8b-it/tree/8b) | `cortex run internlm3-8b-it:8b` |\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexso/internlm3-8b-it\n    ```\n\n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run internlm3-8b-it\n    ```\n\n## Credits\n\n- **Author:** InternLM\n- **Converter:** [Homebrew](https://www.homebrew.ltd/)\n- **Original License:** [License](https://huggingface.co/internlm/internlm3-8b-instruct/blob/main/LICENSE.txt)\n- **Papers:** [InternLM2 Technical Report](https://arxiv.org/abs/2403.17297)",
      "disabled": false,
      "downloads": 229,
      "gated": false,
      "gguf": {
        "architecture": "llama",
        "bos_token": "<s>",
        "chat_template": "{{ bos_token }}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}",
        "context_length": 32768,
        "eos_token": "<|im_end|>",
        "total": 8804241408
      },
      "id": "cortexso/internlm3-8b-it",
      "lastModified": "2025-03-03T05:57:41.000Z",
      "likes": 0,
      "model-index": null,
      "modelId": "cortexso/internlm3-8b-it",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "957eb6aa16a10eda3ce1a87dcacfd99bda5c469a",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "internlm3-8b-instruct-q2_k.gguf"
        },
        {
          "rfilename": "internlm3-8b-instruct-q3_k_l.gguf"
        },
        {
          "rfilename": "internlm3-8b-instruct-q3_k_m.gguf"
        },
        {
          "rfilename": "internlm3-8b-instruct-q3_k_s.gguf"
        },
        {
          "rfilename": "internlm3-8b-instruct-q4_k_m.gguf"
        },
        {
          "rfilename": "internlm3-8b-instruct-q4_k_s.gguf"
        },
        {
          "rfilename": "internlm3-8b-instruct-q5_k_m.gguf"
        },
        {
          "rfilename": "internlm3-8b-instruct-q5_k_s.gguf"
        },
        {
          "rfilename": "internlm3-8b-instruct-q6_k.gguf"
        },
        {
          "rfilename": "internlm3-8b-instruct-q8_0.gguf"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "arxiv:2403.17297",
        "license:apache-2.0",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 56027406208,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "internlm3-8b-it:8b",
        "size": 5358623936
      }
    ]
  },
  {
    "author": "Google",
    "id": "cortexso/gemma3",
    "metadata": {
      "_id": "67d14a4c2e461dfe226bd1be",
      "author": "cortexso",
      "cardData": {
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp", "featured"]
      },
      "createdAt": "2025-03-12T08:48:12.000Z",
      "description": "---\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n- featured\n---\n## Overview\n**Google** developed and released the **Gemma 3** series, featuring multiple model sizes with both pre-trained and instruction-tuned variants. These multimodal models handle both text and image inputs while generating text outputs, making them versatile for various applications. Gemma 3 models are built from the same research and technology used to create the Gemini models, offering state-of-the-art capabilities in a lightweight and accessible format.\n\nThe Gemma 3 models include four different sizes with open weights, providing excellent performance across tasks like question answering, summarization, and reasoning while maintaining efficiency for deployment in resource-constrained environments such as laptops, desktops, or custom cloud infrastructure.\n\n## Variants\n\n### Gemma 3\n| No | Variant                                                | Branch | Cortex CLI command            |\n| -- | ------------------------------------------------------ | ------ | ----------------------------- |\n| 1  | [Gemma-3-1B](https://huggingface.co/cortexso/gemma3/tree/1b)   | 1b     | `cortex run gemma3:1b`        |\n| 2  | [Gemma-3-4B](https://huggingface.co/cortexso/gemma3/tree/4b)   | 4b     | `cortex run gemma3:4b`        |\n| 3  | [Gemma-3-12B](https://huggingface.co/cortexso/gemma3/tree/12b) | 12b    | `cortex run gemma3:12b`       |\n| 4  | [Gemma-3-27B](https://huggingface.co/cortexso/gemma3/tree/27b) | 27b    | `cortex run gemma3:27b`       |\n\nEach branch contains a default quantized version.\n\n### Key Features\n- **Multimodal capabilities**: Handles both text and image inputs\n- **Large context window**: 128K tokens\n- **Multilingual support**: Over 140 languages\n- **Available in multiple sizes**: From 1B to 27B parameters\n- **Open weights**: For both pre-trained and instruction-tuned variants\n\n## Use it with Jan (UI)\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n   ```bash\n   cortexso/gemma3\n   ```\n\n## Use it with Cortex (CLI)\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n   ```bash\n   cortex run gemma3\n   ```\n\n## Credits\n- **Author:** Google\n- **Original License:** [Gemma License](https://ai.google.dev/gemma/terms)\n- **Papers:** [Gemma 3 Technical Report](https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf)",
      "disabled": false,
      "downloads": 5425,
      "gated": false,
      "gguf": {
        "architecture": "gemma3",
        "bos_token": "<bos>",
        "chat_template": "{{ bos_token }}\n{%- if messages[0]['role'] == 'system' -%}\n    {%- if messages[0]['content'] is string -%}\n        {%- set first_user_prefix = messages[0]['content'] + '\n\n' -%}\n    {%- else -%}\n        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n\n' -%}\n    {%- endif -%}\n    {%- set loop_messages = messages[1:] -%}\n{%- else -%}\n    {%- set first_user_prefix = \"\" -%}\n    {%- set loop_messages = messages -%}\n{%- endif -%}\n{%- for message in loop_messages -%}\n    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n    {%- endif -%}\n    {%- if (message['role'] == 'assistant') -%}\n        {%- set role = \"model\" -%}\n    {%- else -%}\n        {%- set role = message['role'] -%}\n    {%- endif -%}\n    {{ '<start_of_turn>' + role + '\n' + (first_user_prefix if loop.first else \"\") }}\n    {%- if message['content'] is string -%}\n        {{ message['content'] | trim }}\n    {%- elif message['content'] is iterable -%}\n        {%- for item in message['content'] -%}\n            {%- if item['type'] == 'image' -%}\n                {{ '<start_of_image>' }}\n            {%- elif item['type'] == 'text' -%}\n                {{ item['text'] | trim }}\n            {%- endif -%}\n        {%- endfor -%}\n    {%- else -%}\n        {{ raise_exception(\"Invalid content type\") }}\n    {%- endif -%}\n    {{ '<end_of_turn>\n' }}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n    {{'<start_of_turn>model\n'}}\n{%- endif -%}\n",
        "context_length": 131072,
        "eos_token": "<eos>",
        "total": 11765788416
      },
      "id": "cortexso/gemma3",
      "lastModified": "2025-05-13T12:45:28.000Z",
      "likes": 1,
      "model-index": null,
      "modelId": "cortexso/gemma3",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "289bd96e0dbb2f82e77c56c9c09d66ff76769895",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "gemma-3-12b-it-q2_k.gguf"
        },
        {
          "rfilename": "gemma-3-12b-it-q3_k_l.gguf"
        },
        {
          "rfilename": "gemma-3-12b-it-q3_k_m.gguf"
        },
        {
          "rfilename": "gemma-3-12b-it-q3_k_s.gguf"
        },
        {
          "rfilename": "gemma-3-12b-it-q4_k_m.gguf"
        },
        {
          "rfilename": "gemma-3-12b-it-q4_k_s.gguf"
        },
        {
          "rfilename": "gemma-3-12b-it-q5_k_m.gguf"
        },
        {
          "rfilename": "gemma-3-12b-it-q5_k_s.gguf"
        },
        {
          "rfilename": "gemma-3-12b-it-q6_k.gguf"
        },
        {
          "rfilename": "gemma-3-12b-it-q8_0.gguf"
        },
        {
          "rfilename": "gemma-3-1b-it-q2_k.gguf"
        },
        {
          "rfilename": "gemma-3-1b-it-q3_k_l.gguf"
        },
        {
          "rfilename": "gemma-3-1b-it-q3_k_m.gguf"
        },
        {
          "rfilename": "gemma-3-1b-it-q3_k_s.gguf"
        },
        {
          "rfilename": "gemma-3-1b-it-q4_k_m.gguf"
        },
        {
          "rfilename": "gemma-3-1b-it-q4_k_s.gguf"
        },
        {
          "rfilename": "gemma-3-1b-it-q5_k_m.gguf"
        },
        {
          "rfilename": "gemma-3-1b-it-q5_k_s.gguf"
        },
        {
          "rfilename": "gemma-3-1b-it-q6_k.gguf"
        },
        {
          "rfilename": "gemma-3-1b-it-q8_0.gguf"
        },
        {
          "rfilename": "gemma-3-27b-it-q2_k.gguf"
        },
        {
          "rfilename": "gemma-3-27b-it-q3_k_l.gguf"
        },
        {
          "rfilename": "gemma-3-27b-it-q3_k_m.gguf"
        },
        {
          "rfilename": "gemma-3-27b-it-q3_k_s.gguf"
        },
        {
          "rfilename": "gemma-3-27b-it-q4_k_m.gguf"
        },
        {
          "rfilename": "gemma-3-27b-it-q4_k_s.gguf"
        },
        {
          "rfilename": "gemma-3-27b-it-q5_k_m.gguf"
        },
        {
          "rfilename": "gemma-3-27b-it-q5_k_s.gguf"
        },
        {
          "rfilename": "gemma-3-27b-it-q6_k.gguf"
        },
        {
          "rfilename": "gemma-3-27b-it-q8_0.gguf"
        },
        {
          "rfilename": "gemma-3-4b-it-q2_k.gguf"
        },
        {
          "rfilename": "gemma-3-4b-it-q3_k_l.gguf"
        },
        {
          "rfilename": "gemma-3-4b-it-q3_k_m.gguf"
        },
        {
          "rfilename": "gemma-3-4b-it-q3_k_s.gguf"
        },
        {
          "rfilename": "gemma-3-4b-it-q4_k_m.gguf"
        },
        {
          "rfilename": "gemma-3-4b-it-q4_k_s.gguf"
        },
        {
          "rfilename": "gemma-3-4b-it-q5_k_m.gguf"
        },
        {
          "rfilename": "gemma-3-4b-it-q5_k_s.gguf"
        },
        {
          "rfilename": "gemma-3-4b-it-q6_k.gguf"
        },
        {
          "rfilename": "gemma-3-4b-it-q8_0.gguf"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "featured",
        "text-generation",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 280561347040,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "gemma3:4b",
        "size": 2489757760
      },
      {
        "id": "gemma3:27b",
        "size": 16546404640
      },
      {
        "id": "gemma3:12b",
        "size": 7300574912
      },
      {
        "id": "gemma3:1b",
        "size": 806058144
      }
    ]
  },
  {
    "author": "Qwen",
    "id": "cortexso/qwen-qwq",
    "metadata": {
      "_id": "67c909487c87605263db5352",
      "author": "cortexso",
      "cardData": {
        "license": "apache-2.0",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp", "featured"]
      },
      "createdAt": "2025-03-06T02:32:40.000Z",
      "description": "---\nlicense: apache-2.0\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n- featured\n---\n\n## Overview  \n\n**QwQ** is the reasoning model of the **Qwen** series. Unlike conventional instruction-tuned models, **QwQ** is designed to think and reason, achieving significantly enhanced performance in downstream tasks, especially challenging problem-solving scenarios.  \n\n**QwQ-32B** is the **medium-sized** reasoning model in the QwQ family, capable of **competitive performance** against state-of-the-art reasoning models, such as **DeepSeek-R1** and **o1-mini**. It is optimized for tasks requiring logical deduction, multi-step reasoning, and advanced comprehension.  \n\nThe model is well-suited for **AI research, automated theorem proving, advanced dialogue systems, and high-level decision-making applications**.  \n\n## Variants  \n\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [QwQ-32B](https://huggingface.co/cortexso/qwen-qwq/tree/main) | `cortex run qwen-qwq:32b` |  \n\n## Use it with Jan (UI)  \n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)  \n2. Use in Jan model Hub:  \n    ```bash\n    cortexso/qwen-qwq\n    ```  \n\n## Use it with Cortex (CLI)  \n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)  \n2. Run the model with command:  \n    ```bash\n    cortex run qwen-qwq\n    ```  \n\n## Credits  \n\n- **Author:** Qwen Team  \n- **Converter:** [Homebrew](https://www.homebrew.ltd/)  \n- **Original License:** [License](https://choosealicense.com/licenses/apache-2.0/)  \n- **Paper:** [Introducing QwQ-32B: The Medium-Sized Reasoning Model](https://qwenlm.github.io/blog/qwq-32b/)",
      "disabled": false,
      "downloads": 582,
      "gated": false,
      "gguf": {
        "architecture": "qwen2",
        "bos_token": "<|endoftext|>",
        "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- '' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n  {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" and not message.tool_calls %}\n        {%- set content = message.content.split('</think>')[-1].lstrip('\\n') %}\n        {{- '<|im_start|>' + message.role + '\\n' + content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set content = message.content.split('</think>')[-1].lstrip('\\n') %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n<think>\\n' }}\n{%- endif %}\n",
        "context_length": 131072,
        "eos_token": "<|im_end|>",
        "total": 32763876352
      },
      "id": "cortexso/qwen-qwq",
      "lastModified": "2025-03-13T02:39:51.000Z",
      "likes": 0,
      "model-index": null,
      "modelId": "cortexso/qwen-qwq",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "17e393edf64f5ecca3089b4b5822d05a165882bd",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        },
        {
          "rfilename": "qwq-32b-q2_k.gguf"
        },
        {
          "rfilename": "qwq-32b-q3_k_l.gguf"
        },
        {
          "rfilename": "qwq-32b-q3_k_m.gguf"
        },
        {
          "rfilename": "qwq-32b-q3_k_s.gguf"
        },
        {
          "rfilename": "qwq-32b-q4_k_m.gguf"
        },
        {
          "rfilename": "qwq-32b-q4_k_s.gguf"
        },
        {
          "rfilename": "qwq-32b-q5_k_m.gguf"
        },
        {
          "rfilename": "qwq-32b-q5_k_s.gguf"
        },
        {
          "rfilename": "qwq-32b-q6_k.gguf"
        },
        {
          "rfilename": "qwq-32b-q8_0.gguf"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "featured",
        "text-generation",
        "license:apache-2.0",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 206130754880,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "qwen-qwq:32b",
        "size": 19851336224
      }
    ]
  },
  {
    "author": "DeepCogito",
    "id": "cortexso/cogito-v1",
    "metadata": {
      "_id": "67f67ca2c68bea1f264edc11",
      "author": "cortexso",
      "cardData": {
        "license": "apache-2.0",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp", "featured"]
      },
      "createdAt": "2025-04-09T13:56:50.000Z",
      "description": "---\nlicense: apache-2.0\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n- featured\n---\n\n## Overview\n\n**DeepCogito** introduces the **Cogito-v1 Preview** series, a powerful suite of hybrid reasoning models trained with Iterated Distillation and Amplification (IDA). These models are designed to push the boundaries of open-weight LLMs through scalable alignment and self-improvement strategies, offering unmatched performance across coding, STEM, multilingual, and agentic use cases.\n\nEach model in this series operates in both **standard** (direct answer) and **reasoning** (self-reflective) modes, significantly outperforming size-equivalent open models such as LLaMA, DeepSeek, and Qwen. The 70B variant notably surpasses the newly released LLaMA 4 109B MoE model in benchmarks.\n\n## Variants\n\n### Cogito-v1 Preview\n\n| No | Variant                                                                                         | Branch | Cortex CLI command                            |\n|----|--------------------------------------------------------------------------------------------------|--------|-----------------------------------------------|\n| 1  | [Cogito-v1-Preview-LLaMA-3B](https://huggingface.co/cortexso/cogito-v1/tree/3b)       | 3b     | `cortex run cognito-v1:3b`                     |\n| 2  | [Cogito-v1-Preview-LLaMA-8B](https://huggingface.co/cortexso/cogito-v1/tree/8b)       | 8b     | `cortex run cognito-v1:8b`                     |\n| 3  | [Cogito-v1-Preview-Qwen-14B](https://huggingface.co/cortexso/cogito-v1/tree/14b)       | 14b    | `cortex run cognito-v1:14b`                    |\n| 4  | [Cogito-v1-Preview-Qwen-32B](https://huggingface.co/cortexso/cogito-v1/tree/32b)       | 32b    | `cortex run cognito-v1:32b`                    |\n| 5  | [Cogito-v1-Preview-LLaMA-70B](https://huggingface.co/cortexso/cogito-v1/tree/70b)     | 70b    | `cortex run cognito-v1:70b`                    |\n\nEach branch contains a default quantized version:\n- **LLaMA-3B:** q4-km  \n- **LLaMA-8B:** q4-km  \n- **Qwen-14B:** q4-km  \n- **Qwen-32B:** q4-km  \n- **LLaMA-70B:** q4-km  \n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)  \n2. Use in Jan model Hub:  \n   ```bash\n   deepcogito/cognito-v1\n   ```\n## Use it with Cortex (CLI)\n\n1. Install Cortex using [Quickstart](https://cortex.so/)\n2. Run the model with command:\n  ```bash\n  cortex run cognito-v1\n  ```\n\n## Credits\n\n- **Author:** DeepCogito\n- **Original License:** [Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0)\n- **Papers:** [Cognito v1 Preview](https://www.deepcogito.com/research/cogito-v1-preview)",
      "disabled": false,
      "downloads": 4045,
      "gated": false,
      "gguf": {
        "architecture": "llama",
        "bos_token": "<|begin_of_text|>",
        "chat_template": "{{- bos_token }}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n{%- if not enable_thinking is defined %}\n    {%- set enable_thinking = false %}\n{%- endif %}\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = \"\" %}\n{%- endif %}\n{#- Set the system message. If enable_thinking is true, add the \"Enable deep thinking subroutine.\" #}\n{%- if enable_thinking %}\n    {%- if system_message != \"\" %}\n        {%- set system_message = \"Enable deep thinking subroutine.\n\n\" ~ system_message %}\n    {%- else %}\n        {%- set system_message = \"Enable deep thinking subroutine.\" %}\n    {%- endif %}\n{%- endif %}\n{#- Set the system message. In case there are tools present, add them to the system message. #}\n{%- if tools is not none or system_message != '' %}\n    {{- \"<|start_header_id|>system<|end_header_id|>\n\n\" }}\n    {{- system_message }}\n    {%- if tools is not none %}\n        {%- if system_message != \"\" %}\n            {{- \"\n\n\" }}\n        {%- endif %}\n        {{- \"Available Tools:\n\" }}\n        {%- for t in tools %}\n            {{- t | tojson(indent=4) }}\n            {{- \"\n\n\" }}\n        {%- endfor %}\n    {%- endif %}\n    {{- \"<|eot_id|>\" }}\n{%- endif %}\n\n{#- Rest of the messages #}\n{%- for message in messages %}\n    {#- The special cases are when the message is from a tool (via role ipython/tool/tool_results) or when the message is from the assistant, but has \"tool_calls\". If not, we add the message directly as usual. #}\n    {#- Case 1 - Usual, non tool related message. #}\n    {%- if not (message.role == \"ipython\" or message.role == \"tool\" or message.role == \"tool_results\" or (message.tool_calls is defined and message.tool_calls is not none)) %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n' }}\n        {%- if message['content'] is string %}\n            {{- message['content'] | trim }}\n        {%- else %}\n            {%- for item in message['content'] %}\n                {%- if item.type == 'text' %}\n                    {{- item.text | trim }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {{- '<|eot_id|>' }}\n    \n    {#- Case 2 - the response is from the assistant, but has a tool call returned. The assistant may also have returned some content along with the tool call. #}\n    {%- elif message.tool_calls is defined and message.tool_calls is not none %}\n        {{- \"<|start_header_id|>assistant<|end_header_id|>\n\n\" }}\n        {%- if message['content'] is string %}\n            {{- message['content'] | trim }}\n        {%- else %}\n            {%- for item in message['content'] %}\n                {%- if item.type == 'text' %}\n                    {{- item.text | trim }}\n                    {%- if item.text | trim != \"\" %}\n                        {{- \"\n\n\" }}\n                    {%- endif %}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {{- \"[\" }}\n        {%- for tool_call in message.tool_calls %}\n            {%- set out = tool_call.function|tojson %}\n            {%- if not tool_call.id is defined %}\n                {{- out }}\n            {%- else %}\n                {{- out[:-1] }}\n                {{- ', \"id\": \"' + tool_call.id + '\"}' }}\n            {%- endif %}\n            {%- if not loop.last %}\n                {{- \", \" }}\n            {%- else %}\n                {{- \"]<|eot_id|>\" }}\n            {%- endif %}\n        {%- endfor %}\n    \n    {#- Case 3 - the response is from a tool call. The tool call may have an id associated with it as well. If it does, we add it to the prompt. #}\n    {%- elif message.role == \"ipython\" or message[\"role\"] == \"tool_results\" or message[\"role\"] == \"tool\" %}\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\n\n\" }}\n        {%- if message.tool_call_id is defined and message.tool_call_id != '' %}\n            {{- '{\"content\": ' + (message.content | tojson) + ', \"call_id\": \"' + message.tool_call_id + '\"}' }}\n        {%- else %}\n            {{- '{\"content\": ' + (message.content | tojson) + '}' }}\n        {%- endif %}\n        {{- \"<|eot_id|>\" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\n\n' }}\n{%- endif %}",
        "context_length": 131072,
        "eos_token": "<|eot_id|>",
        "total": 3606752320
      },
      "id": "cortexso/cogito-v1",
      "lastModified": "2025-04-10T03:02:13.000Z",
      "likes": 3,
      "model-index": null,
      "modelId": "cortexso/cogito-v1",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "7e55c8c2946b9b48c606431e7a2eaf299c15b80d",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "cogito-v1-preview-llama-3b-q2_k.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-llama-3b-q3_k_l.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-llama-3b-q3_k_m.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-llama-3b-q3_k_s.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-llama-3b-q4_k_m.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-llama-3b-q4_k_s.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-llama-3b-q5_k_m.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-llama-3b-q5_k_s.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-llama-3b-q6_k.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-llama-3b-q8_0.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-llama-70b-q4_k_m.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-llama-8b-q2_k.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-llama-8b-q3_k_l.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-llama-8b-q3_k_m.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-llama-8b-q3_k_s.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-llama-8b-q4_k_m.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-llama-8b-q4_k_s.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-llama-8b-q5_k_m.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-llama-8b-q5_k_s.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-llama-8b-q6_k.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-llama-8b-q8_0.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-qwen-14b-q2_k.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-qwen-14b-q3_k_l.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-qwen-14b-q3_k_m.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-qwen-14b-q3_k_s.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-qwen-14b-q4_k_m.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-qwen-14b-q4_k_s.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-qwen-14b-q5_k_m.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-qwen-14b-q5_k_s.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-qwen-14b-q6_k.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-qwen-14b-q8_0.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-qwen-32b-q2_k.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-qwen-32b-q3_k_l.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-qwen-32b-q3_k_m.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-qwen-32b-q3_k_s.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-qwen-32b-q4_k_m.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-qwen-32b-q4_k_s.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-qwen-32b-q5_k_m.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-qwen-32b-q5_k_s.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-qwen-32b-q6_k.gguf"
        },
        {
          "rfilename": "cogito-v1-preview-qwen-32b-q8_0.gguf"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "featured",
        "text-generation",
        "license:apache-2.0",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 417094614784,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "cogito-v1:8b",
        "size": 4920738752
      },
      {
        "id": "cogito-v1:70b",
        "size": 42520398016
      },
      {
        "id": "cogito-v1:3b",
        "size": 2241004384
      },
      {
        "id": "cogito-v1:32b",
        "size": 19848503488
      },
      {
        "id": "cogito-v1:14b",
        "size": 8985277888
      }
    ]
  },
  {
    "author": "ibm-granite",
    "id": "cortexso/granite-3.2-it",
    "metadata": {
      "_id": "67ab23c8e77c0a1c32f62879",
      "author": "cortexso",
      "cardData": {
        "license": "mit",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2025-02-11T10:17:44.000Z",
      "description": "---\nlicense: mit\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n## Overview\nGranite-3.2-it is an advanced AI language model derived from the IBM Granite framework, specifically designed for instruction-following tasks in Italian. Its primary purpose is to facilitate human-like interactions by understanding and generating responses that are contextually relevant and coherent. This model can be effectively utilized in various applications, including customer support, content creation, and language translation, enhancing communication efficiency across diverse sectors. Its performance demonstrates a strong ability to comprehend nuanced instructions and generate accurate outputs, making it suitable for professional and creative environments alike. Overall, Granite-3.2-it stands out for its adaptability, responsiveness, and proficiency in Italian language tasks.\n## Variants\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Granite-3.2-it-8b](https://huggingface.co/cortexso/granite-3.2-it/tree/8b) | cortex run granite-3.2-it:8b|\n## Use it with Jan (UI)\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexso/granite-3.2-it\n    ```\n## Use it with Cortex (CLI)\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run granite-3.2-it\n    ```\n## Credits\n- **Author:** ibm-granite\n- **Converter:** [Homebrew](https://www.homebrew.ltd/)\n- **Original License:** [License](https://www.apache.org/licenses/LICENSE-2.0)\n- **Paper:** [IBM Granite 3.2 Blog](https://www.ibm.com/new/announcements/ibm-granite-3-2-open-source-reasoning-and-vision)",
      "disabled": false,
      "downloads": 352,
      "gated": false,
      "gguf": {
        "architecture": "granite",
        "bos_token": "<|end_of_text|>",
        "chat_template": "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = \"Knowledge Cutoff Date: April 2024.\nToday's Date: \" + strftime_now('%B %d, %Y') + \".\nYou are Granite, developed by IBM.\" %}\n    {%- if tools and documents %}\n        {%- set system_message = system_message + \" You are a helpful AI assistant with access to the following tools. When a tool is required to answer the user's query, respond with <|tool_call|> followed by a JSON list of tools used. If a tool does not exist in the provided list of tools, notify the user that you do not have the ability to fulfill the request.\n\nWrite the response to the user's input by strictly aligning with the facts in the provided documents. If the information needed to answer the question is not available in the documents, inform the user that the question cannot be answered based on the available data.\" %}\n    {%- elif tools %}\n        {%- set system_message = system_message + \" You are a helpful AI assistant with access to the following tools. When a tool is required to answer the user's query, respond with <|tool_call|> followed by a JSON list of tools used. If a tool does not exist in the provided list of tools, notify the user that you do not have the ability to fulfill the request.\" %}\n    {%- elif documents %}\n        {%- set system_message = system_message + \" Write the response to the user's input by strictly aligning with the facts in the provided documents. If the information needed to answer the question is not available in the documents, inform the user that the question cannot be answered based on the available data.\" %}\n    {%- elif thinking %}\n    {%- set system_message = system_message + \" You are a helpful AI assistant.\nRespond to every user query in a comprehensive and detailed way. You can write down your thoughts and reasoning process before responding. In the thought process, engage in a comprehensive cycle of analysis, summarization, exploration, reassessment, reflection, backtracing, and iteration to develop well-considered thinking process. In the response section, based on various attempts, explorations, and reflections from the thoughts section, systematically present the final solution that you deem correct. The response should summarize the thought process. Write your thoughts after 'Here is my thought process:' and write your response after 'Here is my response:' for each user query.\" %}\n    {%- else %}\n        {%- set system_message = system_message + \" You are a helpful AI assistant.\" %}    \n    {%- endif %}\n    {%- if 'citations' in controls and documents %}\n        {%- set system_message = system_message + '\n\nIn your response, use the symbols <co> and </co> to indicate when a fact comes from a document in the search result, e.g <co>0</co> for a fact from document 0. Afterwards, list all the citations with their corresponding documents in an ordered list.' %}\n    {%- endif %}\n    {%- if 'hallucinations' in controls and documents %}\n        {%- set system_message = system_message + '\n\nFinally, after the response is written, include a numbered list of sentences from the response that are potentially hallucinated and not based in the documents.' %}\n    {%- endif %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n{{- '<|start_of_role|>system<|end_of_role|>' + system_message + '<|end_of_text|>\n' }}\n{%- if tools %}\n    {{- '<|start_of_role|>tools<|end_of_role|>' }}\n    {{- tools | tojson(indent=4) }}\n    {{- '<|end_of_text|>\n' }}\n{%- endif %}\n{%- if documents %}\n    {{- '<|start_of_role|>documents<|end_of_role|>' }}\n    {%- for document in documents %}\n        {{- 'Document ' + loop.index0 | string + '\n' }}\n        {{- document['text'] }}\n        {%- if not loop.last %}\n            {{- '\n\n'}}\n        {%- endif%}\n    {%- endfor %}\n    {{- '<|end_of_text|>\n' }}\n{%- endif %}\n{%- for message in loop_messages %}\n    {{- '<|start_of_role|>' + message['role'] + '<|end_of_role|>' + message['content'] + '<|end_of_text|>\n' }}\n    {%- if loop.last and add_generation_prompt %}\n        {{- '<|start_of_role|>assistant' }}\n            {%- if controls %}\n                {{- ' ' + controls | tojson()}}\n            {%- endif %}\n        {{- '<|end_of_role|>' }}\n    {%- endif %}\n{%- endfor %}",
        "context_length": 131072,
        "eos_token": "<|end_of_text|>",
        "total": 8170848256
      },
      "id": "cortexso/granite-3.2-it",
      "lastModified": "2025-03-03T02:11:18.000Z",
      "likes": 1,
      "model-index": null,
      "modelId": "cortexso/granite-3.2-it",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "2fb3d81e43760500c0ad28f9b7d047c75abc16dd",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "granite-3.2-8b-instruct-q2_k.gguf"
        },
        {
          "rfilename": "granite-3.2-8b-instruct-q3_k_l.gguf"
        },
        {
          "rfilename": "granite-3.2-8b-instruct-q3_k_m.gguf"
        },
        {
          "rfilename": "granite-3.2-8b-instruct-q3_k_s.gguf"
        },
        {
          "rfilename": "granite-3.2-8b-instruct-q4_k_m.gguf"
        },
        {
          "rfilename": "granite-3.2-8b-instruct-q4_k_s.gguf"
        },
        {
          "rfilename": "granite-3.2-8b-instruct-q5_k_m.gguf"
        },
        {
          "rfilename": "granite-3.2-8b-instruct-q5_k_s.gguf"
        },
        {
          "rfilename": "granite-3.2-8b-instruct-q6_k.gguf"
        },
        {
          "rfilename": "granite-3.2-8b-instruct-q8_0.gguf"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "license:mit",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 56447768704,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "granite-3.2-it:8b",
        "size": 4942859456
      }
    ]
  },
  {
    "author": "allenai",
    "id": "cortexso/olmo-2",
    "metadata": {
      "_id": "6746c45ca0de7ab99efe78d5",
      "author": "cortexso",
      "cardData": {
        "license": "other",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2024-11-27T07:03:56.000Z",
      "description": "---\nlicense: other\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n\n## Overview\nOLMo-2 is a series of Open Language Models designed to enable the science of language models. These models are trained on the Dolma dataset, with all code, checkpoints, logs (coming soon), and associated training details made openly available.\n\nThe OLMo-2 13B Instruct November 2024 is a post-trained variant of the OLMo-2 13B model, which has undergone supervised fine-tuning on an OLMo-specific variant of the Tülu 3 dataset. Additional training techniques include Direct Preference Optimization (DPO) and Reinforcement Learning from Virtual Rewards (RLVR), optimizing it for state-of-the-art performance across various tasks, including chat, MATH, GSM8K, and IFEval.\n\n## Variants\n\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Olmo-2-7b](https://huggingface.co/cortexso/olmo-2/tree/7b) | `cortex run olmo-2:7b` |\n| 2 | [Olmo-2-13b](https://huggingface.co/cortexso/olmo-2/tree/13b) | `cortex run olmo-2:13b` |\n| 3 | [Olmo-2-32b](https://huggingface.co/cortexso/olmo-2/tree/32b) | `cortex run olmo-2:32b` |\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexhub/olmo-2\n    ```\n\n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run olmo-2\n    ```\n    \n## Credits\n\n- **Author:** allenai\n- **Converter:** [Homebrew](https://homebrew.ltd/)\n- **Original License:** [Licence](https://choosealicense.com/licenses/apache-2.0/)\n- **Papers:** [Paper](https://arxiv.org/abs/2501.00656)",
      "disabled": false,
      "downloads": 352,
      "gated": false,
      "gguf": {
        "architecture": "olmo2",
        "bos_token": "<|endoftext|>",
        "chat_template": "{% for message in messages %}{% if message['role'] == 'system' %}{{ '<|system|>\n' + message['content'] + '\n' }}{% elif message['role'] == 'user' %}{{ '<|user|>\n' + message['content'] + '\n' }}{% elif message['role'] == 'assistant' %}{% if not loop.last %}{{ '<|assistant|>\n'  + message['content'] + eos_token + '\n' }}{% else %}{{ '<|assistant|>\n'  + message['content'] + eos_token }}{% endif %}{% endif %}{% if loop.last and add_generation_prompt %}{{ '<|assistant|>\n' }}{% endif %}{% endfor %}",
        "context_length": 4096,
        "eos_token": "<|endoftext|>",
        "total": 32234279936
      },
      "id": "cortexso/olmo-2",
      "lastModified": "2025-03-14T03:06:15.000Z",
      "likes": 1,
      "model-index": null,
      "modelId": "cortexso/olmo-2",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "b76f7629d2da0ccc9535845bab99291e317de088",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        },
        {
          "rfilename": "olmo-2-0325-32b-instruct-q2_k.gguf"
        },
        {
          "rfilename": "olmo-2-0325-32b-instruct-q3_k_l.gguf"
        },
        {
          "rfilename": "olmo-2-0325-32b-instruct-q3_k_m.gguf"
        },
        {
          "rfilename": "olmo-2-0325-32b-instruct-q3_k_s.gguf"
        },
        {
          "rfilename": "olmo-2-0325-32b-instruct-q4_k_m.gguf"
        },
        {
          "rfilename": "olmo-2-0325-32b-instruct-q4_k_s.gguf"
        },
        {
          "rfilename": "olmo-2-0325-32b-instruct-q5_k_m.gguf"
        },
        {
          "rfilename": "olmo-2-0325-32b-instruct-q5_k_s.gguf"
        },
        {
          "rfilename": "olmo-2-0325-32b-instruct-q6_k.gguf"
        },
        {
          "rfilename": "olmo-2-0325-32b-instruct-q8_0.gguf"
        },
        {
          "rfilename": "olmo-2-1124-13b-instruct-q2_k.gguf"
        },
        {
          "rfilename": "olmo-2-1124-13b-instruct-q3_k_l.gguf"
        },
        {
          "rfilename": "olmo-2-1124-13b-instruct-q3_k_m.gguf"
        },
        {
          "rfilename": "olmo-2-1124-13b-instruct-q3_k_s.gguf"
        },
        {
          "rfilename": "olmo-2-1124-13b-instruct-q4_k_m.gguf"
        },
        {
          "rfilename": "olmo-2-1124-13b-instruct-q4_k_s.gguf"
        },
        {
          "rfilename": "olmo-2-1124-13b-instruct-q5_k_m.gguf"
        },
        {
          "rfilename": "olmo-2-1124-13b-instruct-q5_k_s.gguf"
        },
        {
          "rfilename": "olmo-2-1124-13b-instruct-q6_k.gguf"
        },
        {
          "rfilename": "olmo-2-1124-13b-instruct-q8_0.gguf"
        },
        {
          "rfilename": "olmo-2-1124-7b-instruct-q2_k.gguf"
        },
        {
          "rfilename": "olmo-2-1124-7b-instruct-q3_k_l.gguf"
        },
        {
          "rfilename": "olmo-2-1124-7b-instruct-q3_k_m.gguf"
        },
        {
          "rfilename": "olmo-2-1124-7b-instruct-q3_k_s.gguf"
        },
        {
          "rfilename": "olmo-2-1124-7b-instruct-q4_k_m.gguf"
        },
        {
          "rfilename": "olmo-2-1124-7b-instruct-q4_k_s.gguf"
        },
        {
          "rfilename": "olmo-2-1124-7b-instruct-q5_k_m.gguf"
        },
        {
          "rfilename": "olmo-2-1124-7b-instruct-q5_k_s.gguf"
        },
        {
          "rfilename": "olmo-2-1124-7b-instruct-q6_k.gguf"
        },
        {
          "rfilename": "olmo-2-1124-7b-instruct-q8_0.gguf"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "arxiv:2501.00656",
        "license:other",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 335683989120,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "olmo-2:32b",
        "size": 19482558496
      },
      {
        "id": "olmo-2:13b",
        "size": 8354349408
      },
      {
        "id": "olmo-2:7b",
        "size": 4472020160
      }
    ]
  },
  {
    "author": "Microsoft",
    "id": "cortexso/phi-4",
    "metadata": {
      "_id": "677f682eb2e41c2f45dbee73",
      "author": "cortexso",
      "cardData": {
        "license": "mit",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2025-01-09T06:09:50.000Z",
      "description": "---\nlicense: mit\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n## Overview\n\nPhi-4 model, a state-of-the-art 14B parameter Transformer designed for advanced reasoning, conversational AI, and high-quality text generation. Built on a mix of synthetic datasets, filtered public domain content, academic books, and Q&A datasets, Phi-4 ensures exceptional performance through data quality and alignment. It features a 16K token context length, trained on 9.8T tokens over 21 days using 1920 H100-80G GPUs. Phi-4 underwent rigorous fine-tuning and preference optimization to enhance instruction adherence and safety. Released on December 12, 2024, it represents a static model with data cutoff as of June 2024, suitable for diverse applications in research and dialogue systems.\n\n## Variants\n\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Phi-4-14b](https://huggingface.co/cortexso/phi-4/tree/14b) | `cortex run phi-4:14b` |\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```text\n    cortexso/phi-4\n    ```\n\n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run phi-4\n    ```\n\n## Credits\n\n- **Author:** Microsoft Research\n- **Converter:** [Homebrew](https://www.homebrew.ltd/)\n- **Original License:** [License](https://huggingface.co/microsoft/phi-4/blob/main/LICENSE)\n- **Papers:** [Phi-4 Technical Report](https://arxiv.org/pdf/2412.08905)",
      "disabled": false,
      "downloads": 463,
      "gated": false,
      "gguf": {
        "architecture": "phi3",
        "bos_token": "<|endoftext|>",
        "chat_template": "{% for message in messages %}{% if (message['role'] == 'system') %}{{'<|im_start|>system<|im_sep|>' + message['content'] + '<|im_end|>'}}{% elif (message['role'] == 'user') %}{{'<|im_start|>user<|im_sep|>' + message['content'] + '<|im_end|>'}}{% elif (message['role'] == 'assistant') %}{{'<|im_start|>assistant<|im_sep|>' + message['content'] + '<|im_end|>'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant<|im_sep|>' }}{% endif %}",
        "context_length": 16384,
        "eos_token": "<|im_end|>",
        "total": 14659507200
      },
      "id": "cortexso/phi-4",
      "lastModified": "2025-03-02T15:30:47.000Z",
      "likes": 0,
      "model-index": null,
      "modelId": "cortexso/phi-4",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "cc1f8271734a2ac438a1a7c60a62f111b9476524",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        },
        {
          "rfilename": "phi-4-q2_k.gguf"
        },
        {
          "rfilename": "phi-4-q3_k_l.gguf"
        },
        {
          "rfilename": "phi-4-q3_k_m.gguf"
        },
        {
          "rfilename": "phi-4-q3_k_s.gguf"
        },
        {
          "rfilename": "phi-4-q4_k_m.gguf"
        },
        {
          "rfilename": "phi-4-q4_k_s.gguf"
        },
        {
          "rfilename": "phi-4-q5_k_m.gguf"
        },
        {
          "rfilename": "phi-4-q5_k_s.gguf"
        },
        {
          "rfilename": "phi-4-q6_k.gguf"
        },
        {
          "rfilename": "phi-4-q8_0.gguf"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "arxiv:2412.08905",
        "license:mit",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 93205915520,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "phi-4:14b",
        "size": 9053114560
      }
    ]
  },
  {
    "author": "MistralAI",
    "id": "cortexso/mistral-small-24b",
    "metadata": {
      "_id": "679c3a8f4061a1ab60e703b7",
      "author": "cortexso",
      "cardData": {
        "license": "mit",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2025-01-31T02:50:55.000Z",
      "description": "---\nlicense: mit\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n## Overview\nThe 'mistral-small-24b' model is an advanced AI language model optimized for a variety of natural language processing tasks. It is particularly well-suited for applications such as text generation, chatbots, content summarization, and language translation. Built on the foundation of 'mistralai/Mistral-Small-24B-Base-2501', it leverages state-of-the-art techniques for understanding and generating human-like text. Users can expect significant improvements in fluency and contextual relevance, making it effective for both professional and creative use cases. Its efficiency allows for deployment in resource-constrained environments, catering to a diverse range of industries and applications.\n## Variants\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Mistral-Small-24b](https://huggingface.co/cortexso/mistral-small-24b/tree/24b) | cortex run mistral-small-24b:24b |\n## Use it with Jan (UI)\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    \n    ```bash\n    cortexso/mistral-small-24b\n    ```\n    \n## Use it with Cortex (CLI)\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    \n    ```bash\n      cortex run mistral-small-24b\n    ```\n    \n## Credits\n- **Author:** mistralai\n- **Converter:** [Homebrew](https://www.homebrew.ltd/)\n- **Original License:** [License](https://choosealicense.com/licenses/apache-2.0/)\n- **Paper:** [Mistral Small 3 Blog](https://mistral.ai/news/mistral-small-3)",
      "disabled": false,
      "downloads": 683,
      "gated": false,
      "gguf": {
        "architecture": "llama",
        "bos_token": "<s>",
        "context_length": 32768,
        "eos_token": "</s>",
        "total": 23572403200
      },
      "id": "cortexso/mistral-small-24b",
      "lastModified": "2025-03-03T06:09:47.000Z",
      "likes": 0,
      "model-index": null,
      "modelId": "cortexso/mistral-small-24b",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "5a28cb4b0f1aa4e0b55f527b71c88eb5b56ebd71",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "mistral-small-24b-base-2501-q2_k.gguf"
        },
        {
          "rfilename": "mistral-small-24b-base-2501-q3_k_l.gguf"
        },
        {
          "rfilename": "mistral-small-24b-base-2501-q3_k_m.gguf"
        },
        {
          "rfilename": "mistral-small-24b-base-2501-q3_k_s.gguf"
        },
        {
          "rfilename": "mistral-small-24b-base-2501-q4_k_m.gguf"
        },
        {
          "rfilename": "mistral-small-24b-base-2501-q4_k_s.gguf"
        },
        {
          "rfilename": "mistral-small-24b-base-2501-q5_k_m.gguf"
        },
        {
          "rfilename": "mistral-small-24b-base-2501-q5_k_s.gguf"
        },
        {
          "rfilename": "mistral-small-24b-base-2501-q6_k.gguf"
        },
        {
          "rfilename": "mistral-small-24b-base-2501-q8_0.gguf"
        },
        {
          "rfilename": "model.yml"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "license:mit",
        "endpoints_compatible",
        "region:us"
      ],
      "usedStorage": 148517729600,
      "widgetData": [
        {
          "text": "My name is Julien and I like to"
        },
        {
          "text": "I like traveling by train because"
        },
        {
          "text": "Paris is an amazing place to visit,"
        },
        {
          "text": "Once upon a time,"
        }
      ]
    },
    "models": [
      {
        "id": "mistral-small-24b:24b",
        "size": 14333907488
      }
    ]
  },
  {
    "author": "DeepSeek-AI",
    "id": "cortexso/deepseek-r1-distill-qwen-7b",
    "metadata": {
      "_id": "6790a5b2044aeb2bd5922877",
      "author": "cortexso",
      "cardData": {
        "license": "mit",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2025-01-22T08:00:50.000Z",
      "description": "---\nlicense: mit\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n\n## Overview\n\n**DeepSeek** developed and released the [DeepSeek R1 Distill Qwen 7B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B) model, a distilled version of the Qwen 7B language model. This version is fine-tuned for high-performance text generation and optimized for dialogue and information-seeking tasks, providing even greater capabilities with its larger size compared to the 7B variant.\n\nThe model is designed for applications in customer support, conversational AI, and research, focusing on delivering accurate, helpful, and safe outputs while maintaining efficiency.\n\n## Variants\n\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Deepseek-r1-distill-qwen-7b-7b](https://huggingface.co/cortexso/deepseek-r1-distill-qwen-7b/tree/7b) | `cortex run deepseek-r1-distill-qwen-7b:7b` |\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexso/deepseek-r1-distill-qwen-7b\n    ```\n\n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run deepseek-r1-distill-qwen-7b\n    ```\n\n## Credits\n\n- **Author:** DeepSeek\n- **Converter:** [Homebrew](https://www.homebrew.ltd/)\n- **Original License:** [License](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B#7-license)\n- **Papers:** [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/html/2501.12948v1)",
      "disabled": false,
      "downloads": 1008,
      "gated": false,
      "gguf": {
        "architecture": "qwen2",
        "bos_token": "<｜begin▁of▁sentence｜>",
        "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜><think>\\n'}}{% endif %}",
        "context_length": 131072,
        "eos_token": "<｜end▁of▁sentence｜>",
        "total": 7615616512
      },
      "id": "cortexso/deepseek-r1-distill-qwen-7b",
      "lastModified": "2025-03-03T06:27:42.000Z",
      "likes": 0,
      "model-index": null,
      "modelId": "cortexso/deepseek-r1-distill-qwen-7b",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "8e256fee6ed3616f3f90b0eb453083a115f1fe40",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-7b-q2_k.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-7b-q3_k_l.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-7b-q3_k_m.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-7b-q3_k_s.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-7b-q4_k_m.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-7b-q4_k_s.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-7b-q5_k_m.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-7b-q5_k_s.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-7b-q6_k.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-7b-q8_0.gguf"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "license:mit",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 53341802656,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "deepseek-r1-distill-qwen-7b:7b",
        "size": 4683073184
      }
    ]
  },
  {
    "author": "DeepSeek-AI",
    "id": "cortexso/deepseek-r1-distill-qwen-14b",
    "metadata": {
      "_id": "678fdf2be186002cc0ba006e",
      "author": "cortexso",
      "cardData": {
        "license": "mit",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2025-01-21T17:53:47.000Z",
      "description": "---\nlicense: mit\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n\n## Overview\n\n**DeepSeek** developed and released the [DeepSeek R1 Distill Qwen 14B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B) model, a distilled version of the Qwen 14B language model. This variant represents the largest and most powerful model in the DeepSeek R1 Distill series, fine-tuned for high-performance text generation, dialogue optimization, and advanced reasoning tasks. \n\nThe model is designed for applications that require extensive understanding, such as conversational AI, research, large-scale knowledge systems, and customer service, providing superior performance in accuracy, efficiency, and safety.\n\n## Variants\n\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Deepseek-r1-distill-qwen-14b-14b](https://huggingface.co/cortexso/deepseek-r1-distill-qwen-14b/tree/14b) | `cortex run deepseek-r1-distill-qwen-14b:14b` |\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexso/deepseek-r1-distill-qwen-14b\n    ```\n\n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run deepseek-r1-distill-qwen-14b\n    ```\n\n## Credits\n\n- **Author:** DeepSeek\n- **Converter:** [Homebrew](https://www.homebrew.ltd/)\n- **Original License:** [License](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B#7-license)\n- **Papers:** [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/html/2501.12948v1)",
      "disabled": false,
      "downloads": 1261,
      "gated": false,
      "gguf": {
        "architecture": "qwen2",
        "bos_token": "<｜begin▁of▁sentence｜>",
        "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜><think>\\n'}}{% endif %}",
        "context_length": 131072,
        "eos_token": "<｜end▁of▁sentence｜>",
        "total": 14770033664
      },
      "id": "cortexso/deepseek-r1-distill-qwen-14b",
      "lastModified": "2025-03-03T06:40:22.000Z",
      "likes": 0,
      "model-index": null,
      "modelId": "cortexso/deepseek-r1-distill-qwen-14b",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "ca42c63b1c148ac7be176ef0ed8384d3775bed5b",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-14b-q2_k.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-14b-q3_k_l.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-14b-q3_k_m.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-14b-q3_k_s.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-14b-q4_k_m.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-14b-q4_k_s.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-14b-q5_k_m.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-14b-q5_k_s.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-14b-q6_k.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-14b-q8_0.gguf"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "license:mit",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 102845421536,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "deepseek-r1-distill-qwen-14b:14b",
        "size": 8988109920
      }
    ]
  },
  {
    "author": "DeepSeek-AI",
    "id": "cortexso/deepseek-r1-distill-qwen-32b",
    "metadata": {
      "_id": "678fe132df84bd3d94f37e58",
      "author": "cortexso",
      "cardData": {
        "license": "mit",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2025-01-21T18:02:26.000Z",
      "description": "---\nlicense: mit\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n\n## Overview\n\n**DeepSeek** developed and released the [DeepSeek R1 Distill Qwen 32B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B) model, a distilled version of the Qwen 32B language model. This is the most advanced and largest model in the DeepSeek R1 Distill family, offering unparalleled performance in text generation, dialogue optimization, and reasoning tasks. \n\nThe model is tailored for large-scale applications in conversational AI, research, enterprise solutions, and knowledge systems, delivering exceptional accuracy, efficiency, and safety at scale.\n\n## Variants\n\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Deepseek-r1-distill-qwen-32b-32b](https://huggingface.co/cortexso/deepseek-r1-distill-qwen-32b/tree/32b) | `cortex run deepseek-r1-distill-qwen-32b:32b` |\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexso/deepseek-r1-distill-qwen-32b\n    ```\n\n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run deepseek-r1-distill-qwen-32b\n    ```\n\n## Credits\n\n- **Author:** DeepSeek\n- **Converter:** [Homebrew](https://www.homebrew.ltd/)\n- **Original License:** [License](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B#7-license)\n- **Papers:** [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/html/2501.12948v1)",
      "disabled": false,
      "downloads": 597,
      "gated": false,
      "gguf": {
        "architecture": "qwen2",
        "bos_token": "<｜begin▁of▁sentence｜>",
        "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜><think>\\n'}}{% endif %}",
        "context_length": 131072,
        "eos_token": "<｜end▁of▁sentence｜>",
        "total": 32763876352
      },
      "id": "cortexso/deepseek-r1-distill-qwen-32b",
      "lastModified": "2025-03-03T06:41:05.000Z",
      "likes": 0,
      "model-index": null,
      "modelId": "cortexso/deepseek-r1-distill-qwen-32b",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "0ec9981b2b5ad5c04a5357a3c328f10735efc79a",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-32b-q2_k.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-32b-q3_k_l.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-32b-q3_k_m.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-32b-q3_k_s.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-32b-q4_k_m.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-32b-q4_k_s.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-32b-q5_k_m.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-32b-q5_k_s.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-32b-q6_k.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-32b-q8_0.gguf"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "license:mit",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 225982083296,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "deepseek-r1-distill-qwen-32b:32b",
        "size": 19851335520
      }
    ]
  },
  {
    "author": "DeepSeek-AI",
    "id": "cortexso/deepseek-r1-distill-llama-70b",
    "metadata": {
      "_id": "678fe1673b0a6384a4e1f887",
      "author": "cortexso",
      "cardData": {
        "license": "mit",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2025-01-21T18:03:19.000Z",
      "description": "---\nlicense: mit\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n\n## Overview\n\n**DeepSeek** developed and released the [DeepSeek R1 Distill Llama 70B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B) model, a distilled version of the Llama 70B language model. This model represents the pinnacle of the DeepSeek R1 Distill series, designed for exceptional performance in text generation, dialogue tasks, and advanced reasoning, offering unparalleled capabilities for large-scale AI applications.\n\nThe model is ideal for enterprise-grade applications, research, conversational AI, and large-scale knowledge systems, providing top-tier accuracy, safety, and efficiency.\n\n## Variants\n\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Deepseek-r1-distill-llama-70b-70b](https://huggingface.co/cortexso/deepseek-r1-distill-llama-70b/tree/70b) | `cortex run deepseek-r1-distill-llama-70b:70b` |\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexso/deepseek-r1-distill-llama-70b\n    ```\n\n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run deepseek-r1-distill-llama-70b\n    ```\n\n## Credits\n\n- **Author:** DeepSeek\n- **Converter:** [Homebrew](https://www.homebrew.ltd/)\n- **Original License:** [License](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B#7-license)\n- **Papers:** [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/html/2501.12948v1)",
      "disabled": false,
      "downloads": 580,
      "gated": false,
      "gguf": {
        "architecture": "llama",
        "bos_token": "<｜begin▁of▁sentence｜>",
        "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜><think>\\n'}}{% endif %}",
        "context_length": 131072,
        "eos_token": "<｜end▁of▁sentence｜>",
        "total": 70553706560
      },
      "id": "cortexso/deepseek-r1-distill-llama-70b",
      "lastModified": "2025-03-03T06:42:21.000Z",
      "likes": 0,
      "model-index": null,
      "modelId": "cortexso/deepseek-r1-distill-llama-70b",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "d03fa1c83966573864075845a4b493af9aa8ed53",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "deepseek-r1-distill-llama-70b-q4_k_m.gguf"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "license:mit",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 85040791136,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "deepseek-r1-distill-llama-70b:70b",
        "size": 42520395584
      }
    ]
  },
  {
    "author": "DeepSeek-AI",
    "id": "cortexso/deepseek-r1-distill-llama-8b",
    "metadata": {
      "_id": "678f4b5625a9b93997f1f666",
      "author": "cortexso",
      "cardData": {
        "license": "mit",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2025-01-21T07:23:02.000Z",
      "description": "---\nlicense: mit\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n\n## Overview\n\n**DeepSeek** developed and released the [DeepSeek R1 Distill Llama 8B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B) model, a distilled version of the Llama 8B language model. This variant is fine-tuned for high-performance text generation, optimized for dialogue, and tailored for information-seeking tasks. It offers a robust balance between model size and performance, making it suitable for demanding conversational AI and research use cases.\n\nThe model is designed to deliver accurate, efficient, and safe responses in applications such as customer support, knowledge systems, and research environments.\n\n\n## Variants\n\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Deepseek-r1-distill-llama-8b-8b](https://huggingface.co/cortexso/deepseek-r1-distill-llama-8b/tree/8b) | `cortex run deepseek-r1-distill-llama-8b:8b` |\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexso/deepseek-r1-distill-llama-8b\n    ```\n\n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run deepseek-r1-distill-llama-8b\n    ```\n\n## Credits\n\n- **Author:** DeepSeek\n- **Converter:** [Homebrew](https://www.homebrew.ltd/)\n- **Original License:** [License](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B#7-license)\n- **Papers:** [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/html/2501.12948v1)",
      "disabled": false,
      "downloads": 933,
      "gated": false,
      "gguf": {
        "architecture": "llama",
        "bos_token": "<｜begin▁of▁sentence｜>",
        "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜><think>\\n'}}{% endif %}",
        "context_length": 131072,
        "eos_token": "<｜end▁of▁sentence｜>",
        "total": 8030261312
      },
      "id": "cortexso/deepseek-r1-distill-llama-8b",
      "lastModified": "2025-03-03T06:33:03.000Z",
      "likes": 0,
      "model-index": null,
      "modelId": "cortexso/deepseek-r1-distill-llama-8b",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "b3321ad8a700b3aa2c3fc44ac84a167bd11ecdb8",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "deepseek-r1-distill-llama-8b-q2_k.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-llama-8b-q3_k_l.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-llama-8b-q3_k_m.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-llama-8b-q3_k_s.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-llama-8b-q4_k_m.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-llama-8b-q4_k_s.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-llama-8b-q5_k_m.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-llama-8b-q5_k_s.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-llama-8b-q6_k.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-llama-8b-q8_0.gguf"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "license:mit",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 56187723232,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "deepseek-r1-distill-llama-8b:8b",
        "size": 4920736256
      }
    ]
  },
  {
    "author": "NovaSky-AI",
    "id": "cortexso/sky-t1",
    "metadata": {
      "_id": "6782f82c860ee02fe01dbd60",
      "author": "cortexso",
      "cardData": {
        "license": "apache-2.0",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2025-01-11T23:01:00.000Z",
      "description": "---\nlicense: apache-2.0\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n## Overview\n\n**NovaSky Team** developed and released the [Sky-T1](https://huggingface.co/novasky-ai/Sky-T1-32B-Preview), a 32-billion parameter reasoning model adapted from Qwen2.5-32B-Instruct. This model is designed for advanced reasoning, coding, and mathematical tasks, achieving performance comparable to state-of-the-art models like o1-preview while being cost-efficient. Sky-T1 was trained on 17K verified responses from Qwen/QwQ-32B-Preview, with additional science data from the Still-2 dataset, ensuring high-quality and diverse learning sources.\n\nThe model supports complex reasoning via long chain-of-thought processes and excels in both coding and mathematical challenges. Utilizing Llama-Factory with DeepSpeed Zero-3 Offload, Sky-T1 training was completed in just 19 hours on 8 H100 GPUs, demonstrating efficient resource utilization. These capabilities make Sky-T1 an exceptional tool for applications in programming, academic research, and reasoning-intensive tasks.\n\n## Variants\n\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Sky-t1-32b](https://huggingface.co/cortexso/sky-t1/tree/32b) | `cortex run sky-t1:32b` |\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexso/sky-t1\n    ```\n\n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run sky-t1\n    ```\n\n## Credits\n\n- **Author:** NovaSky Team\n- **Converter:** [Homebrew](https://www.homebrew.ltd/)\n- **Original License:** [License](https://choosealicense.com/licenses/apache-2.0/)\n- **Papers:** [Sky-T1: Fully Open-Source Reasoning Model](https://novasky-ai.github.io/posts/sky-t1/)",
      "disabled": false,
      "downloads": 116,
      "gated": false,
      "gguf": {
        "architecture": "qwen2",
        "bos_token": "<|endoftext|>",
        "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n",
        "context_length": 32768,
        "eos_token": "<|im_end|>",
        "total": 32763876352
      },
      "id": "cortexso/sky-t1",
      "lastModified": "2025-03-03T05:51:45.000Z",
      "likes": 0,
      "model-index": null,
      "modelId": "cortexso/sky-t1",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "05f06ab0191808f8eb21fa3c60c9ec4a6bef4978",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        },
        {
          "rfilename": "sky-t1-32b-preview-q2_k.gguf"
        },
        {
          "rfilename": "sky-t1-32b-preview-q3_k_l.gguf"
        },
        {
          "rfilename": "sky-t1-32b-preview-q3_k_m.gguf"
        },
        {
          "rfilename": "sky-t1-32b-preview-q3_k_s.gguf"
        },
        {
          "rfilename": "sky-t1-32b-preview-q4_k_m.gguf"
        },
        {
          "rfilename": "sky-t1-32b-preview-q4_k_s.gguf"
        },
        {
          "rfilename": "sky-t1-32b-preview-q5_k_m.gguf"
        },
        {
          "rfilename": "sky-t1-32b-preview-q5_k_s.gguf"
        },
        {
          "rfilename": "sky-t1-32b-preview-q6_k.gguf"
        },
        {
          "rfilename": "sky-t1-32b-preview-q8_0.gguf"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "license:apache-2.0",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 225982094944,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "sky-t1:32b",
        "size": 19851336576
      }
    ]
  },
  {
    "author": "CohereForAI",
    "id": "cortexso/aya",
    "metadata": {
      "_id": "672aa4167f36760042e632ed",
      "author": "cortexso",
      "cardData": {
        "license": "cc-by-nc-4.0",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2024-11-05T23:02:46.000Z",
      "description": "---\nlicense: cc-by-nc-4.0\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n## Overview\n\n**Cohere For AI** developed and released the [Aya 23](https://huggingface.co/CohereForAI/aya-23-35B), an open weights instruction fine-tuned model with advanced multilingual capabilities. Aya 23 is built upon the highly performant Command family of models and fine-tuned using the Aya Collection to deliver state-of-the-art performance across 23 languages. This multilingual large language model is designed to support a wide range of use cases, including multilingual text generation, understanding, and translation tasks.\n\nAya 23, balancing efficiency and performance. It offers robust multilingual support for languages such as Arabic, Chinese, English, Spanish, Hindi, Vietnamese, and more, making it a versatile tool for global applications. A 35-billion parameter version is also available [here](https://huggingface.co/CohereForAI/aya-23-35b).\n\n## Variants\n\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Aya-8b](https://huggingface.co/cortexso/aya/tree/8b) | `cortex run aya:8b` |\n| 2 | [Aya-35b](https://huggingface.co/cortexso/aya/tree/35b) | `cortex run aya:35b` |\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexso/aya\n    ```\n\n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run aya\n    ```\n\n## Credits\n\n- **Author:** Cohere For AI\n- **Converter:** [Homebrew](https://www.homebrew.ltd/)\n- **Original License:** [License](https://spdx.org/licenses/CC-BY-NC-4.0)",
      "disabled": false,
      "downloads": 168,
      "gated": false,
      "gguf": {
        "architecture": "command-r",
        "bos_token": "<BOS_TOKEN>",
        "chat_template": "{{ bos_token }}{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif false == true %}{% set loop_messages = messages %}{% set system_message = 'You are Aya, a brilliant, sophisticated, AI-assistant trained to assist human users by providing thorough responses. You are trained by Cohere.' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% if system_message != false %}{{ '<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>' + system_message + '<|END_OF_TURN_TOKEN|>' }}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|START_OF_TURN_TOKEN|><|USER_TOKEN|>' + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% elif message['role'] == 'assistant' %}{{ '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>'  + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>' }}{% endif %}",
        "context_length": 8192,
        "eos_token": "<|END_OF_TURN_TOKEN|>",
        "total": 34980831232
      },
      "id": "cortexso/aya",
      "lastModified": "2025-03-02T14:58:34.000Z",
      "likes": 0,
      "model-index": null,
      "modelId": "cortexso/aya",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "d97fef50adc54a22ec1e3133771f7cb17528742b",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "aya-23-35b-q2_k.gguf"
        },
        {
          "rfilename": "aya-23-35b-q3_k_l.gguf"
        },
        {
          "rfilename": "aya-23-35b-q3_k_m.gguf"
        },
        {
          "rfilename": "aya-23-35b-q3_k_s.gguf"
        },
        {
          "rfilename": "aya-23-35b-q4_k_m.gguf"
        },
        {
          "rfilename": "aya-23-35b-q4_k_s.gguf"
        },
        {
          "rfilename": "aya-23-35b-q5_k_m.gguf"
        },
        {
          "rfilename": "aya-23-35b-q5_k_s.gguf"
        },
        {
          "rfilename": "aya-23-35b-q6_k.gguf"
        },
        {
          "rfilename": "aya-23-35b-q8_0.gguf"
        },
        {
          "rfilename": "aya-23-8b-q2_k.gguf"
        },
        {
          "rfilename": "aya-23-8b-q3_k_l.gguf"
        },
        {
          "rfilename": "aya-23-8b-q3_k_m.gguf"
        },
        {
          "rfilename": "aya-23-8b-q3_k_s.gguf"
        },
        {
          "rfilename": "aya-23-8b-q4_k_m.gguf"
        },
        {
          "rfilename": "aya-23-8b-q4_k_s.gguf"
        },
        {
          "rfilename": "aya-23-8b-q5_k_m.gguf"
        },
        {
          "rfilename": "aya-23-8b-q5_k_s.gguf"
        },
        {
          "rfilename": "aya-23-8b-q6_k.gguf"
        },
        {
          "rfilename": "aya-23-8b-q8_0.gguf"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "license:cc-by-nc-4.0",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 302730192928,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "aya:35b",
        "size": 21527043520
      },
      {
        "id": "aya:8b",
        "size": 5056974496
      }
    ]
  },
  {
    "author": "PowerInfer",
    "id": "cortexso/small-thinker",
    "metadata": {
      "_id": "6777192582e1ec3ecb79d1a4",
      "author": "cortexso",
      "cardData": {
        "license": "mit",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2025-01-02T22:54:29.000Z",
      "description": "---\nlicense: mit\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n## Overview\n\n**PowerInfer** developed and released the [SmallThinker-3B-preview](https://huggingface.co/PowerInfer/SmallThinker-3B-Preview), a fine-tuned version of the Qwen2.5-3B-Instruct model. SmallThinker is optimized for efficient deployment on resource-constrained devices while maintaining high performance in reasoning, coding, and general text generation tasks. It outperforms its base model on key benchmarks, including AIME24, AMC23, and GAOKAO2024, making it a robust tool for both edge deployment and as a draft model for larger systems like QwQ-32B-Preview.\n\nSmallThinker was fine-tuned in two phases using high-quality datasets, including PowerInfer/QWQ-LONGCOT-500K and PowerInfer/LONGCOT-Refine-500K. Its small size allows for up to 70% faster inference speeds compared to larger models, making it ideal for applications requiring quick responses and efficient computation.\n\n## Variants\n\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Small-thinker-3b](https://huggingface.co/cortexso/small-thinker/tree/3b) | `cortex run small-thinker:3b` |\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexso/small-thinker\n    ```\n\n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run small-thinker\n    ```\n\n## Credits\n\n- **Author:** PowerInfer\n- **Converter:** [Homebrew](https://www.homebrew.ltd/)\n- **Original License:** [License](https://huggingface.co/PowerInfer/SmallThinker-3B-Preview/blob/main/LICENSE)",
      "disabled": false,
      "downloads": 273,
      "gated": false,
      "gguf": {
        "architecture": "qwen2",
        "bos_token": "<|endoftext|>",
        "chat_template": "{% set system_message = 'You are a helpful assistant.' %}{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% endif %}{% if system_message is defined %}{{ '<|im_start|>system\n' + system_message + '<|im_end|>\n' }}{% endif %}{% for message in loop_messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\n' + content + '<|im_end|>\n<|im_start|>assistant\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|im_end|>' + '\n' }}{% endif %}{% endfor %}",
        "context_length": 32768,
        "eos_token": "<|im_end|>",
        "total": 3397103616
      },
      "id": "cortexso/small-thinker",
      "lastModified": "2025-03-03T06:05:50.000Z",
      "likes": 0,
      "model-index": null,
      "modelId": "cortexso/small-thinker",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "f2746c69548d6ff92db6ec663400ad9a0dc51bbc",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        },
        {
          "rfilename": "smallthinker-3b-preview-q2_k.gguf"
        },
        {
          "rfilename": "smallthinker-3b-preview-q3_k_l.gguf"
        },
        {
          "rfilename": "smallthinker-3b-preview-q3_k_m.gguf"
        },
        {
          "rfilename": "smallthinker-3b-preview-q3_k_s.gguf"
        },
        {
          "rfilename": "smallthinker-3b-preview-q4_k_m.gguf"
        },
        {
          "rfilename": "smallthinker-3b-preview-q4_k_s.gguf"
        },
        {
          "rfilename": "smallthinker-3b-preview-q5_k_m.gguf"
        },
        {
          "rfilename": "smallthinker-3b-preview-q5_k_s.gguf"
        },
        {
          "rfilename": "smallthinker-3b-preview-q6_k.gguf"
        },
        {
          "rfilename": "smallthinker-3b-preview-q8_0.gguf"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "license:mit",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 23981289568,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "small-thinker:3b",
        "size": 2104931616
      }
    ]
  },
  {
    "author": "Google",
    "id": "cortexso/gemma2",
    "metadata": {
      "_id": "66b06c37491b555fefe0a0bf",
      "author": "cortexso",
      "cardData": {
        "license": "gemma",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2024-08-05T06:07:51.000Z",
      "description": "---\nlicense: gemma\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n\n## Overview\n\nThe [Gemma](https://huggingface.co/google/gemma-2-2b-it), state-of-the-art open model trained with the Gemma datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties. The model belongs to the Gemma family with the 4B, 7B version in two variants 8K and 128K which is the context length (in tokens) that it can support.\n\n## Variants\n\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Gemma2-2b](https://huggingface.co/cortexso/gemma2/tree/2b) | `cortex run gemma2:2b` |\n| 2 | [Gemma2-9b](https://huggingface.co/cortexso/gemma2/tree/9b) | `cortex run gemma2:9b` |\n| 3 | [Gemma2-27b](https://huggingface.co/cortexso/gemma2/tree/27b) | `cortex run gemma2:27b` |\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexso/gemma2\n    ```\n    \n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run gemma2\n    ```\n    \n## Credits\n\n- **Author:** Go‌ogle\n- **Converter:** [Homebrew](https://www.homebrew.ltd/)\n- **Original License:** [License](https://ai.google.dev/gemma/terms)\n- **Papers:** [Gemma Technical Report](https://arxiv.org/abs/2403.08295)",
      "disabled": false,
      "downloads": 796,
      "gated": false,
      "gguf": {
        "architecture": "gemma2",
        "bos_token": "<bos>",
        "chat_template": "{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n' + message['content'] | trim + '<end_of_turn>\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n'}}{% endif %}",
        "context_length": 8192,
        "eos_token": "<eos>",
        "total": 27227128320
      },
      "id": "cortexso/gemma2",
      "lastModified": "2025-03-03T06:25:38.000Z",
      "likes": 0,
      "model-index": null,
      "modelId": "cortexso/gemma2",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "36fdfde32513f2a0be9e1b166952d4cee227aaf6",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "gemma-2-27b-it-q2_k.gguf"
        },
        {
          "rfilename": "gemma-2-27b-it-q3_k_l.gguf"
        },
        {
          "rfilename": "gemma-2-27b-it-q3_k_m.gguf"
        },
        {
          "rfilename": "gemma-2-27b-it-q3_k_s.gguf"
        },
        {
          "rfilename": "gemma-2-27b-it-q4_k_m.gguf"
        },
        {
          "rfilename": "gemma-2-27b-it-q4_k_s.gguf"
        },
        {
          "rfilename": "gemma-2-27b-it-q5_k_m.gguf"
        },
        {
          "rfilename": "gemma-2-27b-it-q5_k_s.gguf"
        },
        {
          "rfilename": "gemma-2-27b-it-q6_k.gguf"
        },
        {
          "rfilename": "gemma-2-27b-it-q8_0.gguf"
        },
        {
          "rfilename": "gemma-2-2b-it-q2_k.gguf"
        },
        {
          "rfilename": "gemma-2-2b-it-q3_k_l.gguf"
        },
        {
          "rfilename": "gemma-2-2b-it-q3_k_m.gguf"
        },
        {
          "rfilename": "gemma-2-2b-it-q3_k_s.gguf"
        },
        {
          "rfilename": "gemma-2-2b-it-q4_k_m.gguf"
        },
        {
          "rfilename": "gemma-2-2b-it-q4_k_s.gguf"
        },
        {
          "rfilename": "gemma-2-2b-it-q5_k_m.gguf"
        },
        {
          "rfilename": "gemma-2-2b-it-q5_k_s.gguf"
        },
        {
          "rfilename": "gemma-2-2b-it-q6_k.gguf"
        },
        {
          "rfilename": "gemma-2-2b-it-q8_0.gguf"
        },
        {
          "rfilename": "gemma-2-9b-it-q2_k.gguf"
        },
        {
          "rfilename": "gemma-2-9b-it-q3_k_l.gguf"
        },
        {
          "rfilename": "gemma-2-9b-it-q3_k_m.gguf"
        },
        {
          "rfilename": "gemma-2-9b-it-q3_k_s.gguf"
        },
        {
          "rfilename": "gemma-2-9b-it-q4_k_m.gguf"
        },
        {
          "rfilename": "gemma-2-9b-it-q4_k_s.gguf"
        },
        {
          "rfilename": "gemma-2-9b-it-q5_k_m.gguf"
        },
        {
          "rfilename": "gemma-2-9b-it-q5_k_s.gguf"
        },
        {
          "rfilename": "gemma-2-9b-it-q6_k.gguf"
        },
        {
          "rfilename": "gemma-2-9b-it-q8_0.gguf"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "arxiv:2403.08295",
        "license:gemma",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 280987360512,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "gemma2:9b",
        "size": 5761057888
      },
      {
        "id": "gemma2:27b",
        "size": 16645381792
      },
      {
        "id": "gemma2:2b",
        "size": 1708582656
      }
    ]
  },
  {
    "author": "agentica-org",
    "id": "cortexso/deepscaler",
    "metadata": {
      "_id": "67aaa7a5a6e6b3d852e347b2",
      "author": "cortexso",
      "cardData": {
        "license": "mit",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2025-02-11T01:28:05.000Z",
      "description": "---\nlicense: mit\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n## Overview\nDeepscaler is an advanced AI model developed from the agentica-org's DeepScaleR-1.5B-Preview, designed to enhance the efficiency and scalability of various machine learning tasks. Its core purpose is to provide high-quality predictive analytics and data processing capabilities while optimizing resource usage. Deepscaler is particularly useful in scenarios such as natural language processing, computer vision, and more complex data interpretation tasks, making it suitable for applications in industries like finance, healthcare, and entertainment. Users can leverage its performance to achieve faster training times and improved accuracy in their models. Overall, Deepscaler's architecture allows it to deliver robust results with reduced computational overhead, making it an excellent choice for developers and organizations aiming to scale their AI solutions.\n## Variants\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Deepscaler-1.5b](https://huggingface.co/cortexso/deepscaler/tree/1.5b) | cortex run deepscaler:1.5b |\n## Use it with Jan (UI)\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexso/deepscaler\n    ```\n    \n## Use it with Cortex (CLI)\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run deepscaler\n    ```\n## Credits\n- **Author:** agentica-org\n- **Converter:** [Homebrew](https://www.homebrew.ltd/)\n- **Original License:** [LICENSE](https://huggingface.co/agentica-org/DeepScaleR-1.5B-Preview/blob/main/LICENSE)",
      "disabled": false,
      "downloads": 404,
      "gated": false,
      "gguf": {
        "architecture": "qwen2",
        "bos_token": "<｜begin▁of▁sentence｜>",
        "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜>'}}{% endif %}",
        "context_length": 131072,
        "eos_token": "<｜end▁of▁sentence｜>",
        "total": 1777088000
      },
      "id": "cortexso/deepscaler",
      "lastModified": "2025-03-03T06:07:30.000Z",
      "likes": 1,
      "model-index": null,
      "modelId": "cortexso/deepscaler",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "f2ac6bdbe311a9dbaf2bc4d77baa460b06b169e6",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "deepscaler-1.5b-preview-q2_k.gguf"
        },
        {
          "rfilename": "deepscaler-1.5b-preview-q3_k_l.gguf"
        },
        {
          "rfilename": "deepscaler-1.5b-preview-q3_k_m.gguf"
        },
        {
          "rfilename": "deepscaler-1.5b-preview-q3_k_s.gguf"
        },
        {
          "rfilename": "deepscaler-1.5b-preview-q4_k_m.gguf"
        },
        {
          "rfilename": "deepscaler-1.5b-preview-q4_k_s.gguf"
        },
        {
          "rfilename": "deepscaler-1.5b-preview-q5_k_m.gguf"
        },
        {
          "rfilename": "deepscaler-1.5b-preview-q5_k_s.gguf"
        },
        {
          "rfilename": "deepscaler-1.5b-preview-q6_k.gguf"
        },
        {
          "rfilename": "deepscaler-1.5b-preview-q8_0.gguf"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "license:mit",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 12728615584,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "deepscaler:1.5b",
        "size": 1117321888
      }
    ]
  },
  {
    "author": "Falcon LLM TII UAE",
    "id": "cortexso/falcon3",
    "metadata": {
      "_id": "6761d4519d9bc9c3b6e25ad4",
      "author": "cortexso",
      "cardData": {
        "license": "apache-2.0",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2024-12-17T19:43:13.000Z",
      "description": "---\nlicense: apache-2.0\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n\n\n## Overview\n\nFalcon3-10B-Instruct is part of the Falcon3 family of Open Foundation Models, offering state-of-the-art performance in reasoning, language understanding, instruction following, code, and mathematics. With 10 billion parameters, Falcon3-10B-Instruct is optimized for high-quality instruction-following tasks and supports multilingual capabilities in English, French, Spanish, and Portuguese. It provides a long context length of up to 32K tokens, making it suitable for extended document understanding and processing.\n\n## Variants\n\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Falcon3-10b](https://huggingface.co/cortexso/falcon3/tree/10b) | `cortex run falcon3:10b` |\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexhub/falcon3\n    ```\n\n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run falcon3\n    ```\n    \n## Credits\n\n- **Author:** Falcon3 Team\n- **Converter:** [Homebrew](https://www.homebrew.ltd/)\n- **Original License:** [License](https://falconllm.tii.ae/falcon-terms-and-conditions.html)\n- **Papers:** [Paper](https://arxiv.org/abs/2311.16867)",
      "disabled": false,
      "downloads": 276,
      "gated": false,
      "gguf": {
        "architecture": "llama",
        "bos_token": "<|endoftext|>",
        "chat_template": "{%- if tools %}\n{{- '<|system|>\\n' }}\n{%- if messages[0]['role'] == 'system' %}\n{{- messages[0]['content'] }}\n{%- set remaining_messages = messages[1:] %}\n{%- else %}\n{%- set remaining_messages = messages %}\n{%- endif %}\n{{- 'You are a Falcon assistant skilled in function calling. You are helpful, respectful, and concise.\\n\\n# Tools\\n\\nYou have access to the following functions. You MUST use them to answer questions when needed. For each function call, you MUST return a JSON object inside <tool_call></tool_call> tags.\\n\\n<tools>' + tools|tojson(indent=2) + '</tools>\\n\\n# Output Format\\n\\nYour response MUST follow this format when making function calls:\\n<tool_call>\\n[\\n  {\"name\": \"function_name\", \"arguments\": {\"arg1\": \"value1\", \"arg2\": \"value2\"}},\\n  {\"name\": \"another_function\", \"arguments\": {\"arg\": \"value\"}}\\n]\\n</tool_call>\\nIf no function calls are needed, respond normally without the tool_call tags.\\n' }}\n{%- for message in remaining_messages %}\n{%- if message['role'] == 'user' %}\n{{- '<|user|>\\n' + message['content'] + '\\n' }}\n{%- elif message['role'] == 'assistant' %}\n{%- if message.content %}\n{{- '<|assistant|>\\n' + message['content'] }}\n{%- endif %}\n{%- if message.tool_calls %}\n{{- '\\n<tool_call>\\n' }}\n{{- message.tool_calls|tojson(indent=2) }}\n{{- '\\n</tool_call>' }}\n{%- endif %}\n{{- eos_token + '\\n' }}\n{%- elif message['role'] == 'tool' %}\n{{- '<|assistant|>\\n<tool_response>\\n' + message['content'] + '\\n</tool_response>\\n' }}\n{%- endif %}\n{%- endfor %}\n{{- '<|assistant|>\\n' if add_generation_prompt }}\n{%- else %}\n{%- for message in messages %}\n{%- if message['role'] == 'system' %}\n{{- '<|system|>\\n' + message['content'] + '\\n' }}\n{%- elif message['role'] == 'user' %}\n{{- '<|user|>\\n' + message['content'] + '\\n' }}\n{%- elif message['role'] == 'assistant' %}\n{%- if not loop.last %}\n{{- '<|assistant|>\\n' + message['content'] + eos_token + '\\n' }}\n{%- else %}\n{{- '<|assistant|>\\n' + message['content'] + eos_token }}\n{%- endif %}\n{%- endif %}\n{%- if loop.last and add_generation_prompt %}\n{{- '<|assistant|>\\n' }}\n{%- endif %}\n{%- endfor %}\n{%- endif %}",
        "context_length": 32768,
        "eos_token": "<|endoftext|>",
        "total": 10305653760
      },
      "id": "cortexso/falcon3",
      "lastModified": "2025-03-03T03:54:15.000Z",
      "likes": 1,
      "model-index": null,
      "modelId": "cortexso/falcon3",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "60030375504feacf3ba4205e8b9809e3dffc2ef7",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "falcon3-10b-instruct-q2_k.gguf"
        },
        {
          "rfilename": "falcon3-10b-instruct-q3_k_l.gguf"
        },
        {
          "rfilename": "falcon3-10b-instruct-q3_k_m.gguf"
        },
        {
          "rfilename": "falcon3-10b-instruct-q3_k_s.gguf"
        },
        {
          "rfilename": "falcon3-10b-instruct-q4_k_m.gguf"
        },
        {
          "rfilename": "falcon3-10b-instruct-q4_k_s.gguf"
        },
        {
          "rfilename": "falcon3-10b-instruct-q5_k_m.gguf"
        },
        {
          "rfilename": "falcon3-10b-instruct-q5_k_s.gguf"
        },
        {
          "rfilename": "falcon3-10b-instruct-q6_k.gguf"
        },
        {
          "rfilename": "falcon3-10b-instruct-q8_0.gguf"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "arxiv:2311.16867",
        "license:apache-2.0",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 65157537088,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "falcon3:10b",
        "size": 6287521312
      }
    ]
  },
  {
    "author": "Qwen",
    "id": "cortexso/qwen2",
    "metadata": {
      "_id": "667917d974da9f6bfc120671",
      "author": "cortexso",
      "cardData": {
        "license": "other",
        "license_link": "https://huggingface.co/Qwen/Qwen2-72B-Instruct/blob/main/LICENSE",
        "license_name": "tongyi-qianwen",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2024-06-24T06:53:13.000Z",
      "description": "---\nlicense: other\nlicense_name: tongyi-qianwen\nlicense_link: https://huggingface.co/Qwen/Qwen2-72B-Instruct/blob/main/LICENSE\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n\n## Overview\n\nQwen2 is the new series of Qwen large language models. For Qwen2, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters, including a Mixture-of-Experts model. This repo contains the instruction-tuned 72B Qwen2 model.\n\n## Variants\n\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Qwen2-7b](https://huggingface.co/cortexso/qwen2/tree/7b) | `cortex run qwen2:7b` |\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexhub/qwen2\n    ```\n    \n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run qwen2\n    ```\n    \n## Credits\n\n- **Author:** Qwen\n- **Converter:** [Homebrew](https://www.homebrew.ltd/)\n- **Original License:** [Licence](https://huggingface.co/Qwen/Qwen2-72B-Instruct/blob/main/LICENSE)",
      "disabled": false,
      "downloads": 130,
      "gated": false,
      "gguf": {
        "architecture": "qwen2",
        "bos_token": "<|endoftext|>",
        "chat_template": "{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}",
        "context_length": 32768,
        "eos_token": "<|im_end|>",
        "total": 7615616512
      },
      "id": "cortexso/qwen2",
      "lastModified": "2025-03-02T15:15:09.000Z",
      "likes": 0,
      "model-index": null,
      "modelId": "cortexso/qwen2",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "e2c6376ad87c7b2da92bc2a2b63ba168d85b1c6d",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        },
        {
          "rfilename": "qwen2-7b-instruct-q2_k.gguf"
        },
        {
          "rfilename": "qwen2-7b-instruct-q3_k_l.gguf"
        },
        {
          "rfilename": "qwen2-7b-instruct-q3_k_m.gguf"
        },
        {
          "rfilename": "qwen2-7b-instruct-q3_k_s.gguf"
        },
        {
          "rfilename": "qwen2-7b-instruct-q4_k_m.gguf"
        },
        {
          "rfilename": "qwen2-7b-instruct-q4_k_s.gguf"
        },
        {
          "rfilename": "qwen2-7b-instruct-q5_k_m.gguf"
        },
        {
          "rfilename": "qwen2-7b-instruct-q5_k_s.gguf"
        },
        {
          "rfilename": "qwen2-7b-instruct-q6_k.gguf"
        },
        {
          "rfilename": "qwen2-7b-instruct-q8_0.gguf"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "license:other",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 53341783520,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "qwen2:7b",
        "size": 4683071456
      }
    ]
  },
  {
    "author": "Nous Research",
    "id": "cortexso/hermes3",
    "metadata": {
      "_id": "675a4743cb0f75e1a3a19ae5",
      "author": "cortexso",
      "cardData": {
        "license": "llama3",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2024-12-12T02:15:31.000Z",
      "description": "---\nlicense: llama3\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n## Overview\n\n**Nous Research** developed and released the [Hermes 3](https://huggingface.co/NousResearch/Hermes-3-Llama-3.2-3B), a state-of-the-art instruction-tuned language model built on Llama-3.2-3B. This 3-billion parameter model is a fine-tuned version of Llama-3.2 and represents a leap forward in reasoning, multi-turn conversation, and structured outputs. It incorporates advanced role-playing capabilities, reliable function calling, and improved coherence over long contexts, making it a versatile assistant for various applications.\n\nHermes 3 was trained with high-quality data, leveraging fine-tuning techniques on H100 GPUs via LambdaLabs GPU Cloud. The model excels in both general-purpose and specialized tasks, including code generation, reasoning, and advanced conversational abilities. With support for ChatML prompt formatting, Hermes 3 ensures compatibility with OpenAI endpoints and facilitates structured, steerable interactions for end-users.\n\n## Variants\n\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Hermes3-3b](https://huggingface.co/cortexso/hermes3/tree/main) | `cortex run hermes3:3b` |\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexso/hermes3\n    ```\n\n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run hermes3\n    ```\n\n## Credits\n\n- **Author:** Nous Research\n- **Converter:** [Homebrew](https://www.homebrew.ltd/)\n- **Original License:** [License](https://huggingface.co/meta-llama/Meta-Llama-3-8B/blob/main/LICENSE)\n- **Papers:** [Hermes 3 Technical Report](https://arxiv.org/pdf/2408.11857)",
      "disabled": false,
      "downloads": 421,
      "gated": false,
      "gguf": {
        "architecture": "llama",
        "bos_token": "<|begin_of_text|>",
        "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}",
        "context_length": 131072,
        "eos_token": "<|im_end|>",
        "total": 3212749888
      },
      "id": "cortexso/hermes3",
      "lastModified": "2025-03-03T02:36:41.000Z",
      "likes": 0,
      "model-index": null,
      "modelId": "cortexso/hermes3",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "b987bf2aa863d1c3590e242aaf5b81a5dc3ea8f3",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "hermes-3-llama-3.2-3b-q2_k.gguf"
        },
        {
          "rfilename": "hermes-3-llama-3.2-3b-q3_k_l.gguf"
        },
        {
          "rfilename": "hermes-3-llama-3.2-3b-q3_k_m.gguf"
        },
        {
          "rfilename": "hermes-3-llama-3.2-3b-q3_k_s.gguf"
        },
        {
          "rfilename": "hermes-3-llama-3.2-3b-q4_k_m.gguf"
        },
        {
          "rfilename": "hermes-3-llama-3.2-3b-q4_k_s.gguf"
        },
        {
          "rfilename": "hermes-3-llama-3.2-3b-q5_k_m.gguf"
        },
        {
          "rfilename": "hermes-3-llama-3.2-3b-q5_k_s.gguf"
        },
        {
          "rfilename": "hermes-3-llama-3.2-3b-q6_k.gguf"
        },
        {
          "rfilename": "hermes-3-llama-3.2-3b-q8_0.gguf"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "arxiv:2408.11857",
        "license:llama3",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 23033625536,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "hermes3:3b",
        "size": 2019373888
      }
    ]
  },
  {
    "author": "Qwen",
    "id": "cortexso/qwen2.5-coder",
    "metadata": {
      "_id": "6732691d254c0b2144f11764",
      "author": "cortexso",
      "cardData": {
        "license": "mit",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2024-11-11T20:29:17.000Z",
      "description": "---\nlicense: mit\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n## Overview\n\n**Qwen Labs** developed and released the [Qwen2.5-Coder](https://huggingface.co/Qwen) model, a state-of-the-art language model tailored for code generation, understanding, and completion tasks. Featuring a 2.5B parameter dense Transformer architecture, Qwen2.5-Coder is designed to assist developers and researchers by generating high-quality code snippets, providing algorithm explanations, and completing coding prompts with accuracy. The model was trained on a diverse blend of programming languages and frameworks using carefully filtered code datasets to ensure precision and relevance. It leverages advanced fine-tuning techniques and rigorous safety measures to optimize instruction adherence and deliver reliable, contextually aware outputs. Released in November 2024, Qwen2.5-Coder offers an effective tool for software development, academic research, and programming education.\n\n## Variants\n\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Qwen2.5-coder-14b](https://huggingface.co/cortexso/qwen2.5-coder/tree/14b) | `cortex run qwen2.5-coder:14b` |\n| 1 | [Qwen2.5-coder-32b](https://huggingface.co/cortexso/qwen2.5-coder/tree/32b) | `cortex run qwen2.5-coder:32b` |\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexso/qwen2.5-coder\n    ```\n\n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run qwen2.5-coder\n    ```\n\n## Credits\n\n- **Author:** Qwen Labs\n- **Converter:** [Homebrew](https://www.homebrew.ltd/)\n- **Original License:** [License](https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct/blob/main/LICENSE)\n- **Papers:** [Qwen2.5-Coder Technical Report](https://arxiv.org/abs/2409.12186)",
      "disabled": false,
      "downloads": 1369,
      "gated": false,
      "gguf": {
        "architecture": "qwen2",
        "bos_token": "<|endoftext|>",
        "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n",
        "context_length": 32768,
        "eos_token": "<|im_end|>",
        "total": 14770033664
      },
      "id": "cortexso/qwen2.5-coder",
      "lastModified": "2025-03-03T04:26:33.000Z",
      "likes": 0,
      "model-index": null,
      "modelId": "cortexso/qwen2.5-coder",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "b472c129cc68732d81e50ce48e621fe1861e8d1c",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        },
        {
          "rfilename": "qwen2.5-coder-14b-instruct-q2_k.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-14b-instruct-q3_k_l.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-14b-instruct-q3_k_m.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-14b-instruct-q3_k_s.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-14b-instruct-q4_k_m.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-14b-instruct-q4_k_s.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-14b-instruct-q5_k_m.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-14b-instruct-q5_k_s.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-14b-instruct-q6_k.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-14b-instruct-q8_0.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-32b-instruct-q2_k.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-32b-instruct-q3_k_l.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-32b-instruct-q3_k_m.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-32b-instruct-q3_k_s.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-32b-instruct-q4_k_m.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-32b-instruct-q4_k_s.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-32b-instruct-q5_k_m.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-32b-instruct-q5_k_s.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-32b-instruct-q6_k.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-32b-instruct-q8_0.gguf"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "arxiv:2409.12186",
        "license:mit",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 328827521152,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "qwen2.5-coder:32b",
        "size": 19851336256
      },
      {
        "id": "qwen2.5-coder:14b",
        "size": 8988110656
      }
    ]
  },
  {
    "author": "Microsoft",
    "id": "cortexso/phi-3.5",
    "metadata": {
      "_id": "67211d1b527f6fcd90b9dca3",
      "author": "cortexso",
      "cardData": {
        "license": "mit",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2024-10-29T17:36:27.000Z",
      "description": "---\nlicense: mit\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n\n## Overview\n**Microsoft** developed and released the [Phi-3.5](https://huggingface.co/microsoft/Phi-3.5-mini-instruct) model, a state-of-the-art large language model built upon the Phi-3 architecture. With its focus on high-quality, reasoning-dense data, this model represents a significant advancement in instruction-tuned language models. Phi-3.5 has been fine-tuned through supervised learning, proximal policy optimization (PPO), and direct preference optimization (DPO) to ensure precise instruction following and robust safety measures. Supporting a 128K token context length, the model demonstrates exceptional performance in tasks requiring extended context understanding and complex reasoning. The model's training data consists of synthetic datasets and carefully filtered publicly available web content, inheriting the high-quality foundation established in the Phi-3 series.\n\n## Variants\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Phi-3.5-3b](https://huggingface.co/cortexso/phi-3.5/tree/3b) | `cortex run phi-3.5:3b` |\n\n## Use it with Jan (UI)\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexso/phi-3.5\n    ```\n\n## Use it with Cortex (CLI)\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run phi-3.5\n    ```\n\n## Credits\n- **Author:** Microsoft\n- **Converter:** [Homebrew](https://www.homebrew.ltd/)\n- **Original License:** [License](https://choosealicense.com/licenses/mit/)\n- **Papers:** [Phi-3.5 Paper](https://arxiv.org/abs/2404.14219)",
      "disabled": false,
      "downloads": 299,
      "gated": false,
      "gguf": {
        "architecture": "phi3",
        "bos_token": "<s>",
        "chat_template": "{% for message in messages %}{% if message['role'] == 'system' and message['content'] %}{{'<|system|>\n' + message['content'] + '<|end|>\n'}}{% elif message['role'] == 'user' %}{{'<|user|>\n' + message['content'] + '<|end|>\n'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\n' + message['content'] + '<|end|>\n'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n' }}{% else %}{{ eos_token }}{% endif %}",
        "context_length": 131072,
        "eos_token": "<|endoftext|>",
        "total": 3821079648
      },
      "id": "cortexso/phi-3.5",
      "lastModified": "2025-03-03T05:42:47.000Z",
      "likes": 0,
      "model-index": null,
      "modelId": "cortexso/phi-3.5",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "7fd139ae9bdff00feae40ad3e4d7ce6dc0c48a91",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        },
        {
          "rfilename": "phi-3.5-mini-instruct-q2_k.gguf"
        },
        {
          "rfilename": "phi-3.5-mini-instruct-q3_k_l.gguf"
        },
        {
          "rfilename": "phi-3.5-mini-instruct-q3_k_m.gguf"
        },
        {
          "rfilename": "phi-3.5-mini-instruct-q3_k_s.gguf"
        },
        {
          "rfilename": "phi-3.5-mini-instruct-q4_k_m.gguf"
        },
        {
          "rfilename": "phi-3.5-mini-instruct-q4_k_s.gguf"
        },
        {
          "rfilename": "phi-3.5-mini-instruct-q5_k_m.gguf"
        },
        {
          "rfilename": "phi-3.5-mini-instruct-q5_k_s.gguf"
        },
        {
          "rfilename": "phi-3.5-mini-instruct-q6_k.gguf"
        },
        {
          "rfilename": "phi-3.5-mini-instruct-q8_0.gguf"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "arxiv:2404.14219",
        "license:mit",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 26770128384,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "phi-3.5:3b",
        "size": 2393232384
      }
    ]
  },
  {
    "author": "meta-llama",
    "id": "cortexso/llama3.3",
    "metadata": {
      "_id": "67568c9b6ac1ee73523d7623",
      "author": "cortexso",
      "cardData": {
        "license": "mit",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2024-12-09T06:22:19.000Z",
      "description": "---\nlicense: mit\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n## Overview\n\n**Meta** developed and released the [Llama3.3](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) model, a state-of-the-art multilingual large language model designed for instruction-tuned generative tasks. With 70 billion parameters, this model is optimized for multilingual dialogue use cases, providing high-quality text input and output. Llama3.3 has been fine-tuned through supervised learning and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. It sets a new standard in performance, outperforming many open-source and closed-source chat models on common industry benchmarks. The model’s capabilities make it a powerful tool for applications requiring conversational AI, multilingual support, and instruction adherence.\n\n## Variants\n\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Llama3.3-70b](https://huggingface.co/cortexso/llama3.3/tree/70b) | `cortex run llama3.3:70b` |\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexso/llama3.3\n    ```\n\n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run llama3.3\n    ```\n\n## Credits\n\n- **Author:** Meta\n- **Converter:** [Homebrew](https://www.homebrew.ltd/)\n- **Original License:** [License](https://llama.meta.com/llama3/license/)\n- **Papers:** [Llama-3 Blog](https://llama.meta.com/llama3/)",
      "disabled": false,
      "downloads": 964,
      "gated": false,
      "gguf": {
        "architecture": "llama",
        "bos_token": "<|begin_of_text|>",
        "chat_template": "{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- set date_string = \"26 Jul 2024\" %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = \"\" %}\n{%- endif %}\n\n{#- System message + builtin tools #}\n{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n{%- if builtin_tools is defined or tools is not none %}\n    {{- \"Environment: ipython\\n\" }}\n{%- endif %}\n{%- if builtin_tools is defined %}\n    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n{%- endif %}\n{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- \"<|eot_id|>\" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0]['content']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n{%- endif %}\n    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n    {{- first_user_message + \"<|eot_id|>\"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n    {%- elif 'tool_calls' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\n                {{- arg_name + '=\"' + arg_val + '\"' }}\n                {%- if not loop.last %}\n                    {{- \", \" }}\n                {%- endif %}\n                {%- endfor %}\n            {{- \")\" }}\n        {%- else  %}\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n            {{- '\"parameters\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \"}\" }}\n        {%- endif %}\n        {%- if builtin_tools is defined %}\n            {#- This means we're in ipython mode #}\n            {{- \"<|eom_id|>\" }}\n        {%- else %}\n            {{- \"<|eot_id|>\" }}\n        {%- endif %}\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- \"<|eot_id|>\" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}\n",
        "context_length": 131072,
        "eos_token": "<|eot_id|>",
        "total": 70553706560
      },
      "id": "cortexso/llama3.3",
      "lastModified": "2025-03-03T03:59:38.000Z",
      "likes": 0,
      "model-index": null,
      "modelId": "cortexso/llama3.3",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "9cc0507ae02f03cf59c630c1ffa5d369441e27eb",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "llama-3.3-70b-instruct-q4_k_m.gguf"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "license:mit",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 42520398432,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "llama3.3:70b",
        "size": 42520398432
      }
    ]
  },
  {
    "author": "inftech.ai",
    "id": "cortexso/opencoder",
    "metadata": {
      "_id": "672fb2f43db04d9bf3f4c393",
      "author": "cortexso",
      "cardData": {
        "license": "other",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2024-11-09T19:07:32.000Z",
      "description": "---\nlicense: other\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n\n## Overview\n\nOpenCoder is an open and reproducible code LLM family, featuring 1.5B and 8B base and chat models that support both English and Chinese languages. Built from scratch, OpenCoder is pretrained on 2.5 trillion tokens, composed of 90% raw code and 10% code-related web data. It undergoes supervised fine-tuning (SFT) with over 4.5 million high-quality examples, achieving performance on par with top-tier code LLMs\n\n## Variants\n\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Opencoder-8b](https://huggingface.co/cortexso/opencoder/tree/8b) | `cortex run opencoder:8b` |\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexhub/opencoder\n    ```\n\n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run opencoder\n    ```\n    \n## Credits\n\n- **Author:** inftech.ai\n- **Converter:** [Homebrew](https://homebrew.ltd/)\n- **Original License:** [Licence](https://huggingface.co/infly/OpenCoder-8B-Instruct/blob/main/LICENSE)\n- **Papers:** [Paper](https://arxiv.org/abs/2411.04905)",
      "disabled": false,
      "downloads": 650,
      "gated": false,
      "gguf": {
        "architecture": "llama",
        "bos_token": "<|im_start|>",
        "chat_template": "{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\nYou are OpenCoder, created by OpenCoder Team.<|im_end|>\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}",
        "context_length": 8192,
        "eos_token": "<|im_end|>",
        "total": 7771262976
      },
      "id": "cortexso/opencoder",
      "lastModified": "2025-03-03T02:25:59.000Z",
      "likes": 0,
      "model-index": null,
      "modelId": "cortexso/opencoder",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "2b98756c8b01811470941deb8a0259de3dd4018c",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        },
        {
          "rfilename": "opencoder-8b-instruct-q2_k.gguf"
        },
        {
          "rfilename": "opencoder-8b-instruct-q3_k_l.gguf"
        },
        {
          "rfilename": "opencoder-8b-instruct-q3_k_m.gguf"
        },
        {
          "rfilename": "opencoder-8b-instruct-q3_k_s.gguf"
        },
        {
          "rfilename": "opencoder-8b-instruct-q4_k_m.gguf"
        },
        {
          "rfilename": "opencoder-8b-instruct-q4_k_s.gguf"
        },
        {
          "rfilename": "opencoder-8b-instruct-q5_k_m.gguf"
        },
        {
          "rfilename": "opencoder-8b-instruct-q5_k_s.gguf"
        },
        {
          "rfilename": "opencoder-8b-instruct-q6_k.gguf"
        },
        {
          "rfilename": "opencoder-8b-instruct-q8_0.gguf"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "arxiv:2411.04905",
        "license:other",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 54076349664,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "opencoder:8b",
        "size": 4736059168
      }
    ]
  },
  {
    "author": "Google",
    "id": "cortexso/gemma",
    "metadata": {
      "_id": "6667b642f760460127737cc6",
      "author": "cortexso",
      "cardData": {
        "license": "gemma",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2024-06-11T02:28:18.000Z",
      "description": "---\nlicense: gemma\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n\n## Overview\n\nThe [Gemma](https://huggingface.co/google/gemma-7b), state-of-the-art open model trained with the Gemma datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties. The model belongs to the Gemma family with the 4B, 7B version in two variants 8K and 128K which is the context length (in tokens) that it can support.\n\n## Variants\n\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Gemma-7b](https://huggingface.co/cortexso/gemma/tree/7b) | `cortex run gemma:7b` |\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexso/gemma\n    ```\n    \n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run gemma\n    ```\n    \n## Credits\n\n- **Author:** Go‌ogle\n- **Converter:** [Homebrew](https://www.homebrew.ltd/)\n- **Original License:** [License](https://ai.google.dev/gemma/terms)\n- **Papers:** [Gemma Technical Report](https://arxiv.org/abs/2403.08295)",
      "disabled": false,
      "downloads": 280,
      "gated": false,
      "gguf": {
        "architecture": "gemma",
        "bos_token": "<bos>",
        "chat_template": "{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n' + message['content'] | trim + '<end_of_turn>\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n'}}{% endif %}",
        "context_length": 8192,
        "eos_token": "<eos>",
        "total": 8537680896
      },
      "id": "cortexso/gemma",
      "lastModified": "2025-03-03T06:14:39.000Z",
      "likes": 0,
      "model-index": null,
      "modelId": "cortexso/gemma",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "801b78a606397281d5953e5e8f2a64b6158e2db2",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "gemma-7b-it-q2_k.gguf"
        },
        {
          "rfilename": "gemma-7b-it-q3_k_l.gguf"
        },
        {
          "rfilename": "gemma-7b-it-q3_k_m.gguf"
        },
        {
          "rfilename": "gemma-7b-it-q3_k_s.gguf"
        },
        {
          "rfilename": "gemma-7b-it-q4_k_m.gguf"
        },
        {
          "rfilename": "gemma-7b-it-q4_k_s.gguf"
        },
        {
          "rfilename": "gemma-7b-it-q5_k_m.gguf"
        },
        {
          "rfilename": "gemma-7b-it-q5_k_s.gguf"
        },
        {
          "rfilename": "gemma-7b-it-q6_k.gguf"
        },
        {
          "rfilename": "gemma-7b-it-q8_0.gguf"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "arxiv:2403.08295",
        "license:gemma",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 60258935328,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "gemma:7b",
        "size": 5329759680
      }
    ]
  },
  {
    "author": "MistralAI",
    "id": "cortexso/mistral-nemo",
    "metadata": {
      "_id": "66f4e292515759ca6d5287bd",
      "author": "cortexso",
      "cardData": {
        "license": "apache-2.0",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2024-09-26T04:26:58.000Z",
      "description": "---\nlicense: apache-2.0\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n\n## Overview\n\nMistralai developed and released the [Mistral-Nemo](https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407) family of large language models (LLMs).\n\n## Variants\n\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Mistral-nemo-12b](https://huggingface.co/cortexso/mistral-nemo/tree/12b) | `cortex run mistral-nemo:12b` ||\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexso/mistral-nemo\n    ```\n\n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run mistral-nemo\n    ```\n\n## Credits\n\n- **Author:** MistralAI\n- **Converter:** [Homebrew](https://www.homebrew.ltd/)\n- **Original License:** [Apache 2 License](https://choosealicense.com/licenses/apache-2.0/)\n- **Papers:** [Mistral Nemo Blog](https://mistral.ai/news/mistral-nemo/)",
      "disabled": false,
      "downloads": 546,
      "gated": false,
      "gguf": {
        "architecture": "llama",
        "bos_token": "<s>",
        "chat_template": "{%- if messages[0][\"role\"] == \"system\" %}\n    {%- set system_message = messages[0][\"content\"] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n{%- set user_messages = loop_messages | selectattr(\"role\", \"equalto\", \"user\") | list %}\n\n{#- This block checks for alternating user/assistant messages, skipping tool calling messages #}\n{%- set ns = namespace() %}\n{%- set ns.index = 0 %}\n{%- for message in loop_messages %}\n    {%- if not (message.role == \"tool\" or message.role == \"tool_results\" or (message.tool_calls is defined and message.tool_calls is not none)) %}\n        {%- if (message[\"role\"] == \"user\") != (ns.index % 2 == 0) %}\n            {{- raise_exception(\"After the optional system message, conversation roles must alternate user/assistant/user/assistant/...\") }}\n        {%- endif %}\n        {%- set ns.index = ns.index + 1 %}\n    {%- endif %}\n{%- endfor %}\n\n{{- bos_token }}\n{%- for message in loop_messages %}\n    {%- if message[\"role\"] == \"user\" %}\n        {%- if tools is not none and (message == user_messages[-1]) %}\n            {{- \"[AVAILABLE_TOOLS][\" }}\n            {%- for tool in tools %}\n                {%- set tool = tool.function %}\n                {{- '{\"type\": \"function\", \"function\": {' }}\n                {%- for key, val in tool.items() if key != \"return\" %}\n                    {%- if val is string %}\n                        {{- '\"' + key + '\": \"' + val + '\"' }}\n                    {%- else %}\n                        {{- '\"' + key + '\": ' + val|tojson }}\n                    {%- endif %}\n                    {%- if not loop.last %}\n                        {{- \", \" }}\n                    {%- endif %}\n                {%- endfor %}\n                {{- \"}}\" }}\n                {%- if not loop.last %}\n                    {{- \", \" }}\n                {%- else %}\n                    {{- \"]\" }}\n                {%- endif %}\n            {%- endfor %}\n            {{- \"[/AVAILABLE_TOOLS]\" }}\n            {%- endif %}\n        {%- if loop.last and system_message is defined %}\n            {{- \"[INST]\" + system_message + \"\\n\\n\" + message[\"content\"] + \"[/INST]\" }}\n        {%- else %}\n            {{- \"[INST]\" + message[\"content\"] + \"[/INST]\" }}\n        {%- endif %}\n    {%- elif (message.tool_calls is defined and message.tool_calls is not none) %}\n        {{- \"[TOOL_CALLS][\" }}\n        {%- for tool_call in message.tool_calls %}\n            {%- set out = tool_call.function|tojson %}\n            {{- out[:-1] }}\n            {%- if not tool_call.id is defined or tool_call.id|length != 9 %}\n                {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n            {%- endif %}\n            {{- ', \"id\": \"' + tool_call.id + '\"}' }}\n            {%- if not loop.last %}\n                {{- \", \" }}\n            {%- else %}\n                {{- \"]\" + eos_token }}\n            {%- endif %}\n        {%- endfor %}\n    {%- elif message[\"role\"] == \"assistant\" %}\n        {{- message[\"content\"] + eos_token}}\n    {%- elif message[\"role\"] == \"tool_results\" or message[\"role\"] == \"tool\" %}\n        {%- if message.content is defined and message.content.content is defined %}\n            {%- set content = message.content.content %}\n        {%- else %}\n            {%- set content = message.content %}\n        {%- endif %}\n        {{- '[TOOL_RESULTS]{\"content\": ' + content|string + \", \" }}\n        {%- if not message.tool_call_id is defined or message.tool_call_id|length != 9 %}\n            {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n        {%- endif %}\n        {{- '\"call_id\": \"' + message.tool_call_id + '\"}[/TOOL_RESULTS]' }}\n    {%- else %}\n        {{- raise_exception(\"Only user and assistant roles are supported, with the exception of an initial optional system message!\") }}\n    {%- endif %}\n{%- endfor %}\n",
        "context_length": 131072,
        "eos_token": "</s>",
        "total": 12247782400
      },
      "id": "cortexso/mistral-nemo",
      "lastModified": "2025-03-03T02:42:16.000Z",
      "likes": 0,
      "model-index": null,
      "modelId": "cortexso/mistral-nemo",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "487a202e44ea08566ab73ed16b5f7f685d12cf6b",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "mistral-nemo-instruct-2407-q2_k.gguf"
        },
        {
          "rfilename": "mistral-nemo-instruct-2407-q3_k_l.gguf"
        },
        {
          "rfilename": "mistral-nemo-instruct-2407-q3_k_m.gguf"
        },
        {
          "rfilename": "mistral-nemo-instruct-2407-q3_k_s.gguf"
        },
        {
          "rfilename": "mistral-nemo-instruct-2407-q4_k_m.gguf"
        },
        {
          "rfilename": "mistral-nemo-instruct-2407-q4_k_s.gguf"
        },
        {
          "rfilename": "mistral-nemo-instruct-2407-q5_k_m.gguf"
        },
        {
          "rfilename": "mistral-nemo-instruct-2407-q5_k_s.gguf"
        },
        {
          "rfilename": "mistral-nemo-instruct-2407-q6_k.gguf"
        },
        {
          "rfilename": "mistral-nemo-instruct-2407-q8_0.gguf"
        },
        {
          "rfilename": "model.yml"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "license:apache-2.0",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 85369454144,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "mistral-nemo:12b",
        "size": 7477207744
      }
    ]
  },
  {
    "author": "meta-llama",
    "id": "cortexso/llama3.2",
    "metadata": {
      "_id": "66f63309ba963b1db95deaa4",
      "author": "cortexso",
      "cardData": {
        "license": "llama3.2",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp", "featured"]
      },
      "createdAt": "2024-09-27T04:22:33.000Z",
      "description": "---\nlicense: llama3.2\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n- featured\n---\n\n## Overview\n\nMeta developed and released the [Meta Llama 3.2](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct) family of large language models (LLMs), a collection of pretrained and instruction tuned generative text models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks.\n\n## Variants\n\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [LLama3.2-1b](https://huggingface.co/cortexso/llama3.2/tree/1b) | `cortex run llama3.2:1b` |\n| 2 | [LLama3.2-3b](https://huggingface.co/cortexso/llama3.2/tree/3b) | `cortex run llama3.2:3b` |\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexso/llama3.2\n    ```\n\n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run llama3.2\n    ```\n\n## Credits\n\n- **Author:** Meta\n- **Converter:** [Homebrew](https://www.homebrew.ltd/)\n- **Original License:** [License](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct/blob/main/LICENSE.txt)\n- **Papers:** [Llama-3.2 Blog](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/)",
      "disabled": false,
      "downloads": 11227,
      "gated": false,
      "gguf": {
        "architecture": "llama",
        "bos_token": "<|begin_of_text|>",
        "chat_template": "{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- if strftime_now is defined %}\n        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n    {%- else %}\n        {%- set date_string = \"26 Jul 2024\" %}\n    {%- endif %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = \"\" %}\n{%- endif %}\n\n{#- System message #}\n{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n{%- if tools is not none %}\n    {{- \"Environment: ipython\\n\" }}\n{%- endif %}\n{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- \"<|eot_id|>\" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0]['content']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n{%- endif %}\n    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n    {{- first_user_message + \"<|eot_id|>\"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n    {%- elif 'tool_calls' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n        {{- '\"parameters\": ' }}\n        {{- tool_call.arguments | tojson }}\n        {{- \"}\" }}\n        {{- \"<|eot_id|>\" }}\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- \"<|eot_id|>\" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}\n",
        "context_length": 131072,
        "eos_token": "<|eot_id|>",
        "total": 1235814432
      },
      "id": "cortexso/llama3.2",
      "lastModified": "2025-03-03T06:22:08.000Z",
      "likes": 0,
      "model-index": null,
      "modelId": "cortexso/llama3.2",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "73313225fbeff0cebf5ccf48121cba6ca1a80e7d",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "llama-3.2-1b-instruct-q2_k.gguf"
        },
        {
          "rfilename": "llama-3.2-1b-instruct-q3_k_l.gguf"
        },
        {
          "rfilename": "llama-3.2-1b-instruct-q3_k_m.gguf"
        },
        {
          "rfilename": "llama-3.2-1b-instruct-q3_k_s.gguf"
        },
        {
          "rfilename": "llama-3.2-1b-instruct-q4_k_m.gguf"
        },
        {
          "rfilename": "llama-3.2-1b-instruct-q4_k_s.gguf"
        },
        {
          "rfilename": "llama-3.2-1b-instruct-q5_k_m.gguf"
        },
        {
          "rfilename": "llama-3.2-1b-instruct-q5_k_s.gguf"
        },
        {
          "rfilename": "llama-3.2-1b-instruct-q6_k.gguf"
        },
        {
          "rfilename": "llama-3.2-1b-instruct-q8_0.gguf"
        },
        {
          "rfilename": "llama-3.2-3b-instruct-q2_k.gguf"
        },
        {
          "rfilename": "llama-3.2-3b-instruct-q3_k_l.gguf"
        },
        {
          "rfilename": "llama-3.2-3b-instruct-q3_k_m.gguf"
        },
        {
          "rfilename": "llama-3.2-3b-instruct-q3_k_s.gguf"
        },
        {
          "rfilename": "llama-3.2-3b-instruct-q4_k_m.gguf"
        },
        {
          "rfilename": "llama-3.2-3b-instruct-q4_k_s.gguf"
        },
        {
          "rfilename": "llama-3.2-3b-instruct-q5_k_m.gguf"
        },
        {
          "rfilename": "llama-3.2-3b-instruct-q5_k_s.gguf"
        },
        {
          "rfilename": "llama-3.2-3b-instruct-q6_k.gguf"
        },
        {
          "rfilename": "llama-3.2-3b-instruct-q8_0.gguf"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "featured",
        "text-generation",
        "license:llama3.2",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 31409886432,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "llama3.2:3b",
        "size": 2019377312
      },
      {
        "id": "llama3.2:1b",
        "size": 911503104
      }
    ]
  },
  {
    "author": "Qwen",
    "id": "cortexso/qwen2.5",
    "metadata": {
      "_id": "671d0d55748faf685e6450a3",
      "author": "cortexso",
      "cardData": {
        "license": "apache-2.0",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2024-10-26T15:40:05.000Z",
      "description": "---\nlicense: apache-2.0\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n\n## Overview\n\nQwen2.5 by Qwen is a family of model include various specialized models for coding and mathematics available in multiple sizes from 0.5B to 72B parameters\n\n## Variants\n\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Qwen-2.5-0.5b](https://huggingface.co/cortexso/qwen2.5/tree/0.5b) | `cortex run qwen2.5:0.5b` |\n| 2 | [Qwen-2.5-1.5b](https://huggingface.co/cortexso/qwen2.5/tree/1.5b) | `cortex run qwen2.5:1.5b` |\n| 3 | [Qwen-2.5-3b](https://huggingface.co/cortexso/qwen2.5/tree/3b) | `cortex run qwen2.5:3b` |\n| 4 | [Qwen-2.5-7b](https://huggingface.co/cortexso/qwen2.5/tree/7b) | `cortex run qwen2.5:7b` |\n| 5 | [Qwen-2.5-14b](https://huggingface.co/cortexso/qwen2.5/tree/14b) | `cortex run qwen2.5:14b` |\n| 6 | [Qwen-2.5-32b](https://huggingface.co/cortexso/qwen2.5/tree/32b) | `cortex run qwen2.5:32b` |\n| 7 | [Qwen-2.5-72b](https://huggingface.co/cortexso/qwen2.5/tree/72b) | `cortex run qwen2.5:72b` |\n| 8 | [Qwen-2.5-coder-1.5b](https://huggingface.co/cortexso/qwen2.5/tree/coder-1.5b) | `cortex run qwen2.5:coder-1.5b` |\n| 9 | [Qwen-2.5-coder-7b](https://huggingface.co/cortexso/qwen2.5/tree/coder-7b) | `cortex run qwen2.5:coder-7b` |\n| 10 | [Qwen-2.5-math-1.5b](https://huggingface.co/cortexso/qwen2.5/tree/math-1.5b) | `cortex run qwen2.5:math-1.5b` |\n| 11 | [Qwen-2.5-math-7b](https://huggingface.co/cortexso/qwen2.5/tree/math-7b) | `cortex run qwen2.5:math-7b` |\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```\n    cortexso/qwen2.5\n    ```\n\n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```\n    cortex run qwen2.5\n    ```\n\n## Credits\n\n- **Author:** Qwen\n- **Converter:** [Homebrew](https://www.homebrew.ltd/)\n- **Original License:** [License Apache 2.0](https://choosealicense.com/licenses/apache-2.0/)\n- **Papers:** [Qwen2.5 Blog](https://qwenlm.github.io/blog/qwen2.5/)",
      "disabled": false,
      "downloads": 3608,
      "gated": false,
      "gguf": {
        "architecture": "qwen2",
        "bos_token": "<|endoftext|>",
        "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n",
        "context_length": 32768,
        "eos_token": "<|im_end|>",
        "total": 494032768
      },
      "id": "cortexso/qwen2.5",
      "lastModified": "2025-03-03T04:07:15.000Z",
      "likes": 1,
      "model-index": null,
      "modelId": "cortexso/qwen2.5",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "d801e60d205491ab449425f3779b13bedbbe463d",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        },
        {
          "rfilename": "qwen2.5-0.5b-instruct-q2_k.gguf"
        },
        {
          "rfilename": "qwen2.5-0.5b-instruct-q3_k_l.gguf"
        },
        {
          "rfilename": "qwen2.5-0.5b-instruct-q3_k_m.gguf"
        },
        {
          "rfilename": "qwen2.5-0.5b-instruct-q3_k_s.gguf"
        },
        {
          "rfilename": "qwen2.5-0.5b-instruct-q4_k_m.gguf"
        },
        {
          "rfilename": "qwen2.5-0.5b-instruct-q4_k_s.gguf"
        },
        {
          "rfilename": "qwen2.5-0.5b-instruct-q5_k_m.gguf"
        },
        {
          "rfilename": "qwen2.5-0.5b-instruct-q5_k_s.gguf"
        },
        {
          "rfilename": "qwen2.5-0.5b-instruct-q6_k.gguf"
        },
        {
          "rfilename": "qwen2.5-0.5b-instruct-q8_0.gguf"
        },
        {
          "rfilename": "qwen2.5-1.5b-instruct-q2_k.gguf"
        },
        {
          "rfilename": "qwen2.5-1.5b-instruct-q3_k_l.gguf"
        },
        {
          "rfilename": "qwen2.5-1.5b-instruct-q3_k_m.gguf"
        },
        {
          "rfilename": "qwen2.5-1.5b-instruct-q3_k_s.gguf"
        },
        {
          "rfilename": "qwen2.5-1.5b-instruct-q4_k_m.gguf"
        },
        {
          "rfilename": "qwen2.5-1.5b-instruct-q4_k_s.gguf"
        },
        {
          "rfilename": "qwen2.5-1.5b-instruct-q5_k_m.gguf"
        },
        {
          "rfilename": "qwen2.5-1.5b-instruct-q5_k_s.gguf"
        },
        {
          "rfilename": "qwen2.5-1.5b-instruct-q6_k.gguf"
        },
        {
          "rfilename": "qwen2.5-1.5b-instruct-q8_0.gguf"
        },
        {
          "rfilename": "qwen2.5-14b-instruct-q2_k.gguf"
        },
        {
          "rfilename": "qwen2.5-14b-instruct-q3_k_l.gguf"
        },
        {
          "rfilename": "qwen2.5-14b-instruct-q3_k_m.gguf"
        },
        {
          "rfilename": "qwen2.5-14b-instruct-q3_k_s.gguf"
        },
        {
          "rfilename": "qwen2.5-14b-instruct-q4_k_m.gguf"
        },
        {
          "rfilename": "qwen2.5-14b-instruct-q4_k_s.gguf"
        },
        {
          "rfilename": "qwen2.5-14b-instruct-q5_k_m.gguf"
        },
        {
          "rfilename": "qwen2.5-14b-instruct-q5_k_s.gguf"
        },
        {
          "rfilename": "qwen2.5-14b-instruct-q6_k.gguf"
        },
        {
          "rfilename": "qwen2.5-14b-instruct-q8_0.gguf"
        },
        {
          "rfilename": "qwen2.5-32b-instruct-q2_k.gguf"
        },
        {
          "rfilename": "qwen2.5-32b-instruct-q3_k_l.gguf"
        },
        {
          "rfilename": "qwen2.5-32b-instruct-q3_k_m.gguf"
        },
        {
          "rfilename": "qwen2.5-32b-instruct-q3_k_s.gguf"
        },
        {
          "rfilename": "qwen2.5-32b-instruct-q4_k_m.gguf"
        },
        {
          "rfilename": "qwen2.5-32b-instruct-q4_k_s.gguf"
        },
        {
          "rfilename": "qwen2.5-32b-instruct-q5_k_m.gguf"
        },
        {
          "rfilename": "qwen2.5-32b-instruct-q5_k_s.gguf"
        },
        {
          "rfilename": "qwen2.5-32b-instruct-q6_k.gguf"
        },
        {
          "rfilename": "qwen2.5-32b-instruct-q8_0.gguf"
        },
        {
          "rfilename": "qwen2.5-3b-instruct-q2_k.gguf"
        },
        {
          "rfilename": "qwen2.5-3b-instruct-q3_k_l.gguf"
        },
        {
          "rfilename": "qwen2.5-3b-instruct-q3_k_m.gguf"
        },
        {
          "rfilename": "qwen2.5-3b-instruct-q3_k_s.gguf"
        },
        {
          "rfilename": "qwen2.5-3b-instruct-q4_k_m.gguf"
        },
        {
          "rfilename": "qwen2.5-3b-instruct-q4_k_s.gguf"
        },
        {
          "rfilename": "qwen2.5-3b-instruct-q5_k_m.gguf"
        },
        {
          "rfilename": "qwen2.5-3b-instruct-q5_k_s.gguf"
        },
        {
          "rfilename": "qwen2.5-3b-instruct-q6_k.gguf"
        },
        {
          "rfilename": "qwen2.5-3b-instruct-q8_0.gguf"
        },
        {
          "rfilename": "qwen2.5-72b-instruct-q4_k_m.gguf"
        },
        {
          "rfilename": "qwen2.5-7b-instruct-q2_k.gguf"
        },
        {
          "rfilename": "qwen2.5-7b-instruct-q3_k_l.gguf"
        },
        {
          "rfilename": "qwen2.5-7b-instruct-q3_k_m.gguf"
        },
        {
          "rfilename": "qwen2.5-7b-instruct-q3_k_s.gguf"
        },
        {
          "rfilename": "qwen2.5-7b-instruct-q4_k_m.gguf"
        },
        {
          "rfilename": "qwen2.5-7b-instruct-q4_k_s.gguf"
        },
        {
          "rfilename": "qwen2.5-7b-instruct-q5_k_m.gguf"
        },
        {
          "rfilename": "qwen2.5-7b-instruct-q5_k_s.gguf"
        },
        {
          "rfilename": "qwen2.5-7b-instruct-q6_k.gguf"
        },
        {
          "rfilename": "qwen2.5-7b-instruct-q8_0.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-1.5b-instruct-q2_k.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-1.5b-instruct-q3_k_l.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-1.5b-instruct-q3_k_m.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-1.5b-instruct-q3_k_s.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-1.5b-instruct-q4_k_m.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-1.5b-instruct-q4_k_s.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-1.5b-instruct-q5_k_m.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-1.5b-instruct-q5_k_s.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-1.5b-instruct-q6_k.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-1.5b-instruct-q8_0.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-7b-instruct-q2_k.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-7b-instruct-q3_k_l.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-7b-instruct-q3_k_m.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-7b-instruct-q3_k_s.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-7b-instruct-q4_k_m.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-7b-instruct-q4_k_s.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-7b-instruct-q5_k_m.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-7b-instruct-q5_k_s.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-7b-instruct-q6_k.gguf"
        },
        {
          "rfilename": "qwen2.5-coder-7b-instruct-q8_0.gguf"
        },
        {
          "rfilename": "qwen2.5-math-1.5b-instruct-q2_k.gguf"
        },
        {
          "rfilename": "qwen2.5-math-1.5b-instruct-q3_k_l.gguf"
        },
        {
          "rfilename": "qwen2.5-math-1.5b-instruct-q3_k_m.gguf"
        },
        {
          "rfilename": "qwen2.5-math-1.5b-instruct-q3_k_s.gguf"
        },
        {
          "rfilename": "qwen2.5-math-1.5b-instruct-q4_k_m.gguf"
        },
        {
          "rfilename": "qwen2.5-math-1.5b-instruct-q4_k_s.gguf"
        },
        {
          "rfilename": "qwen2.5-math-1.5b-instruct-q5_k_m.gguf"
        },
        {
          "rfilename": "qwen2.5-math-1.5b-instruct-q5_k_s.gguf"
        },
        {
          "rfilename": "qwen2.5-math-1.5b-instruct-q6_k.gguf"
        },
        {
          "rfilename": "qwen2.5-math-1.5b-instruct-q8_0.gguf"
        },
        {
          "rfilename": "qwen2.5-math-7b-instruct-q2_k.gguf"
        },
        {
          "rfilename": "qwen2.5-math-7b-instruct-q3_k_l.gguf"
        },
        {
          "rfilename": "qwen2.5-math-7b-instruct-q3_k_m.gguf"
        },
        {
          "rfilename": "qwen2.5-math-7b-instruct-q3_k_s.gguf"
        },
        {
          "rfilename": "qwen2.5-math-7b-instruct-q4_k_m.gguf"
        },
        {
          "rfilename": "qwen2.5-math-7b-instruct-q4_k_s.gguf"
        },
        {
          "rfilename": "qwen2.5-math-7b-instruct-q5_k_m.gguf"
        },
        {
          "rfilename": "qwen2.5-math-7b-instruct-q5_k_s.gguf"
        },
        {
          "rfilename": "qwen2.5-math-7b-instruct-q6_k.gguf"
        },
        {
          "rfilename": "qwen2.5-math-7b-instruct-q8_0.gguf"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "license:apache-2.0",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 596251612960,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "qwen2.5:1.5b",
        "size": 986048384
      },
      {
        "id": "qwen2.5:math-1.5b",
        "size": 986048416
      },
      {
        "id": "qwen2.5:3b",
        "size": 1929902912
      },
      {
        "id": "qwen2.5:14b",
        "size": 8988110592
      },
      {
        "id": "qwen2.5:0.5b",
        "size": 397807808
      },
      {
        "id": "qwen2.5:72b",
        "size": 47415715104
      },
      {
        "id": "qwen2.5:coder-1.5b",
        "size": 986048480
      },
      {
        "id": "qwen2.5:32b",
        "size": 19851336192
      },
      {
        "id": "qwen2.5:math-7b",
        "size": 4683073856
      },
      {
        "id": "qwen2.5:7b",
        "size": 4683073856
      },
      {
        "id": "qwen2.5:coder-7b",
        "size": 4683073920
      }
    ]
  },
  {
    "author": "MistralAI",
    "id": "cortexso/codestral",
    "metadata": {
      "_id": "66724fb044ee478111905260",
      "author": "cortexso",
      "cardData": {
        "license": "other",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2024-06-19T03:25:36.000Z",
      "description": "---\nlicense: other\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n\n## Overview\n\nCodestral-22B-v0.1 is trained on a diverse dataset of 80+ programming languages, including the most popular ones, such as Python, Java, C, C++, JavaScript, and Bash\n\n## Variants\n\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Codestral-22b](https://huggingface.co/cortexso/codestral/tree/22b) | `cortex run codestral:22b` |\n\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexhub/codestral\n    ```\n    \n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run codestral\n    ```\n    \n## Credits\n\n- **Author:** Mistral AI\n- **Converter:** [Homebrew](https://www.homebrew.ltd/)\n- **Original License:** [Licence](https://mistral.ai/licenses/MNPL-0.1.md)\n- **Papers:** [Codestral Blog](https://mistral.ai/news/codestral/)",
      "disabled": false,
      "downloads": 517,
      "gated": false,
      "gguf": {
        "architecture": "llama",
        "bos_token": "<s>",
        "chat_template": "{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content'] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n\n{{- bos_token }}\n{%- for message in loop_messages %}\n    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}\n        {{- raise_exception('After the optional system message, conversation roles must alternate user/assistant/user/assistant/...') }}\n    {%- endif %}\n    {%- if message['role'] == 'user' %}\n        {%- if loop.last and system_message is defined %}\n            {{- '[INST] ' + system_message + '\\n\\n' + message['content'] + '[/INST]' }}\n        {%- else %}\n            {{- '[INST] ' + message['content'] + '[/INST]' }}\n        {%- endif %}\n    {%- elif message['role'] == 'assistant' %}\n        {{- ' ' + message['content'] + eos_token}}\n    {%- else %}\n        {{- raise_exception('Only user and assistant roles are supported, with the exception of an initial optional system message!') }}\n    {%- endif %}\n{%- endfor %}\n",
        "context_length": 32768,
        "eos_token": "</s>",
        "total": 22247282688
      },
      "id": "cortexso/codestral",
      "lastModified": "2025-03-02T15:11:11.000Z",
      "likes": 0,
      "model-index": null,
      "modelId": "cortexso/codestral",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "6b522a6f0ce9c94a2f317c3802180aca4f526a30",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "codestral-22b-v0.1-q2_k.gguf"
        },
        {
          "rfilename": "codestral-22b-v0.1-q3_k_l.gguf"
        },
        {
          "rfilename": "codestral-22b-v0.1-q3_k_m.gguf"
        },
        {
          "rfilename": "codestral-22b-v0.1-q3_k_s.gguf"
        },
        {
          "rfilename": "codestral-22b-v0.1-q4_k_m.gguf"
        },
        {
          "rfilename": "codestral-22b-v0.1-q4_k_s.gguf"
        },
        {
          "rfilename": "codestral-22b-v0.1-q5_k_m.gguf"
        },
        {
          "rfilename": "codestral-22b-v0.1-q5_k_s.gguf"
        },
        {
          "rfilename": "codestral-22b-v0.1-q6_k.gguf"
        },
        {
          "rfilename": "codestral-22b-v0.1-q8_0.gguf"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "license:other",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 166025350400,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "codestral:22b",
        "size": 13341239008
      }
    ]
  },
  {
    "author": "Nous Research",
    "id": "cortexso/openhermes-2.5",
    "metadata": {
      "_id": "6669ee8d6993100c6f8befa7",
      "author": "cortexso",
      "cardData": {
        "license": "apache-2.0",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2024-06-12T18:53:01.000Z",
      "description": "---\nlicense: apache-2.0\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n\n## Overview\n\nOpenHermes 2.5 Mistral 7B is a state of the art Mistral Fine-tune, a continuation of OpenHermes 2 model, which trained on additional code datasets.\n\n## Variants\n\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [OpenHermes 2.5-7b](https://huggingface.co/cortexso/openhermes-2.5/tree/7b) | `cortex run openhermes-2.5:7b` |\n\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexhub/openhermes-2.5\n    ```\n    \n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run openhermes-2.5\n    ```\n    \n## Credits\n\n- **Author:** Nous Research\n- **Converter:** [Homebrew](https://www.homebrew.ltd/)\n- **Original License:** [Licence](https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md)\n- **Papers:** [Openhermes 2.5](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B)",
      "disabled": false,
      "downloads": 230,
      "gated": false,
      "gguf": {
        "architecture": "llama",
        "bos_token": "<s>",
        "chat_template": "{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}",
        "context_length": 32768,
        "eos_token": "<|im_end|>",
        "total": 7241748480
      },
      "id": "cortexso/openhermes-2.5",
      "lastModified": "2025-03-02T14:54:17.000Z",
      "likes": 0,
      "model-index": null,
      "modelId": "cortexso/openhermes-2.5",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "e4ef98ea46b61d21e434a79704717f7065c306a9",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        },
        {
          "rfilename": "openhermes-2.5-mistral-7b-q2_k.gguf"
        },
        {
          "rfilename": "openhermes-2.5-mistral-7b-q3_k_l.gguf"
        },
        {
          "rfilename": "openhermes-2.5-mistral-7b-q3_k_m.gguf"
        },
        {
          "rfilename": "openhermes-2.5-mistral-7b-q3_k_s.gguf"
        },
        {
          "rfilename": "openhermes-2.5-mistral-7b-q4_k_m.gguf"
        },
        {
          "rfilename": "openhermes-2.5-mistral-7b-q4_k_s.gguf"
        },
        {
          "rfilename": "openhermes-2.5-mistral-7b-q5_k_m.gguf"
        },
        {
          "rfilename": "openhermes-2.5-mistral-7b-q5_k_s.gguf"
        },
        {
          "rfilename": "openhermes-2.5-mistral-7b-q6_k.gguf"
        },
        {
          "rfilename": "openhermes-2.5-mistral-7b-q8_0.gguf"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "license:apache-2.0",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 122667617430,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "openhermes-2.5:7b",
        "size": 4368451712
      }
    ]
  },
  {
    "author": "sail",
    "id": "cortexso/sailor-2",
    "metadata": {
      "_id": "674f5d998f1ed02584bf68d8",
      "author": "cortexso",
      "cardData": {
        "license": "apache-2.0",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2024-12-03T19:35:53.000Z",
      "description": "---\nlicense: apache-2.0\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n\n## Overview\n\nSailor2 is a community-driven initiative that brings cutting-edge multilingual language models to South-East Asia (SEA). It is designed to address the growing demand for diverse, robust, and accessible language technologies in the region. Built upon the foundation of Qwen 2.5, Sailor2 is continuously pre-trained on 500B tokens, significantly improving its support for 15 languages with a unified model. These languages include English, Chinese, Burmese, Cebuano, Ilocano, Indonesian, Javanese, Khmer, Lao, Malay, Sundanese, Tagalog, Thai, Vietnamese, and Waray.\n\nSailor2 is available in three sizes: 1B, 8B, and 20B, which are expansions from the Qwen2.5 base models of 0.5B, 7B, and 14B, respectively. These models serve a wide range of applications, from production use to research and speculative decoding, ensuring accessibility to advanced language technologies across SEA.\n\n## Variants\n\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Sailor-2-1b](https://huggingface.co/cortexso/sailor-2/tree/1b) | `cortex run sailor-2:1b` |\n| 2 | [Sailor-2-8b](https://huggingface.co/cortexso/sailor-2/tree/8b) | `cortex run sailor-2:8b` |\n| 3 | [Sailor-2-20b](https://huggingface.co/cortexso/sailor-2/tree/20b) | `cortex run sailor-2:20b` |\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexhub/sailor-2\n    ```\n\n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run sailor-2\n    ```\n    \n## Credits\n\n- **Author:** Community-driven (Sailor2 Initiative)\n- **Converter:** [Homebrew](https://homebrew.ltd/)\n- **Original License:** [Licence](https://choosealicense.com/licenses/apache-2.0/)\n- **Papers:** [Technical Paper](https://arxiv.org/pdf/2502.12982)",
      "disabled": false,
      "downloads": 178,
      "gated": false,
      "gguf": {
        "architecture": "qwen2",
        "bos_token": "<|endoftext|>",
        "chat_template": "{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\nYou are an AI assistant named Sailor2, created by Sea AI Lab. As an AI assistant, you can answer questions in English, Chinese, and Southeast Asian languages such as Burmese, Cebuano, Ilocano, Indonesian, Javanese, Khmer, Lao, Malay, Sundanese, Tagalog, Thai, Vietnamese, and Waray. Your responses should be friendly, unbiased, informative, detailed, and faithful.<|im_end|>\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}",
        "context_length": 4096,
        "eos_token": "<|im_end|>",
        "total": 988064640
      },
      "id": "cortexso/sailor-2",
      "lastModified": "2025-03-03T02:58:28.000Z",
      "likes": 0,
      "model-index": null,
      "modelId": "cortexso/sailor-2",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "89b3079762dedf6ff4fbc94545632b3554c16420",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        },
        {
          "rfilename": "sailor2-1b-chat-q2_k.gguf"
        },
        {
          "rfilename": "sailor2-1b-chat-q3_k_l.gguf"
        },
        {
          "rfilename": "sailor2-1b-chat-q3_k_m.gguf"
        },
        {
          "rfilename": "sailor2-1b-chat-q3_k_s.gguf"
        },
        {
          "rfilename": "sailor2-1b-chat-q4_k_m.gguf"
        },
        {
          "rfilename": "sailor2-1b-chat-q4_k_s.gguf"
        },
        {
          "rfilename": "sailor2-1b-chat-q5_k_m.gguf"
        },
        {
          "rfilename": "sailor2-1b-chat-q5_k_s.gguf"
        },
        {
          "rfilename": "sailor2-1b-chat-q6_k.gguf"
        },
        {
          "rfilename": "sailor2-1b-chat-q8_0.gguf"
        },
        {
          "rfilename": "sailor2-20b-chat-q2_k.gguf"
        },
        {
          "rfilename": "sailor2-20b-chat-q3_k_l.gguf"
        },
        {
          "rfilename": "sailor2-20b-chat-q3_k_m.gguf"
        },
        {
          "rfilename": "sailor2-20b-chat-q3_k_s.gguf"
        },
        {
          "rfilename": "sailor2-20b-chat-q4_k_m.gguf"
        },
        {
          "rfilename": "sailor2-20b-chat-q4_k_s.gguf"
        },
        {
          "rfilename": "sailor2-20b-chat-q5_k_m.gguf"
        },
        {
          "rfilename": "sailor2-20b-chat-q5_k_s.gguf"
        },
        {
          "rfilename": "sailor2-20b-chat-q6_k.gguf"
        },
        {
          "rfilename": "sailor2-20b-chat-q8_0.gguf"
        },
        {
          "rfilename": "sailor2-8b-chat-q2_k.gguf"
        },
        {
          "rfilename": "sailor2-8b-chat-q3_k_l.gguf"
        },
        {
          "rfilename": "sailor2-8b-chat-q3_k_m.gguf"
        },
        {
          "rfilename": "sailor2-8b-chat-q3_k_s.gguf"
        },
        {
          "rfilename": "sailor2-8b-chat-q4_k_m.gguf"
        },
        {
          "rfilename": "sailor2-8b-chat-q4_k_s.gguf"
        },
        {
          "rfilename": "sailor2-8b-chat-q5_k_m.gguf"
        },
        {
          "rfilename": "sailor2-8b-chat-q5_k_s.gguf"
        },
        {
          "rfilename": "sailor2-8b-chat-q6_k.gguf"
        },
        {
          "rfilename": "sailor2-8b-chat-q8_0.gguf"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "arxiv:2502.12982",
        "license:apache-2.0",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 201040376768,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "sailor-2:1b",
        "size": 738628256
      },
      {
        "id": "sailor-2:20b",
        "size": 11622380384
      },
      {
        "id": "sailor-2:8b",
        "size": 5242934176
      }
    ]
  },
  {
    "author": "CohereForAI",
    "id": "cortexso/aya-expanse",
    "metadata": {
      "_id": "671ac0aee98f80735b80ce0d",
      "author": "cortexso",
      "cardData": {
        "license": "cc-by-sa-4.0",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2024-10-24T21:48:30.000Z",
      "description": "---\nlicense: cc-by-sa-4.0\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n\n## Overview\n\nAya Expanse is an open-weight research release of a model with highly advanced multilingual capabilities. It focuses on pairing a highly performant pre-trained Command family of models with the result of a year’s dedicated research from Cohere For AI, including data arbitrage, multilingual preference training, safety tuning, and model merging. The result is a powerful multilingual large language model serving 23 languages.\n\n## Variants\n\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Aya-expanse-8b](https://huggingface.co/cortexso/aya-expanse/tree/8b) | `cortex run aya-expanse:8b` |\n| 2 | [Aya-expanse-32b](https://huggingface.co/cortexso/aya-expanse/tree/32b) | `cortex run aya-expanse:32b` |\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexso/aya-expanse\n    ```\n\n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run aya-expanse\n    ```\n\n## Credits\n\n- **Author:** CohereAI\n- **Converter:** [Homebrew](https://www.homebrew.ltd/)\n- **Original License:** [License](https://cohere.com/c4ai-cc-by-nc-license)\n- **Papers:** [Aya Expanse Blog](https://cohere.com/blog/aya-expanse-connecting-our-world)",
      "disabled": false,
      "downloads": 219,
      "gated": false,
      "gguf": {
        "architecture": "command-r",
        "bos_token": "<BOS_TOKEN>",
        "chat_template": "{{ bos_token }}{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif false == true %}{% set loop_messages = messages %}{% set system_message = 'You are Aya, a brilliant, sophisticated, multilingual AI-assistant trained to assist human users by providing thorough responses. You are able to interact and respond to questions in 23 languages and you are powered by a multilingual model built by Cohere For AI.' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% if system_message != false %}{{ '<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>' + system_message + '<|END_OF_TURN_TOKEN|>' }}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|START_OF_TURN_TOKEN|><|USER_TOKEN|>' + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% elif message['role'] == 'assistant' %}{{ '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>'  + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>' }}{% endif %}",
        "context_length": 8192,
        "eos_token": "<|END_OF_TURN_TOKEN|>",
        "total": 32296476672
      },
      "id": "cortexso/aya-expanse",
      "lastModified": "2025-03-03T05:45:56.000Z",
      "likes": 1,
      "model-index": null,
      "modelId": "cortexso/aya-expanse",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "d3de661105fcf536bac3f1ec747a2d39d25fe08f",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "aya-expanse-32b-q2_k.gguf"
        },
        {
          "rfilename": "aya-expanse-32b-q3_k_l.gguf"
        },
        {
          "rfilename": "aya-expanse-32b-q3_k_m.gguf"
        },
        {
          "rfilename": "aya-expanse-32b-q3_k_s.gguf"
        },
        {
          "rfilename": "aya-expanse-32b-q4_k_m.gguf"
        },
        {
          "rfilename": "aya-expanse-32b-q4_k_s.gguf"
        },
        {
          "rfilename": "aya-expanse-32b-q5_k_m.gguf"
        },
        {
          "rfilename": "aya-expanse-32b-q5_k_s.gguf"
        },
        {
          "rfilename": "aya-expanse-32b-q6_k.gguf"
        },
        {
          "rfilename": "aya-expanse-32b-q8_0.gguf"
        },
        {
          "rfilename": "aya-expanse-8b-q2_k.gguf"
        },
        {
          "rfilename": "aya-expanse-8b-q3_k_l.gguf"
        },
        {
          "rfilename": "aya-expanse-8b-q3_k_m.gguf"
        },
        {
          "rfilename": "aya-expanse-8b-q3_k_s.gguf"
        },
        {
          "rfilename": "aya-expanse-8b-q4_k_m.gguf"
        },
        {
          "rfilename": "aya-expanse-8b-q4_k_s.gguf"
        },
        {
          "rfilename": "aya-expanse-8b-q5_k_m.gguf"
        },
        {
          "rfilename": "aya-expanse-8b-q5_k_s.gguf"
        },
        {
          "rfilename": "aya-expanse-8b-q6_k.gguf"
        },
        {
          "rfilename": "aya-expanse-8b-q8_0.gguf"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "license:cc-by-sa-4.0",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 283759636448,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "aya-expanse:8b",
        "size": 5056974624
      },
      {
        "id": "aya-expanse:32b",
        "size": 19800825408
      }
    ]
  },
  {
    "author": "CohereForAI",
    "id": "cortexso/command-r",
    "metadata": {
      "_id": "66751b98585f2bf57092b2ae",
      "author": "cortexso",
      "cardData": {
        "license": "cc-by-nc-4.0",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2024-06-21T06:20:08.000Z",
      "description": "---\nlicense: cc-by-nc-4.0\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n\n## Overview\n\nC4AI Command-R is a research release of a 35 billion parameter highly performant generative model. Command-R is a large language model with open weights optimized for a variety of use cases including reasoning, summarization, and question answering. Command-R has the capability for multilingual generation evaluated in 10 languages and highly performant RAG capabilities.\n\n## Variants\n\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Command-r-32b](https://huggingface.co/cortexhub/command-r/tree/32b) | `cortex run command-r:32b` |\n| 1 | [Command-r-35b](https://huggingface.co/cortexhub/command-r/tree/35b) | `cortex run command-r:35b` |\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexhub/command-r\n    ```\n    \n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run command-r\n    ```\n    \n## Credits\n\n- **Author:** Cohere For AI\n- **Converter:** [Homebrew](https://www.homebrew.ltd/)\n- **Original License:** [Licence](https://cohere.com/c4ai-cc-by-nc-license)",
      "disabled": false,
      "downloads": 613,
      "gated": false,
      "gguf": {
        "architecture": "command-r",
        "bos_token": "<BOS_TOKEN>",
        "chat_template": "{{ bos_token }}{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif false == true %}{% set loop_messages = messages %}{% set system_message = 'You are a large language model called Command R built by the company Cohere. You act as a brilliant, sophisticated, AI-assistant chatbot trained to assist human users by providing thorough responses.' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% if system_message != false %}{{ '<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>' + system_message + '<|END_OF_TURN_TOKEN|>' }}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|START_OF_TURN_TOKEN|><|USER_TOKEN|>' + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% elif message['role'] == 'assistant' %}{{ '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>'  + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>' }}{% endif %}",
        "context_length": 131072,
        "eos_token": "<|END_OF_TURN_TOKEN|>",
        "total": 32296476672
      },
      "id": "cortexso/command-r",
      "lastModified": "2025-03-03T05:55:03.000Z",
      "likes": 1,
      "model-index": null,
      "modelId": "cortexso/command-r",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "829fc0c4d726206187684dcbaf2a53c658d5d34a",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "c4ai-command-r-08-2024-q2_k.gguf"
        },
        {
          "rfilename": "c4ai-command-r-08-2024-q3_k_l.gguf"
        },
        {
          "rfilename": "c4ai-command-r-08-2024-q3_k_m.gguf"
        },
        {
          "rfilename": "c4ai-command-r-08-2024-q3_k_s.gguf"
        },
        {
          "rfilename": "c4ai-command-r-08-2024-q4_k_m.gguf"
        },
        {
          "rfilename": "c4ai-command-r-08-2024-q4_k_s.gguf"
        },
        {
          "rfilename": "c4ai-command-r-08-2024-q5_k_m.gguf"
        },
        {
          "rfilename": "c4ai-command-r-08-2024-q5_k_s.gguf"
        },
        {
          "rfilename": "c4ai-command-r-08-2024-q6_k.gguf"
        },
        {
          "rfilename": "c4ai-command-r-08-2024-q8_0.gguf"
        },
        {
          "rfilename": "c4ai-command-r-v01-q2_k.gguf"
        },
        {
          "rfilename": "c4ai-command-r-v01-q3_k_l.gguf"
        },
        {
          "rfilename": "c4ai-command-r-v01-q3_k_m.gguf"
        },
        {
          "rfilename": "c4ai-command-r-v01-q3_k_s.gguf"
        },
        {
          "rfilename": "c4ai-command-r-v01-q4_k_m.gguf"
        },
        {
          "rfilename": "c4ai-command-r-v01-q4_k_s.gguf"
        },
        {
          "rfilename": "c4ai-command-r-v01-q5_k_m.gguf"
        },
        {
          "rfilename": "c4ai-command-r-v01-q5_k_s.gguf"
        },
        {
          "rfilename": "c4ai-command-r-v01-q6_k.gguf"
        },
        {
          "rfilename": "c4ai-command-r-v01-q8_0.gguf"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "license:cc-by-nc-4.0",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 471257928608,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "command-r:32b",
        "size": 19800837184
      },
      {
        "id": "command-r:35b",
        "size": 21527055296
      }
    ]
  },
  {
    "author": "simplescaling",
    "id": "cortexso/simplescaling-s1",
    "metadata": {
      "_id": "67a4e03a6f317f30b9a285b0",
      "author": "cortexso",
      "cardData": {
        "license": "mit",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2025-02-06T16:15:54.000Z",
      "description": "---\nlicense: mit\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n## Overview\nThe 'simplescaling-s1' model is a refined version of 'simplescaling/s1-32B,' designed to enhance scalability and streamline tasks in AI applications. It focuses on efficiently managing resource allocation while maintaining high performance across various workloads. This model is particularly effective for text generation, summarization, and conversational AI, as it balances speed and accuracy. Users can leverage 'simplescaling-s1' for building scalable applications that require processing large datasets or generating content quickly. Overall, the model achieves impressive results with reduced computational overhead, making it suitable for both research and practical deployments.\n## Variants\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Simplescaling-s1-32b](https://huggingface.co/cortexso/simplescaling-s1/tree/32b) | cortex run simplescaling-s1:32b |\n## Use it with Jan (UI)\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexso/simplescaling-s1\n    ```\n    \n## Use it with Cortex (CLI)\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run simplescaling-s1\n    ```\n## Credits\n- **Author:** simplescaling\n- **Converter:** [Homebrew](https://www.homebrew.ltd/)\n- **Original License:** [License](https://choosealicense.com/licenses/apache-2.0/)\n- **Paper**: [Paper](https://arxiv.org/abs/2501.19393)",
      "disabled": false,
      "downloads": 104,
      "gated": false,
      "gguf": {
        "architecture": "qwen2",
        "bos_token": "<|endoftext|>",
        "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n",
        "context_length": 32768,
        "eos_token": "<|im_end|>",
        "total": 32763876352
      },
      "id": "cortexso/simplescaling-s1",
      "lastModified": "2025-03-03T03:46:24.000Z",
      "likes": 0,
      "model-index": null,
      "modelId": "cortexso/simplescaling-s1",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "5755e76ec22a9ca9d0271ce16f5287bb9ad3c1a6",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        },
        {
          "rfilename": "s1-32b-q2_k.gguf"
        },
        {
          "rfilename": "s1-32b-q3_k_l.gguf"
        },
        {
          "rfilename": "s1-32b-q3_k_m.gguf"
        },
        {
          "rfilename": "s1-32b-q3_k_s.gguf"
        },
        {
          "rfilename": "s1-32b-q4_k_m.gguf"
        },
        {
          "rfilename": "s1-32b-q4_k_s.gguf"
        },
        {
          "rfilename": "s1-32b-q5_k_m.gguf"
        },
        {
          "rfilename": "s1-32b-q5_k_s.gguf"
        },
        {
          "rfilename": "s1-32b-q6_k.gguf"
        },
        {
          "rfilename": "s1-32b-q8_0.gguf"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "arxiv:2501.19393",
        "license:mit",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 206130756480,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "simplescaling-s1:32b",
        "size": 19851336384
      }
    ]
  },
  {
    "author": "Qwen",
    "id": "cortexso/qwq",
    "metadata": {
      "_id": "67497b496615e96c7c8d6b05",
      "author": "cortexso",
      "cardData": {
        "license": "other",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2024-11-29T08:28:57.000Z",
      "description": "---\nlicense: other\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n\n## Overview\n\nQwQ-32B-Preview is an experimental large-scale research model by the Qwen Team, focusing on advanced AI reasoning. While it demonstrates strong analytical capabilities, it also presents notable limitations:\n\n## Variants\n\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Qwq-32b](https://huggingface.co/cortexso/qwq/tree/32b) | `cortex run qwq:32b` |\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexhub/qwq\n    ```\n\n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run qwq\n    ```\n    \n## Credits\n\n- **Author:** Qwen\n- **Converter:** [Homebrew](https://homebrew.ltd/)\n- **Original License:** [Licence](https://huggingface.co/Qwen/QwQ-32B-Preview/blob/main/LICENSE)\n- **Papers:** [QwQ Blog](https://qwenlm.github.io/blog/qwq-32b-preview/)",
      "disabled": false,
      "downloads": 101,
      "gated": false,
      "gguf": {
        "architecture": "qwen2",
        "bos_token": "<|endoftext|>",
        "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are a helpful and harmless assistant. You are Qwen developed by Alibaba. You should think step-by-step.' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are a helpful and harmless assistant. You are Qwen developed by Alibaba. You should think step-by-step.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n",
        "context_length": 32768,
        "eos_token": "<|im_end|>",
        "total": 32763876352
      },
      "id": "cortexso/qwq",
      "lastModified": "2025-03-03T02:23:40.000Z",
      "likes": 0,
      "model-index": null,
      "modelId": "cortexso/qwq",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "fc6f23c0d5c8faf8b79b11e03aaa7c656fed8dfd",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        },
        {
          "rfilename": "qwq-32b-preview-q2_k.gguf"
        },
        {
          "rfilename": "qwq-32b-preview-q3_k_l.gguf"
        },
        {
          "rfilename": "qwq-32b-preview-q3_k_m.gguf"
        },
        {
          "rfilename": "qwq-32b-preview-q3_k_s.gguf"
        },
        {
          "rfilename": "qwq-32b-preview-q4_k_m.gguf"
        },
        {
          "rfilename": "qwq-32b-preview-q4_k_s.gguf"
        },
        {
          "rfilename": "qwq-32b-preview-q5_k_m.gguf"
        },
        {
          "rfilename": "qwq-32b-preview-q5_k_s.gguf"
        },
        {
          "rfilename": "qwq-32b-preview-q6_k.gguf"
        },
        {
          "rfilename": "qwq-32b-preview-q8_0.gguf"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "license:other",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 206130755200,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "qwq:32b",
        "size": 19851336256
      }
    ]
  },
  {
    "author": "Nexusflow",
    "id": "cortexso/athene",
    "metadata": {
      "_id": "6737ae7de6b1d15ff54d0a08",
      "author": "cortexso",
      "cardData": {
        "license": "other",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2024-11-15T20:26:37.000Z",
      "description": "---\nlicense: other\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n\n## Overview\n\nAthene-V2-Chat-72B is an open-weight LLM that competes on par with GPT-4o across various benchmarks. It is currently ranked as the best open model on Chatbot Arena, where it outperforms GPT-4o-0513 (the highest-ranked GPT-4o model on Arena) in hard and math categories. It also matches GPT-4o-0513 in coding, instruction following, longer queries, and multi-turn conversations.\n\nTrained through RLHF with Qwen-2.5-72B-Instruct as the base model, Athene-V2-Chat-72B excels in chat, math, and coding. Additionally, its sister model, Athene-V2-Agent-72B, surpasses GPT-4o in complex function calling and agentic applications, further extending its capabilities.\n## Variants\n\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Athene-72b](https://huggingface.co/cortexso/athene/tree/72b) | `cortex run athene:72b` |\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexhub/athene\n    ```\n\n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run athene\n    ```\n    \n## Credits\n\n- **Author:** Nexusflow\n- **Converter:** [Homebrew](https://homebrew.ltd/)\n- **Original License:** [Licence](https://huggingface.co/Nexusflow/Athene-V2-Chat/blob/main/Nexusflow_Research_License_.pdf)\n- **Papers:** [Athene V2 Blog](https://nexusflow.ai/blogs/athene-v2)",
      "disabled": false,
      "downloads": 13,
      "gated": false,
      "gguf": {
        "architecture": "qwen2",
        "bos_token": "<|endoftext|>",
        "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n",
        "context_length": 32768,
        "eos_token": "<|im_end|>",
        "total": 72706203648
      },
      "id": "cortexso/athene",
      "lastModified": "2025-03-03T06:04:09.000Z",
      "likes": 0,
      "model-index": null,
      "modelId": "cortexso/athene",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "a92447ca675e741541855ac03b8f144dee1067c4",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "athene-v2-chat-q4_k_m.gguf"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "license:other",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 47415715136,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "athene:72b",
        "size": 47415715136
      }
    ]
  },
  {
    "author": "MistralAI",
    "id": "cortexso/mistral",
    "metadata": {
      "_id": "6667b1796e382e809d62b9fc",
      "author": "cortexso",
      "cardData": {
        "license": "other",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2024-06-11T02:07:53.000Z",
      "description": "---\nlicense: other\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n\n## Overview\n\nMistral 7B, a 7-billion-parameter Large Language Model by Mistral AI. Designed for efficiency and performance, it suits real-time applications requiring swift responses.\n\n## Variants\n\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Mistra-7b](https://huggingface.co/cortexhub/mistral/tree/7b) | `cortex run mistral:7b` |\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexhub/mistral\n    ```\n\n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run mistral\n    ```\n    \n## Credits\n\n- **Author:** MistralAI\n- **Converter:** [Homebrew](https://www.homebrew.ltd/)\n- **Original License:** [Licence](https://mistral.ai/licenses/MNPL-0.1.md)\n- **Papers:** [Mistral paper](https://arxiv.org/abs/2310.06825)",
      "disabled": false,
      "downloads": 1895,
      "gated": false,
      "gguf": {
        "architecture": "llama",
        "bos_token": "<s>",
        "chat_template": "{%- if messages[0][\"role\"] == \"system\" %}\n    {%- set system_message = messages[0][\"content\"] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n{%- set user_messages = loop_messages | selectattr(\"role\", \"equalto\", \"user\") | list %}\n\n{#- This block checks for alternating user/assistant messages, skipping tool calling messages #}\n{%- set ns = namespace() %}\n{%- set ns.index = 0 %}\n{%- for message in loop_messages %}\n    {%- if not (message.role == \"tool\" or message.role == \"tool_results\" or (message.tool_calls is defined and message.tool_calls is not none)) %}\n        {%- if (message[\"role\"] == \"user\") != (ns.index % 2 == 0) %}\n            {{- raise_exception(\"After the optional system message, conversation roles must alternate user/assistant/user/assistant/...\") }}\n        {%- endif %}\n        {%- set ns.index = ns.index + 1 %}\n    {%- endif %}\n{%- endfor %}\n\n{{- bos_token }}\n{%- for message in loop_messages %}\n    {%- if message[\"role\"] == \"user\" %}\n        {%- if tools is not none and (message == user_messages[-1]) %}\n            {{- \"[AVAILABLE_TOOLS] [\" }}\n            {%- for tool in tools %}\n                {%- set tool = tool.function %}\n                {{- '{\"type\": \"function\", \"function\": {' }}\n                {%- for key, val in tool.items() if key != \"return\" %}\n                    {%- if val is string %}\n                        {{- '\"' + key + '\": \"' + val + '\"' }}\n                    {%- else %}\n                        {{- '\"' + key + '\": ' + val|tojson }}\n                    {%- endif %}\n                    {%- if not loop.last %}\n                        {{- \", \" }}\n                    {%- endif %}\n                {%- endfor %}\n                {{- \"}}\" }}\n                {%- if not loop.last %}\n                    {{- \", \" }}\n                {%- else %}\n                    {{- \"]\" }}\n                {%- endif %}\n            {%- endfor %}\n            {{- \"[/AVAILABLE_TOOLS]\" }}\n            {%- endif %}\n        {%- if loop.last and system_message is defined %}\n            {{- \"[INST] \" + system_message + \"\\n\\n\" + message[\"content\"] + \"[/INST]\" }}\n        {%- else %}\n            {{- \"[INST] \" + message[\"content\"] + \"[/INST]\" }}\n        {%- endif %}\n    {%- elif message.tool_calls is defined and message.tool_calls is not none %}\n        {{- \"[TOOL_CALLS] [\" }}\n        {%- for tool_call in message.tool_calls %}\n            {%- set out = tool_call.function|tojson %}\n            {{- out[:-1] }}\n            {%- if not tool_call.id is defined or tool_call.id|length != 9 %}\n                {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n            {%- endif %}\n            {{- ', \"id\": \"' + tool_call.id + '\"}' }}\n            {%- if not loop.last %}\n                {{- \", \" }}\n            {%- else %}\n                {{- \"]\" + eos_token }}\n            {%- endif %}\n        {%- endfor %}\n    {%- elif message[\"role\"] == \"assistant\" %}\n        {{- \" \" + message[\"content\"]|trim + eos_token}}\n    {%- elif message[\"role\"] == \"tool_results\" or message[\"role\"] == \"tool\" %}\n        {%- if message.content is defined and message.content.content is defined %}\n            {%- set content = message.content.content %}\n        {%- else %}\n            {%- set content = message.content %}\n        {%- endif %}\n        {{- '[TOOL_RESULTS] {\"content\": ' + content|string + \", \" }}\n        {%- if not message.tool_call_id is defined or message.tool_call_id|length != 9 %}\n            {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n        {%- endif %}\n        {{- '\"call_id\": \"' + message.tool_call_id + '\"}[/TOOL_RESULTS]' }}\n    {%- else %}\n        {{- raise_exception(\"Only user and assistant roles are supported, with the exception of an initial optional system message!\") }}\n    {%- endif %}\n{%- endfor %}\n",
        "context_length": 32768,
        "eos_token": "</s>",
        "total": 7248023552
      },
      "id": "cortexso/mistral",
      "lastModified": "2025-03-03T02:39:43.000Z",
      "likes": 1,
      "model-index": null,
      "modelId": "cortexso/mistral",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "125b0ef1bdf6441d5c00f6a6a24a491214e532bd",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "mistral-7b-instruct-v0.3-q2_k.gguf"
        },
        {
          "rfilename": "mistral-7b-instruct-v0.3-q3_k_l.gguf"
        },
        {
          "rfilename": "mistral-7b-instruct-v0.3-q3_k_m.gguf"
        },
        {
          "rfilename": "mistral-7b-instruct-v0.3-q3_k_s.gguf"
        },
        {
          "rfilename": "mistral-7b-instruct-v0.3-q4_k_m.gguf"
        },
        {
          "rfilename": "mistral-7b-instruct-v0.3-q4_k_s.gguf"
        },
        {
          "rfilename": "mistral-7b-instruct-v0.3-q5_k_m.gguf"
        },
        {
          "rfilename": "mistral-7b-instruct-v0.3-q5_k_s.gguf"
        },
        {
          "rfilename": "mistral-7b-instruct-v0.3-q6_k.gguf"
        },
        {
          "rfilename": "mistral-7b-instruct-v0.3-q8_0.gguf"
        },
        {
          "rfilename": "model.yml"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "arxiv:2310.06825",
        "license:other",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 49914826528,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "mistral:7b",
        "size": 4372815680
      }
    ]
  },
  {
    "author": "HuggingFaceTB",
    "id": "cortexso/smollm2",
    "metadata": {
      "_id": "672408e4603a8644ff7505f0",
      "author": "cortexso",
      "cardData": {
        "license": "apache-2.0",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2024-10-31T22:47:00.000Z",
      "description": "---\nlicense: apache-2.0\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n\n## Overview\n\nSmolLM2 is a family of compact language models available in three sizes: 135M, 360M, and 1.7B parameters. These models are designed to solve a wide range of tasks while being lightweight enough for on-device deployment. More details can be found in the [SmolLM2 paper](https://arxiv.org/abs/2502.02737v1).\n\nThe **1.7B variant** demonstrates significant improvements over its predecessor, SmolLM1-1.7B, especially in instruction following, knowledge retention, reasoning, and mathematical problem-solving. It was trained on **11 trillion tokens** using a diverse dataset combination, including **FineWeb-Edu, DCLM, The Stack**, and newly curated mathematics and coding datasets that will be released soon.\n\nThe **instruct version** of SmolLM2 was developed through **supervised fine-tuning (SFT)** using a mix of public datasets and curated proprietary datasets. It further benefits from **Direct Preference Optimization (DPO)** using **UltraFeedback**. \n\nAdditionally, the instruct model supports tasks such as **text rewriting, summarization, and function calling**, enabled by datasets from **Argilla**, including **Synth-APIGen-v0.1**. The SFT dataset is available at: [SmolTalk SFT Dataset](https://huggingface.co/datasets/HuggingFaceTB/smoltalk).\n\nFor further details, visit the [SmolLM2 GitHub repository](https://github.com/huggingface/smollm), where you will find resources for **pre-training, post-training, evaluation, and local inference**.\n\n## Variants\n\n| No | Variant                                                | Cortex CLI command     |\n| -- | ------------------------------------------------------ | ---------------------- |\n| 1  | [Smollm2-1.7b](https://huggingface.co/cortexso/smollm2/tree/1.7b)           | `cortex run smollm2:1.7b`  |\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n   ```bash\n   cortexhub/smollm2\n   ```\n\n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n   ```bash\n   cortex run smollm2\n   ```\n\n## Credits\n\n- **Author:** SmolLM2 Team\n- **Converter:** [Homebrew](https://www.homebrew.ltd/)\n- **Original License:** [Apache 2.0](https://choosealicense.com/licenses/apache-2.0/)\n- **Papers:** [SmolLM2 Research](https://arxiv.org/abs/2502.02737v1)",
      "disabled": false,
      "downloads": 237,
      "gated": false,
      "gguf": {
        "architecture": "llama",
        "bos_token": "<|im_start|>",
        "chat_template": "{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\nYou are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}",
        "context_length": 8192,
        "eos_token": "<|im_end|>",
        "total": 1711376384
      },
      "id": "cortexso/smollm2",
      "lastModified": "2025-03-03T03:51:13.000Z",
      "likes": 0,
      "model-index": null,
      "modelId": "cortexso/smollm2",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "b825edad383d925571b4433f8d6b16eb7cc1e9fc",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        },
        {
          "rfilename": "smollm2-1.7b-instruct-q2_k.gguf"
        },
        {
          "rfilename": "smollm2-1.7b-instruct-q3_k_l.gguf"
        },
        {
          "rfilename": "smollm2-1.7b-instruct-q3_k_m.gguf"
        },
        {
          "rfilename": "smollm2-1.7b-instruct-q3_k_s.gguf"
        },
        {
          "rfilename": "smollm2-1.7b-instruct-q4_k_m.gguf"
        },
        {
          "rfilename": "smollm2-1.7b-instruct-q4_k_s.gguf"
        },
        {
          "rfilename": "smollm2-1.7b-instruct-q5_k_m.gguf"
        },
        {
          "rfilename": "smollm2-1.7b-instruct-q5_k_s.gguf"
        },
        {
          "rfilename": "smollm2-1.7b-instruct-q6_k.gguf"
        },
        {
          "rfilename": "smollm2-1.7b-instruct-q8_0.gguf"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "arxiv:2502.02737",
        "license:apache-2.0",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 11998369216,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "smollm2:1.7b",
        "size": 1055609728
      }
    ]
  },
  {
    "author": "allenai",
    "id": "cortexso/tulu3",
    "metadata": {
      "_id": "6744a6a2e08fe3da3fcdfb36",
      "author": "cortexso",
      "cardData": {
        "license": "other",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2024-11-25T16:32:34.000Z",
      "description": "---\nlicense: other\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n\n## Overview\n\nTülu3 is a state-of-the-art instruction-following model family developed by Allen Institute for AI. It is designed to excel in a wide range of tasks beyond standard chat applications, including complex problem-solving in domains such as MATH, GSM8K, and IFEval. The Tülu3 series provides a fully open-source ecosystem, offering access to datasets, training code, and fine-tuning recipes to facilitate advanced model customization and experimentation.\n\n## Variants\n\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Tulu3-8b](https://huggingface.co/cortexso/tulu3/tree/8b) | `cortex run tulu3:8b` |\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexhub/tulu3\n    ```\n\n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run tulu3\n    ```\n    \n## Credits\n\n- **Author:** Allenai\n- **Converter:** [Homebrew](https://homebrew.ltd/)\n- **Original License:** [Licence](https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct/blob/main/LICENSE)\n- **Papers:** [Paper](https://arxiv.org/abs/2411.15124)",
      "disabled": false,
      "downloads": 252,
      "gated": false,
      "gguf": {
        "architecture": "llama",
        "bos_token": "<|begin_of_text|>",
        "chat_template": "{% for message in messages %}{% if message['role'] == 'system' %}{{ '<|system|>\n' + message['content'] + '\n' }}{% elif message['role'] == 'user' %}{{ '<|user|>\n' + message['content'] + '\n' }}{% elif message['role'] == 'assistant' %}{% if not loop.last %}{{ '<|assistant|>\n'  + message['content'] + eos_token + '\n' }}{% else %}{{ '<|assistant|>\n'  + message['content'] + eos_token }}{% endif %}{% endif %}{% if loop.last and add_generation_prompt %}{{ '<|assistant|>\n' }}{% endif %}{% endfor %}",
        "context_length": 131072,
        "eos_token": "<|end_of_text|>",
        "total": 8030326848
      },
      "id": "cortexso/tulu3",
      "lastModified": "2025-03-03T03:48:16.000Z",
      "likes": 1,
      "model-index": null,
      "modelId": "cortexso/tulu3",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "693fb27ee973a686d66f33ecc72b41172ec5a7d6",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "llama-3.1-tulu-3-8b-sft-q2_k.gguf"
        },
        {
          "rfilename": "llama-3.1-tulu-3-8b-sft-q3_k_l.gguf"
        },
        {
          "rfilename": "llama-3.1-tulu-3-8b-sft-q3_k_m.gguf"
        },
        {
          "rfilename": "llama-3.1-tulu-3-8b-sft-q3_k_s.gguf"
        },
        {
          "rfilename": "llama-3.1-tulu-3-8b-sft-q4_k_m.gguf"
        },
        {
          "rfilename": "llama-3.1-tulu-3-8b-sft-q4_k_s.gguf"
        },
        {
          "rfilename": "llama-3.1-tulu-3-8b-sft-q5_k_m.gguf"
        },
        {
          "rfilename": "llama-3.1-tulu-3-8b-sft-q5_k_s.gguf"
        },
        {
          "rfilename": "llama-3.1-tulu-3-8b-sft-q6_k.gguf"
        },
        {
          "rfilename": "llama-3.1-tulu-3-8b-sft-q8_0.gguf"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "arxiv:2411.15124",
        "license:other",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 56188233120,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "tulu3:8b",
        "size": 4920780768
      }
    ]
  },
  {
    "author": "Qwen Team",
    "id": "cortexso/qwen3",
    "metadata": {
      "_id": "6810288ccbe4f92b62636b50",
      "author": "cortexso",
      "cardData": {
        "license": "apache-2.0",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp", "featured"]
      },
      "createdAt": "2025-04-29T01:17:00.000Z",
      "description": "---\nlicense: apache-2.0\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n- featured\n---\n\n## Overview\n\n**Qwen Team** developed and released the **Qwen3** series, a state-of-the-art family of language models optimized for advanced reasoning, dialogue, instruction-following, and agentic use cases. Qwen3 introduces innovative thinking/non-thinking mode switching, long context capabilities, and multilingual support, all while achieving high efficiency and performance.\n\nThe Qwen3 models span several sizes and include support for seamless reasoning, complex tool usage, and detailed multi-turn conversations, making them ideal for applications such as research assistants, code generation, enterprise chatbots, and more.\n\n## Variants\n\n### Qwen3\n\n| No | Variant                                                                                   | Branch | Cortex CLI command             |\n|----|--------------------------------------------------------------------------------------------|--------|-------------------------------|\n| 1  | [Qwen3-0.6B](https://huggingface.co/cortexso/qwen3/tree/0.6b)                                  | 0.6b     | `cortex run qwen3:0.6b`         |\n| 2  | [Qwen3-1.7B](https://huggingface.co/cortexso/qwen3/tree/1.7b)                                  | 1.7b     | `cortex run qwen3:1.7b`         |\n| 3  | [Qwen3-4B](https://huggingface.co/cortexso/qwen3/tree/4b)                                  | 4b     | `cortex run qwen3:4b`         |\n| 4  | [Qwen3-8B](https://huggingface.co/cortexso/qwen3/tree/8b)                                  | 8b     | `cortex run qwen3:8b`         |\n| 5  | [Qwen3-14B](https://huggingface.co/cortexso/qwen3/tree/14b)                                  | 14b     | `cortex run qwen3:14b`         |\n| 6  | [Qwen3-32B](https://huggingface.co/cortexso/qwen3/tree/32b)                                | 32b    | `cortex run qwen3:32b`        |\n| 7  | [Qwen3-30B-A3B](https://huggingface.co/cortexso/qwen3/tree/30b-a3b)                        | 30b-a3b| `cortex run qwen3:30b-a3b`    |\n\nEach branch contains multiple quantized GGUF versions:\n- **Qwen3-0.6B:** q2_k, q3_k_l, q3_k_m, q3_k_s, q4_k_m, q4_k_s, q5_k_m, q5_k_s, q6_k, q8_0\n- **Qwen3-1.7B:** q2_k, q3_k_l, q3_k_m, q3_k_s, q4_k_m, q4_k_s, q5_k_m, q5_k_s, q6_k, q8_0\n- **Qwen3-4B:** q2_k, q3_k_l, q3_k_m, q3_k_s, q4_k_m, q4_k_s, q5_k_m, q5_k_s, q6_k, q8_0\n- **Qwen3-8B:** q2_k, q3_k_l, q3_k_m, q3_k_s, q4_k_m, q4_k_s, q5_k_m, q5_k_s, q6_k, q8_0\n- **Qwen3-32B:** q2_k, q3_k_l, q3_k_m, q3_k_s, q4_k_m, q4_k_s, q5_k_m, q5_k_s, q6_k, q8_0\n- **Qwen3-30B-A3B:** *q2_k, q3_k_l, q3_k_m, q3_k_s, q4_k_m, q4_k_s, q5_k_m, q5_k_s, q6_k, q8_0\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n   ```bash\n   cortexso/qwen3\n   ```\n\n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n   ```bash\n   cortex run qwen3\n   ```\n\n## Credits\n\n- **Author:** Qwen Team\n- **Converter:** [Menlo Research](https://menlo.ai/)\n- **Original License:** [License](https://www.apache.org/licenses/LICENSE-2.0)\n- **Blogs:** [Qwen3: Think Deeper, Act Faster](https://qwenlm.github.io/blog/qwen3/)",
      "disabled": false,
      "downloads": 6693,
      "gated": false,
      "gguf": {
        "architecture": "qwen3",
        "bos_token": "<|endoftext|>",
        "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0].role == 'system' %}\n        {{- messages[0].content + '\\n\\n' }}\n    {%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0].role == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == \"user\" and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set content = message.content %}\n        {%- set reasoning_content = '' %}\n        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if '</think>' in message.content %}\n                {%- set content = message.content.split('</think>')[-1].lstrip('\\n') %}\n                {%- set reasoning_content = message.content.split('</think>')[0].rstrip('\\n').split('<think>')[-1].lstrip('\\n') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip('\\n') + '\\n</think>\\n\\n' + content.lstrip('\\n') }}\n            {%- else %}\n                {{- '<|im_start|>' + message.role + '\\n' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- '<|im_start|>' + message.role + '\\n' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- '\\n' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- '<tool_call>\\n{\"name\": \"' }}\n                {{- tool_call.name }}\n                {{- '\", \"arguments\": ' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- '}\\n</tool_call>' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n    {%- if enable_thinking is defined and enable_thinking is false %}\n        {{- '<think>\\n\\n</think>\\n\\n' }}\n    {%- endif %}\n{%- endif %}",
        "context_length": 40960,
        "eos_token": "<|im_end|>",
        "total": 751632384
      },
      "id": "cortexso/qwen3",
      "lastModified": "2025-05-08T15:50:21.000Z",
      "likes": 1,
      "model-index": null,
      "modelId": "cortexso/qwen3",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "d25d0999fbab8909f16173f21f2db8f9f58c0a28",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        },
        {
          "rfilename": "qwen3-0.6b-q2_k.gguf"
        },
        {
          "rfilename": "qwen3-0.6b-q3_k_l.gguf"
        },
        {
          "rfilename": "qwen3-0.6b-q3_k_m.gguf"
        },
        {
          "rfilename": "qwen3-0.6b-q3_k_s.gguf"
        },
        {
          "rfilename": "qwen3-0.6b-q4_k_m.gguf"
        },
        {
          "rfilename": "qwen3-0.6b-q4_k_s.gguf"
        },
        {
          "rfilename": "qwen3-0.6b-q5_k_m.gguf"
        },
        {
          "rfilename": "qwen3-0.6b-q5_k_s.gguf"
        },
        {
          "rfilename": "qwen3-0.6b-q6_k.gguf"
        },
        {
          "rfilename": "qwen3-0.6b-q8_0.gguf"
        },
        {
          "rfilename": "qwen3-1.7b-q2_k.gguf"
        },
        {
          "rfilename": "qwen3-1.7b-q3_k_l.gguf"
        },
        {
          "rfilename": "qwen3-1.7b-q3_k_m.gguf"
        },
        {
          "rfilename": "qwen3-1.7b-q3_k_s.gguf"
        },
        {
          "rfilename": "qwen3-1.7b-q4_k_m.gguf"
        },
        {
          "rfilename": "qwen3-1.7b-q4_k_s.gguf"
        },
        {
          "rfilename": "qwen3-1.7b-q5_k_m.gguf"
        },
        {
          "rfilename": "qwen3-1.7b-q5_k_s.gguf"
        },
        {
          "rfilename": "qwen3-1.7b-q6_k.gguf"
        },
        {
          "rfilename": "qwen3-1.7b-q8_0.gguf"
        },
        {
          "rfilename": "qwen3-14b-q2_k.gguf"
        },
        {
          "rfilename": "qwen3-14b-q3_k_l.gguf"
        },
        {
          "rfilename": "qwen3-14b-q3_k_m.gguf"
        },
        {
          "rfilename": "qwen3-14b-q3_k_s.gguf"
        },
        {
          "rfilename": "qwen3-14b-q4_k_m.gguf"
        },
        {
          "rfilename": "qwen3-14b-q4_k_s.gguf"
        },
        {
          "rfilename": "qwen3-14b-q5_k_m.gguf"
        },
        {
          "rfilename": "qwen3-14b-q5_k_s.gguf"
        },
        {
          "rfilename": "qwen3-14b-q6_k.gguf"
        },
        {
          "rfilename": "qwen3-14b-q8_0.gguf"
        },
        {
          "rfilename": "qwen3-30b-a3b-q2_k.gguf"
        },
        {
          "rfilename": "qwen3-30b-a3b-q3_k_l.gguf"
        },
        {
          "rfilename": "qwen3-30b-a3b-q3_k_m.gguf"
        },
        {
          "rfilename": "qwen3-30b-a3b-q3_k_s.gguf"
        },
        {
          "rfilename": "qwen3-30b-a3b-q4_k_m.gguf"
        },
        {
          "rfilename": "qwen3-30b-a3b-q4_k_s.gguf"
        },
        {
          "rfilename": "qwen3-30b-a3b-q5_k_m.gguf"
        },
        {
          "rfilename": "qwen3-30b-a3b-q5_k_s.gguf"
        },
        {
          "rfilename": "qwen3-30b-a3b-q6_k.gguf"
        },
        {
          "rfilename": "qwen3-30b-a3b-q8_0.gguf"
        },
        {
          "rfilename": "qwen3-32b-q2_k.gguf"
        },
        {
          "rfilename": "qwen3-32b-q3_k_l.gguf"
        },
        {
          "rfilename": "qwen3-32b-q3_k_m.gguf"
        },
        {
          "rfilename": "qwen3-32b-q3_k_s.gguf"
        },
        {
          "rfilename": "qwen3-32b-q4_k_m.gguf"
        },
        {
          "rfilename": "qwen3-32b-q4_k_s.gguf"
        },
        {
          "rfilename": "qwen3-32b-q5_k_m.gguf"
        },
        {
          "rfilename": "qwen3-32b-q5_k_s.gguf"
        },
        {
          "rfilename": "qwen3-32b-q6_k.gguf"
        },
        {
          "rfilename": "qwen3-32b-q8_0.gguf"
        },
        {
          "rfilename": "qwen3-4b-q2_k.gguf"
        },
        {
          "rfilename": "qwen3-4b-q3_k_l.gguf"
        },
        {
          "rfilename": "qwen3-4b-q3_k_m.gguf"
        },
        {
          "rfilename": "qwen3-4b-q3_k_s.gguf"
        },
        {
          "rfilename": "qwen3-4b-q4_k_m.gguf"
        },
        {
          "rfilename": "qwen3-4b-q4_k_s.gguf"
        },
        {
          "rfilename": "qwen3-4b-q5_k_m.gguf"
        },
        {
          "rfilename": "qwen3-4b-q5_k_s.gguf"
        },
        {
          "rfilename": "qwen3-4b-q6_k.gguf"
        },
        {
          "rfilename": "qwen3-4b-q8_0.gguf"
        },
        {
          "rfilename": "qwen3-8b-q2_k.gguf"
        },
        {
          "rfilename": "qwen3-8b-q3_k_l.gguf"
        },
        {
          "rfilename": "qwen3-8b-q3_k_m.gguf"
        },
        {
          "rfilename": "qwen3-8b-q3_k_s.gguf"
        },
        {
          "rfilename": "qwen3-8b-q4_k_m.gguf"
        },
        {
          "rfilename": "qwen3-8b-q4_k_s.gguf"
        },
        {
          "rfilename": "qwen3-8b-q5_k_m.gguf"
        },
        {
          "rfilename": "qwen3-8b-q5_k_s.gguf"
        },
        {
          "rfilename": "qwen3-8b-q6_k.gguf"
        },
        {
          "rfilename": "qwen3-8b-q8_0.gguf"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "featured",
        "text-generation",
        "license:apache-2.0",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 588411644672,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "qwen3:32b",
        "size": 19762149088
      },
      {
        "id": "qwen3:8b",
        "size": 5027783808
      },
      {
        "id": "qwen3:0.6b",
        "size": 484219968
      },
      {
        "id": "qwen3:4b",
        "size": 2497280608
      },
      {
        "id": "qwen3:30b-a3b",
        "size": 18556686208
      },
      {
        "id": "qwen3:14b",
        "size": 9001753280
      },
      {
        "id": "qwen3:1.7b",
        "size": 1282439232
      }
    ]
  },
  {
    "author": "TinyLlama",
    "id": "cortexso/tinyllama",
    "metadata": {
      "_id": "66791800ca45b9165970f2fe",
      "author": "cortexso",
      "cardData": {
        "license": "apache-2.0",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2024-06-24T06:53:52.000Z",
      "description": "---\nlicense: apache-2.0\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n\n## Overview\n\nThe [TinyLlama](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0) project aims to pretrain a 1.1B Llama model on 3 trillion tokens. This is the chat model finetuned  on a diverse range of synthetic dialogues generated by ChatGPT.\n\n## Variants\n\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [TinyLLama-1b](https://huggingface.co/cortexso/tinyllama/tree/1b) | `cortex run tinyllama:1b` |\n\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexhub/tinyllama\n    ```\n    \n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run tinyllama\n    ```\n    \n## Credits\n\n- **Author:** Microsoft\n- **Converter:** [Homebrew](https://www.homebrew.ltd/)\n- **Original License:** [License](https://choosealicense.com/licenses/apache-2.0/)\n- **Papers:** [Tinyllama Paper](https://arxiv.org/abs/2401.02385)",
      "disabled": false,
      "downloads": 562,
      "gated": false,
      "gguf": {
        "architecture": "llama",
        "bos_token": "<s>",
        "chat_template": "{% for message in messages %}\n{% if message['role'] == 'user' %}\n{{ '<|user|>\n' + message['content'] + eos_token }}\n{% elif message['role'] == 'system' %}\n{{ '<|system|>\n' + message['content'] + eos_token }}\n{% elif message['role'] == 'assistant' %}\n{{ '<|assistant|>\n'  + message['content'] + eos_token }}\n{% endif %}\n{% if loop.last and add_generation_prompt %}\n{{ '<|assistant|>' }}\n{% endif %}\n{% endfor %}",
        "context_length": 2048,
        "eos_token": "</s>",
        "total": 1100048384
      },
      "id": "cortexso/tinyllama",
      "lastModified": "2025-03-03T06:16:24.000Z",
      "likes": 0,
      "model-index": null,
      "modelId": "cortexso/tinyllama",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "953054fd3565023c2bbd2381f2566f904f5bdc1f",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        },
        {
          "rfilename": "tinyllama-1.1b-chat-v1.0-q2_k.gguf"
        },
        {
          "rfilename": "tinyllama-1.1b-chat-v1.0-q3_k_l.gguf"
        },
        {
          "rfilename": "tinyllama-1.1b-chat-v1.0-q3_k_m.gguf"
        },
        {
          "rfilename": "tinyllama-1.1b-chat-v1.0-q3_k_s.gguf"
        },
        {
          "rfilename": "tinyllama-1.1b-chat-v1.0-q4_k_m.gguf"
        },
        {
          "rfilename": "tinyllama-1.1b-chat-v1.0-q4_k_s.gguf"
        },
        {
          "rfilename": "tinyllama-1.1b-chat-v1.0-q5_k_m.gguf"
        },
        {
          "rfilename": "tinyllama-1.1b-chat-v1.0-q5_k_s.gguf"
        },
        {
          "rfilename": "tinyllama-1.1b-chat-v1.0-q6_k.gguf"
        },
        {
          "rfilename": "tinyllama-1.1b-chat-v1.0-q8_0.gguf"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "arxiv:2401.02385",
        "license:apache-2.0",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 8451229056,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "tinyllama:1b",
        "size": 782045248
      }
    ]
  },
  {
    "author": "meta-llama",
    "id": "cortexso/llama3",
    "metadata": {
      "_id": "6667a6d52e5f1c08ec14469c",
      "author": "cortexso",
      "cardData": {
        "license": "llama3",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2024-06-11T01:22:29.000Z",
      "description": "---\nlicense: llama3\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n\n## Overview\n\nMeta developed and released the [Meta Llama 3](https://huggingface.co/meta-llama/Meta-Llama-3-8B) family of large language models (LLMs), a collection of pretrained and instruction tuned generative text models in 8 and 70B sizes. The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks. Further, in developing these models, we took great care to optimize helpfulness and safety.\n\n## Variants\n\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Llama3-8b](https://huggingface.co/cortexso/llama3/tree/8b) | `cortex run llama3:8b` |\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexso/llama3\n    ```\n\n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run llama3\n    ```\n\n## Credits\n\n- **Author:** Meta\n- **Converter:** [Homebrew](https://www.homebrew.ltd/)\n- **Original License:** [License](https://llama.meta.com/llama3/license/)\n- **Papers:** [Llama-3 Blog](https://llama.meta.com/llama3/)",
      "disabled": false,
      "downloads": 646,
      "gated": false,
      "gguf": {
        "architecture": "llama",
        "bos_token": "<|begin_of_text|>",
        "chat_template": "{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- set date_string = \"26 Jul 2024\" %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = \"\" %}\n{%- endif %}\n\n{#- System message + builtin tools #}\n{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n{%- if builtin_tools is defined or tools is not none %}\n    {{- \"Environment: ipython\\n\" }}\n{%- endif %}\n{%- if builtin_tools is defined %}\n    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n{%- endif %}\n{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- \"<|eot_id|>\" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0]['content']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n{%- endif %}\n    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n    {{- first_user_message + \"<|eot_id|>\"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n    {%- elif 'tool_calls' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\n                {{- arg_name + '=\"' + arg_val + '\"' }}\n                {%- if not loop.last %}\n                    {{- \", \" }}\n                {%- endif %}\n                {%- endfor %}\n            {{- \")\" }}\n        {%- else  %}\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n            {{- '\"parameters\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \"}\" }}\n        {%- endif %}\n        {%- if builtin_tools is defined %}\n            {#- This means we're in ipython mode #}\n            {{- \"<|eom_id|>\" }}\n        {%- else %}\n            {{- \"<|eot_id|>\" }}\n        {%- endif %}\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- \"<|eot_id|>\" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}\n",
        "context_length": 131072,
        "eos_token": "<|eot_id|>",
        "total": 8030261312
      },
      "id": "cortexso/llama3",
      "lastModified": "2025-03-03T06:19:24.000Z",
      "likes": 0,
      "model-index": null,
      "modelId": "cortexso/llama3",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "fcf18c0b14bb2dc64c7f78da40ca88a8ff759fd5",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "llama-3.1-8b-instruct-q2_k.gguf"
        },
        {
          "rfilename": "llama-3.1-8b-instruct-q3_k_l.gguf"
        },
        {
          "rfilename": "llama-3.1-8b-instruct-q3_k_m.gguf"
        },
        {
          "rfilename": "llama-3.1-8b-instruct-q3_k_s.gguf"
        },
        {
          "rfilename": "llama-3.1-8b-instruct-q4_k_m.gguf"
        },
        {
          "rfilename": "llama-3.1-8b-instruct-q4_k_s.gguf"
        },
        {
          "rfilename": "llama-3.1-8b-instruct-q5_k_m.gguf"
        },
        {
          "rfilename": "llama-3.1-8b-instruct-q5_k_s.gguf"
        },
        {
          "rfilename": "llama-3.1-8b-instruct-q6_k.gguf"
        },
        {
          "rfilename": "llama-3.1-8b-instruct-q8_0.gguf"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "license:llama3",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 70949951936,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "llama3:8b",
        "size": 4920739072
      }
    ]
  },
  {
    "author": "meta-llama",
    "id": "cortexso/llama3.1",
    "metadata": {
      "_id": "66a76e01a1037fe261a5a472",
      "author": "cortexso",
      "cardData": {
        "license": "llama3.1",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2024-07-29T10:25:05.000Z",
      "description": "---\nlicense: llama3.1\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n\n## Overview\n\nMeta developed and released the [Meta Llama 3.1](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B) family of large language models (LLMs), a collection of pretrained and instruction tuned generative text models in 8 and 70B sizes. The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks. Further, in developing these models, we took great care to optimize helpfulness and safety.\n\n## Variants\n\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Llama3.1-8b](https://huggingface.co/cortexso/llama3.1/tree/8b) | `cortex run llama3.1:8b` |\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexso/llama3.1\n    ```\n\n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run llama3.1\n    ```\n\n## Credits\n\n- **Author:** Meta\n- **Converter:** [Homebrew](https://www.homebrew.ltd/)\n- **Original License:** [License](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/blob/main/LICENSE)\n- **Papers:** [Llama-3.1 Blog](https://ai.meta.com/blog/meta-llama-3-1/)",
      "disabled": false,
      "downloads": 1048,
      "gated": false,
      "gguf": {
        "architecture": "llama",
        "bos_token": "<|begin_of_text|>",
        "context_length": 131072,
        "eos_token": "<|end_of_text|>",
        "total": 8030261312
      },
      "id": "cortexso/llama3.1",
      "lastModified": "2025-03-02T14:27:57.000Z",
      "likes": 0,
      "model-index": null,
      "modelId": "cortexso/llama3.1",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "256c4f2118a75d93a1dc368ac4ccf1fea16751c2",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "llama-3.1-8b-q2_k.gguf"
        },
        {
          "rfilename": "llama-3.1-8b-q3_k_l.gguf"
        },
        {
          "rfilename": "llama-3.1-8b-q3_k_m.gguf"
        },
        {
          "rfilename": "llama-3.1-8b-q3_k_s.gguf"
        },
        {
          "rfilename": "llama-3.1-8b-q4_k_m.gguf"
        },
        {
          "rfilename": "llama-3.1-8b-q4_k_s.gguf"
        },
        {
          "rfilename": "llama-3.1-8b-q5_k_m.gguf"
        },
        {
          "rfilename": "llama-3.1-8b-q5_k_s.gguf"
        },
        {
          "rfilename": "llama-3.1-8b-q6_k.gguf"
        },
        {
          "rfilename": "llama-3.1-8b-q8_0.gguf"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "license:llama3.1",
        "endpoints_compatible",
        "region:us"
      ],
      "usedStorage": 66029173888,
      "widgetData": [
        {
          "text": "My name is Julien and I like to"
        },
        {
          "text": "I like traveling by train because"
        },
        {
          "text": "Paris is an amazing place to visit,"
        },
        {
          "text": "Once upon a time,"
        }
      ]
    },
    "models": [
      {
        "id": "llama3.1:8b",
        "size": 4920734176
      }
    ]
  },
  {
    "author": "AIDC-AI",
    "id": "cortexso/marco-o1",
    "metadata": {
      "_id": "6743b6140d46fa30e6ff2879",
      "author": "cortexso",
      "cardData": {
        "license": "other",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2024-11-24T23:26:12.000Z",
      "description": "---\nlicense: other\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n\n## Overview\nMarco-o1 not only focuses on disciplines with standard answers, such as mathematics, physics, and coding—which are well-suited for reinforcement learning (RL)—but also places greater emphasis on open-ended resolutions. We aim to address the question: \"Can the o1 model effectively generalize to broader domains where clear standards are absent and rewards are challenging to quantify?\"\n\nCurrently, Marco-o1 Large Language Model (LLM) is powered by Chain-of-Thought (CoT) fine-tuning, Monte Carlo Tree Search (MCTS), reflection mechanisms, and innovative reasoning strategies—optimized for complex real-world problem-solving tasks.\n\n## Variants\n\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Marco-o1-8b](https://huggingface.co/cortexso/marco-o1/tree/8b) | `cortex run marco-o1:8b` |\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexhub/marco-o1\n    ```\n\n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run marco-o1\n    ```\n    \n## Credits\n\n- **Author:** AIDC-AI\n- **Converter:** [Homebrew](https://homebrew.ltd/)\n- **Original License:** [Licence](https://huggingface.co/AIDC-AI/Marco-o1/blob/main/LICENSE)\n- **Papers:** [Paper](https://arxiv.org/abs/2411.14405)",
      "disabled": false,
      "downloads": 122,
      "gated": false,
      "gguf": {
        "architecture": "qwen2",
        "bos_token": "<|endoftext|>",
        "chat_template": "{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\n\n你是一个经过良好训练的AI助手，你的名字是Marco-o1.由阿里国际数字商业集团的AI Business创造.\n        \n## 重要！！！！！\n当你回答问题时，你的思考应该在<Thought>内完成，<Output>内输出你的结果。\n<Thought>应该尽可能是英文，但是有2个特例，一个是对原文中的引用，另一个是是数学应该使用markdown格式，<Output>内的输出需要遵循用户输入的语言。\n        <|im_end|>\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}",
        "context_length": 32768,
        "eos_token": "<|im_end|>",
        "total": 7615616512
      },
      "id": "cortexso/marco-o1",
      "lastModified": "2025-03-03T02:27:27.000Z",
      "likes": 0,
      "model-index": null,
      "modelId": "cortexso/marco-o1",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "0c8e0cdbfb898e000cad200b2694c5c6e6710fc6",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "marco-o1-q2_k.gguf"
        },
        {
          "rfilename": "marco-o1-q3_k_l.gguf"
        },
        {
          "rfilename": "marco-o1-q3_k_m.gguf"
        },
        {
          "rfilename": "marco-o1-q3_k_s.gguf"
        },
        {
          "rfilename": "marco-o1-q4_k_m.gguf"
        },
        {
          "rfilename": "marco-o1-q4_k_s.gguf"
        },
        {
          "rfilename": "marco-o1-q5_k_m.gguf"
        },
        {
          "rfilename": "marco-o1-q5_k_s.gguf"
        },
        {
          "rfilename": "marco-o1-q6_k.gguf"
        },
        {
          "rfilename": "marco-o1-q8_0.gguf"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "arxiv:2411.14405",
        "license:other",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 53341785824,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "marco-o1:8b",
        "size": 4683071648
      }
    ]
  },
  {
    "author": "DeepSeek-AI",
    "id": "cortexso/deepseek-r1-distill-qwen-1.5b",
    "metadata": {
      "_id": "678e84d99d66241aabee008a",
      "author": "cortexso",
      "cardData": {
        "license": "mit",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2025-01-20T17:16:09.000Z",
      "description": "---\nlicense: mit\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n## Overview\n\n**DeepSeek** developed and released the [DeepSeek R1 Distill Qwen 1.5B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B) model, a distilled version of the Qwen 1.5B language model. It is fine-tuned for high-performance text generation and optimized for dialogue and information-seeking tasks. This model achieves a balance of efficiency and accuracy while maintaining a smaller footprint compared to the original Qwen 1.5B.\n\nThe model is designed for applications in customer support, conversational AI, and research, prioritizing both helpfulness and safety.\n\n## Variants\n\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Deepseek-r1-distill-qwen-1.5b-1.5b](https://huggingface.co/cortexso/deepseek-r1-distill-qwen-1.5b/tree/1.5b) | `cortex run deepseek-r1-distill-qwen-1.5b:1.5b` |\n\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexso/deepseek-r1-distill-qwen-1.5b\n    ```\n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run deepseek-r1-distill-qwen-1.5b\n    ```\n## Credits\n\n- **Author:** DeepSeek\n- **Converter:** [Homebrew](https://www.homebrew.ltd/)\n- **Original License:** [License](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B#7-license)\n- **Papers:** [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/html/2501.12948v1)",
      "disabled": false,
      "downloads": 539,
      "gated": false,
      "gguf": {
        "architecture": "qwen2",
        "bos_token": "<｜begin▁of▁sentence｜>",
        "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<｜User｜>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<｜Assistant｜><｜tool▁calls▁begin｜><｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<｜tool▁call▁begin｜>' + tool['type'] + '<｜tool▁sep｜>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<｜tool▁call▁end｜>'}}{{'<｜tool▁calls▁end｜><｜end▁of▁sentence｜>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<｜tool▁outputs▁end｜>' + message['content'] + '<｜end▁of▁sentence｜>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<｜Assistant｜>' + content + '<｜end▁of▁sentence｜>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<｜tool▁outputs▁begin｜><｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<｜tool▁output▁begin｜>' + message['content'] + '<｜tool▁output▁end｜>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<｜tool▁outputs▁end｜>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<｜Assistant｜><think>\\n'}}{% endif %}",
        "context_length": 131072,
        "eos_token": "<｜end▁of▁sentence｜>",
        "total": 1777088000
      },
      "id": "cortexso/deepseek-r1-distill-qwen-1.5b",
      "lastModified": "2025-03-03T05:24:13.000Z",
      "likes": 0,
      "model-index": null,
      "modelId": "cortexso/deepseek-r1-distill-qwen-1.5b",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "14cbd3c8ac57a346c35f676fd5fe55befebd911e",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-1.5b-q2_k.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-1.5b-q3_k_l.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-1.5b-q3_k_m.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-1.5b-q3_k_s.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-1.5b-q4_k_m.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-1.5b-q4_k_s.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-1.5b-q5_k_m.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-1.5b-q5_k_s.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-1.5b-q6_k.gguf"
        },
        {
          "rfilename": "deepseek-r1-distill-qwen-1.5b-q8_0.gguf"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "license:mit",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 12728600096,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "deepseek-r1-distill-qwen-1.5b:1.5b",
        "size": 1117320480
      }
    ]
  },
  {
    "author": "PrimeIntellect",
    "id": "cortexso/intellect-1",
    "metadata": {
      "_id": "674e48fc24f1ef616cd485de",
      "author": "cortexso",
      "cardData": {
        "license": "other",
        "pipeline_tag": "text-generation",
        "tags": ["cortex.cpp"]
      },
      "createdAt": "2024-12-02T23:55:40.000Z",
      "description": "---\nlicense: other\npipeline_tag: text-generation\ntags:\n- cortex.cpp\n---\n\n## Overview\n\nIntellect-1 is a high-performance instruction-tuned model developed by Qwen, designed to handle a broad range of natural language processing tasks with efficiency and precision. Optimized for dialogue, reasoning, and knowledge-intensive applications, Intellect-1 excels in structured generation, summarization, and retrieval-augmented tasks. It is part of an open ecosystem, providing transparency in training data, model architecture, and fine-tuning methodologies.\n\n\n## Variants\n\n| No | Variant | Cortex CLI command |\n| --- | --- | --- |\n| 1 | [Intellect-1-10b](https://huggingface.co/cortexso/intellect-1/tree/10b) | `cortex run intellect-1:10b` |\n\n## Use it with Jan (UI)\n\n1. Install **Jan** using [Quickstart](https://jan.ai/docs/quickstart)\n2. Use in Jan model Hub:\n    ```bash\n    cortexhub/intellect-1\n    ```\n\n## Use it with Cortex (CLI)\n\n1. Install **Cortex** using [Quickstart](https://cortex.jan.ai/docs/quickstart)\n2. Run the model with command:\n    ```bash\n    cortex run intellect-1\n    ```\n    \n## Credits\n\n- **Author:** Qwen\n- **Converter:** [Homebrew](https://homebrew.ltd/)\n- **Original License:** [Licence](https://choosealicense.com/licenses/apache-2.0/)\n- **Papers:** [Technical Paper](https://github.com/PrimeIntellect-ai/prime)",
      "disabled": false,
      "downloads": 182,
      "gated": false,
      "gguf": {
        "architecture": "llama",
        "bos_token": "<|begin_of_text|>",
        "chat_template": "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}{% endif %}",
        "context_length": 8192,
        "eos_token": "<|eot_id|>",
        "total": 10211381248
      },
      "id": "cortexso/intellect-1",
      "lastModified": "2025-03-03T02:32:47.000Z",
      "likes": 0,
      "model-index": null,
      "modelId": "cortexso/intellect-1",
      "pipeline_tag": "text-generation",
      "private": false,
      "sha": "f46fd8109130aab2969fd9229d390051f774a761",
      "siblings": [
        {
          "rfilename": ".gitattributes"
        },
        {
          "rfilename": "README.md"
        },
        {
          "rfilename": "intellect-1-instruct-q2_k.gguf"
        },
        {
          "rfilename": "intellect-1-instruct-q3_k_l.gguf"
        },
        {
          "rfilename": "intellect-1-instruct-q3_k_m.gguf"
        },
        {
          "rfilename": "intellect-1-instruct-q3_k_s.gguf"
        },
        {
          "rfilename": "intellect-1-instruct-q4_k_m.gguf"
        },
        {
          "rfilename": "intellect-1-instruct-q4_k_s.gguf"
        },
        {
          "rfilename": "intellect-1-instruct-q5_k_m.gguf"
        },
        {
          "rfilename": "intellect-1-instruct-q5_k_s.gguf"
        },
        {
          "rfilename": "intellect-1-instruct-q6_k.gguf"
        },
        {
          "rfilename": "intellect-1-instruct-q8_0.gguf"
        },
        {
          "rfilename": "metadata.yml"
        },
        {
          "rfilename": "model.yml"
        }
      ],
      "spaces": [],
      "tags": [
        "gguf",
        "cortex.cpp",
        "text-generation",
        "license:other",
        "endpoints_compatible",
        "region:us",
        "conversational"
      ],
      "usedStorage": 71113603904,
      "widgetData": [
        {
          "text": "Hi, what can you help me with?"
        },
        {
          "text": "What is 84 * 3 / 2?"
        },
        {
          "text": "Tell me an interesting fact about the universe!"
        },
        {
          "text": "Explain quantum computing in simple terms."
        }
      ]
    },
    "models": [
      {
        "id": "intellect-1:10b",
        "size": 6229006784
      }
    ]
  }
]
