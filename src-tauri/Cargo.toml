[package]
name = "Jan-nightly"
version = "0.5.16-1320"
description = "Use offline LLMs with your own data. Run open source models like Llama2 or Falcon on your internal computers/servers."
authors = ["Jan <service@jan.ai>"]
license = "MIT"
repository = "https://github.com/menloresearch/jan"
edition = "2021"
rust-version = "1.77.2"

[lib]
name = "app_lib"
crate-type = ["staticlib", "cdylib", "rlib"]

[build-dependencies]
tauri-build = { version = "2.0.2", features = [] }

[dependencies]
serde_json = "1.0"
serde = { version = "1.0", features = ["derive"] }
log = "0.4"
tauri = { version = "2.1.0", features = [ "protocol-asset", "macos-private-api",
    "test",
] }
tauri-plugin-log = "2.0.0-rc"
tauri-plugin-shell = "2.2.0"
flate2 = "1.0"
tar = "0.4"
rand = "0.8"
tauri-plugin-http = { version = "2", features = ["unsafe-headers"] }
tauri-plugin-store = "2"
hyper = { version = "0.14", features = ["server"] }
reqwest = { version = "0.11", features = ["json"] }
tokio = { version = "1", features = ["full"] }
rmcp = { git = "https://github.com/modelcontextprotocol/rust-sdk", branch = "main", features = [
    "client",
    "transport-sse",
    "transport-child-process",
    "tower",
] }
uuid = { version = "1.7", features = ["v4"] }

[target.'cfg(not(any(target_os = "android", target_os = "ios")))'.dependencies]
tauri-plugin-updater = "2"
once_cell = "1.18"
